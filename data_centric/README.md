# Data-Centric
- In this folder, we will be adding the code for all the methods from **Data-Centric Approach** that we will be trying.

## File Descriptions
- `trad_aug.ipynb` -- This `.ipynb` notebook covers the code for applying the Traditional Augmentation approaches on the Dataset and then training the Baseline Model on the Augmented Dataset. The kernel consists of 3 different approaches, random-sampling based augmentation, augmentation for class balancing, and augmentation based on class-wise performance. Out of the 3 approaches, none of them gave any boost in terms of accuracy and/or log-loss. **Augmentation for class balancing** gave the highest **test accuracy of 0.7746** amaong the 3 with a **log-loss of 0.8160** on the test dataset.
- `gan_aug_1.ipynb` -- This `.ipynb` notebook covers the code for applying the GAN-Based Augmentation approaches. In this kernel, we have used PyTorch to train a GAN for 1000 epochs on the CIFAR-10 Dataset, and then used the fake images from the GAN to augment the existing dataset. This is the first kernel for the approach in which we have prepared the augmented datasets. 
- `gan_aug_2.ipynb` -- This `.ipynb` notebook is the second kernel for the GAN-Based Augmentation approach, in which, we have trained the models on the augmented datasets. Just like the traditional augmentation, we have tried 3 different approaches, random-sampling based augmentation, augmentation for class balancing, and augmentation based on class-wise performance, however, none of them gave any boost in terms of log-loss and/or accuracy for the test dataset. **Augmentation based on class-wise performance** gave the highest **test accuracy of 0.7664** amaong the 3 with a **log-loss of 0.8320** on the test dataset.