{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea064f1b",
   "metadata": {
    "papermill": {
     "duration": 0.023712,
     "end_time": "2022-04-15T16:57:47.234708",
     "exception": false,
     "start_time": "2022-04-15T16:57:47.210996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A Simple Framework for Contrastive Learning of Visual Representations\n",
    "* In this notebook, we will be implementing the code for SimCLR.\n",
    "* We have done the augmentation, as per the original code provided the researchers behind this paper.\n",
    "* [Reference](http://proceedings.mlr.press/v119/chen20j.html) for the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330f9356",
   "metadata": {
    "papermill": {
     "duration": 0.021802,
     "end_time": "2022-04-15T16:57:47.279018",
     "exception": false,
     "start_time": "2022-04-15T16:57:47.257216",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importing Packages and Boilerplate code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdcc7a04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T16:57:47.333266Z",
     "iopub.status.busy": "2022-04-15T16:57:47.332498Z",
     "iopub.status.idle": "2022-04-15T16:57:58.358415Z",
     "shell.execute_reply": "2022-04-15T16:57:58.357816Z",
     "shell.execute_reply.started": "2022-04-15T16:21:16.679366Z"
    },
    "papermill": {
     "duration": 11.057302,
     "end_time": "2022-04-15T16:57:58.358568",
     "exception": false,
     "start_time": "2022-04-15T16:57:47.301266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imutils\r\n",
      "  Downloading imutils-0.5.4.tar.gz (17 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hBuilding wheels for collected packages: imutils\r\n",
      "  Building wheel for imutils (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25860 sha256=15ae5b2aff70f5539997aa4f0c974f28f322c6cb3f4153a25c6139ae0c2323d4\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/86/d7/0a/4923351ed1cec5d5e24c1eaf8905567b02a0343b24aa873df2\r\n",
      "Successfully built imutils\r\n",
      "Installing collected packages: imutils\r\n",
      "Successfully installed imutils-0.5.4\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c7bc10a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T16:57:58.418622Z",
     "iopub.status.busy": "2022-04-15T16:57:58.414923Z",
     "iopub.status.idle": "2022-04-15T16:58:05.127685Z",
     "shell.execute_reply": "2022-04-15T16:58:05.128269Z",
     "shell.execute_reply.started": "2022-04-15T16:29:17.316530Z"
    },
    "papermill": {
     "duration": 6.743563,
     "end_time": "2022-04-15T16:58:05.128419",
     "exception": false,
     "start_time": "2022-04-15T16:57:58.384856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version :  2.6.2\n"
     ]
    }
   ],
   "source": [
    "# Basic Imports \n",
    "import math\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.models import *\n",
    "\n",
    "# Plotting libraries\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# Utilities\n",
    "import datetime\n",
    "import os,sys\n",
    "import tempfile\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "sys.path.append(\"/kaggle/input/helper-files\")\n",
    "import random \n",
    "import gc\n",
    "import time\n",
    "import functools\n",
    "from imutils import paths\n",
    "from tqdm import tqdm\n",
    "from typing import Callable\n",
    "\n",
    "# Random seed fixation for experiment result repitition\n",
    "tf.random.set_seed(10)\n",
    "np.random.seed(10)\n",
    "\n",
    "print(\"Tensorflow version : \",tf.__version__)\n",
    "from shutil import copyfile\n",
    "from tabulate import tabulate\n",
    "from sklearn.metrics import accuracy_score, log_loss, confusion_matrix\n",
    "import tensorflow.keras.layers as tfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92b25348",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T16:58:05.187732Z",
     "iopub.status.busy": "2022-04-15T16:58:05.186817Z",
     "iopub.status.idle": "2022-04-15T16:58:05.188548Z",
     "shell.execute_reply": "2022-04-15T16:58:05.188998Z",
     "shell.execute_reply.started": "2022-04-15T16:21:24.054854Z"
    },
    "papermill": {
     "duration": 0.034821,
     "end_time": "2022-04-15T16:58:05.189153",
     "exception": false,
     "start_time": "2022-04-15T16:58:05.154332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting the seeds\n",
    "SEED = 0\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2757f9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T16:58:07.263563Z",
     "iopub.status.busy": "2022-04-15T16:58:07.262708Z",
     "iopub.status.idle": "2022-04-15T16:58:07.268690Z",
     "shell.execute_reply": "2022-04-15T16:58:07.267997Z",
     "shell.execute_reply.started": "2022-04-15T16:21:24.065372Z"
    },
    "papermill": {
     "duration": 2.053893,
     "end_time": "2022-04-15T16:58:07.268860",
     "exception": false,
     "start_time": "2022-04-15T16:58:05.214967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Making sure that Tensorflow is able to detect the GPU\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if \"GPU\" not in device_name:\n",
    "    print(\"GPU device not found\")\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d939833",
   "metadata": {
    "papermill": {
     "duration": 0.041383,
     "end_time": "2022-04-15T16:58:07.352452",
     "exception": false,
     "start_time": "2022-04-15T16:58:07.311069",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importing Train/Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "151cf123",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T16:58:07.449860Z",
     "iopub.status.busy": "2022-04-15T16:58:07.448985Z",
     "iopub.status.idle": "2022-04-15T16:58:07.452254Z",
     "shell.execute_reply": "2022-04-15T16:58:07.452817Z",
     "shell.execute_reply.started": "2022-04-15T16:21:24.077718Z"
    },
    "papermill": {
     "duration": 0.055366,
     "end_time": "2022-04-15T16:58:07.453003",
     "exception": false,
     "start_time": "2022-04-15T16:58:07.397637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# These are the usual ipython objects\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Defining a function to list the memory consumed\n",
    "# Only outputs variables taking at least 1MB space\n",
    "def list_storage(inp_dir):\n",
    "    # Get a sorted list of the objects and their sizes\n",
    "    vars_defined = [x for x in inp_dir if not x.startswith('_') and x not in sys.modules and x not in ipython_vars]\n",
    "    sto = sorted([(x, sys.getsizeof(globals().get(x))) for x in vars_defined], key=lambda x: x[1], reverse=True)\n",
    "    sto = [(x[0], str(round((x[1] / 2**20), 2)) + ' MB') for x in sto if x[1] >= 2**20]\n",
    "    print(tabulate(sto, headers = ['Variable', 'Storage (in MB)']))\n",
    "\n",
    "# In order to use this function, use the below line of code\n",
    "# list_storage(dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f18b37eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T16:58:07.544757Z",
     "iopub.status.busy": "2022-04-15T16:58:07.543898Z",
     "iopub.status.idle": "2022-04-15T16:58:33.655493Z",
     "shell.execute_reply": "2022-04-15T16:58:33.654904Z",
     "shell.execute_reply.started": "2022-04-15T16:21:24.089067Z"
    },
    "papermill": {
     "duration": 26.159966,
     "end_time": "2022-04-15T16:58:33.655650",
     "exception": false,
     "start_time": "2022-04-15T16:58:07.495684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing the Labelled Dataset\n",
    "df_train = pd.read_csv(\"../input/cifar10/train_lab_x.csv\")\n",
    "y_train = pd.read_csv(\"../input/cifar10/train_lab_y.csv\")\n",
    "\n",
    "# Importing the Test Dataset\n",
    "df_test = pd.read_csv(\"../input/cifar10/test_x.csv\")\n",
    "y_test = pd.read_csv(\"../input/cifar10/test_y.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d0eb35",
   "metadata": {
    "papermill": {
     "duration": 0.058213,
     "end_time": "2022-04-15T16:58:33.752931",
     "exception": false,
     "start_time": "2022-04-15T16:58:33.694718",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Basic Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e492e384",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T16:58:33.864496Z",
     "iopub.status.busy": "2022-04-15T16:58:33.863688Z",
     "iopub.status.idle": "2022-04-15T16:58:37.135211Z",
     "shell.execute_reply": "2022-04-15T16:58:37.135878Z",
     "shell.execute_reply.started": "2022-04-15T16:21:46.857659Z"
    },
    "papermill": {
     "duration": 3.328603,
     "end_time": "2022-04-15T16:58:37.136117",
     "exception": false,
     "start_time": "2022-04-15T16:58:33.807514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40006, 3072) (40006, 1)\n",
      "(40006, 3, 32, 32)\n",
      "[0]\n",
      "(40006, 32, 32, 3) (40006, 10)\n",
      "(10000, 3072) (10000, 1)\n",
      "(10000, 3, 32, 32)\n",
      "(10000, 32, 32, 3) (10000, 10)\n",
      "Tensorflow version :  2.6.2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 108x108 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = np.array(df_train)\n",
    "y_train = np.array(y_train)\n",
    "print(df_train.shape, y_train.shape)\n",
    "\n",
    "# Reshaping the dataset\n",
    "df_train = np.reshape(df_train, (-1, 3, 32, 32))\n",
    "print(df_train.shape)\n",
    "\n",
    "# Visualizing a single image\n",
    "ind = 11\n",
    "example = df_train[ind, : , : , : ]\n",
    "example = example.transpose((1, 2, 0))\n",
    "plt.figure(figsize=(1.5, 1.5))\n",
    "plt.imshow(example)\n",
    "print(y_train[ind])\n",
    "\n",
    "# Creating a random permutation\n",
    "perm = np.random.permutation(df_train.shape[0])\n",
    "\n",
    "# Shuffling the training dataset\n",
    "df_train = df_train[perm, : , : , : ]\n",
    "y_train = y_train[perm]\n",
    "\n",
    "# Reshaping, rescaling and one-hot encoding\n",
    "df_train = np.transpose(np.array(df_train), (0, 2, 3, 1))\n",
    "df_train = df_train / 255\n",
    "y_train_oh = tf.one_hot(np.ravel(y_train), depth = 10)\n",
    "\n",
    "print(df_train.shape, y_train_oh.shape)\n",
    "\n",
    "df_test = np.array(df_test)\n",
    "y_test = np.array(y_test)\n",
    "print(df_test.shape, y_test.shape)\n",
    "\n",
    "# Reshaping the dataset\n",
    "df_test = np.reshape(df_test, (-1, 3, 32, 32))\n",
    "print(df_test.shape)\n",
    "\n",
    "# Reshaping, rescaling and one-hot encoding\n",
    "df_test = np.transpose(np.array(df_test), (0, 2, 3, 1))\n",
    "df_test = df_test / 255\n",
    "y_test_oh = tf.one_hot(np.ravel(y_test), depth = 10)\n",
    "print(df_test.shape, y_test_oh.shape)\n",
    "\n",
    "# Random seed fixation for experiment result repitition\n",
    "tf.random.set_seed(10)\n",
    "np.random.seed(10)\n",
    "\n",
    "print(\"Tensorflow version : \",tf.__version__)\n",
    "\n",
    "# In-order run function decorators in tf2.0\n",
    "tf.config.run_functions_eagerly(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f421862",
   "metadata": {
    "papermill": {
     "duration": 0.058148,
     "end_time": "2022-04-15T16:58:37.250660",
     "exception": false,
     "start_time": "2022-04-15T16:58:37.192512",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helper Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "213b3bd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T16:58:37.313971Z",
     "iopub.status.busy": "2022-04-15T16:58:37.312373Z",
     "iopub.status.idle": "2022-04-15T16:58:37.314597Z",
     "shell.execute_reply": "2022-04-15T16:58:37.314990Z",
     "shell.execute_reply.started": "2022-04-15T16:21:50.102646Z"
    },
    "papermill": {
     "duration": 0.035642,
     "end_time": "2022-04-15T16:58:37.315139",
     "exception": false,
     "start_time": "2022-04-15T16:58:37.279497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Image training properties \n",
    "IMG_H, IMG_W = (32, 32)\n",
    "\n",
    "# How powerful you want the colour augmentations to be\n",
    "color_jitter_strength = 0.3\n",
    "\n",
    "# Minimum crop area you want\n",
    "minimum_object_coverage = 0.7\n",
    "\n",
    "# Range of crop area\n",
    "area_range = (minimum_object_coverage, 1.0)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4544d6",
   "metadata": {
    "papermill": {
     "duration": 0.027614,
     "end_time": "2022-04-15T16:58:37.370587",
     "exception": false,
     "start_time": "2022-04-15T16:58:37.342973",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Building\n",
    "## Model Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b8491d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T16:58:37.433145Z",
     "iopub.status.busy": "2022-04-15T16:58:37.432329Z",
     "iopub.status.idle": "2022-04-15T16:58:37.434824Z",
     "shell.execute_reply": "2022-04-15T16:58:37.434393Z",
     "shell.execute_reply.started": "2022-04-15T16:21:50.111482Z"
    },
    "papermill": {
     "duration": 0.036644,
     "end_time": "2022-04-15T16:58:37.434925",
     "exception": false,
     "start_time": "2022-04-15T16:58:37.398281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "@tf.function\n",
    "def input_image_loader(image):\n",
    "    image_norm = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    \n",
    "    # The IMG_H & IMG_W are constants that will be read only once during graph tracing step\n",
    "    image_norm = tf.image.resize(image_norm, size=[IMG_H, IMG_W])\n",
    "    \n",
    "\n",
    "    aug_image_1 = preprocess_image(image = image_norm, \n",
    "                                    height = IMG_H, \n",
    "                                    width  = IMG_W, \n",
    "                                    cjs = color_jitter_strength,\n",
    "                                    m_obj_cov = minimum_object_coverage,\n",
    "                                    a_range = area_range)\n",
    "\n",
    "    aug_image_2 = preprocess_image(image = image_norm, \n",
    "                                    height = IMG_H, \n",
    "                                    width  = IMG_W, \n",
    "                                    cjs = color_jitter_strength,\n",
    "                                    m_obj_cov = minimum_object_coverage,\n",
    "                                    a_range = area_range)\n",
    "    # view 1 & view 2\n",
    "    return aug_image_1, aug_image_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f12bb64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T16:58:37.498452Z",
     "iopub.status.busy": "2022-04-15T16:58:37.497563Z",
     "iopub.status.idle": "2022-04-15T16:58:37.508366Z",
     "shell.execute_reply": "2022-04-15T16:58:37.507909Z",
     "shell.execute_reply.started": "2022-04-15T16:21:50.126897Z"
    },
    "papermill": {
     "duration": 0.045969,
     "end_time": "2022-04-15T16:58:37.508470",
     "exception": false,
     "start_time": "2022-04-15T16:58:37.462501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# adding l2 reg\n",
    "def add_regularization(model, regularizer = tf.keras.regularizers.l2(0.0001)):\n",
    "    \"\"\"\n",
    "    Helper function to add l2 regularisation to each layer of a either a preTrained or \n",
    "    randomly initialised built in model\n",
    "    Arguments:\n",
    "        model : (keras.model) input model \n",
    "        regularizer : ( tf.keras.regularizers.l2) object from keras that defines a l2 regularizer\n",
    "    Returns:\n",
    "        model : all layers contain the \"regularizer\" object & incase we pass a pretrained model then the \n",
    "                original weights are preserved\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(regularizer, tf.keras.regularizers.Regularizer):\n",
    "        print(\"Regularizer must be a subclass of tf.keras.regularizers.Regularizer\")\n",
    "        return model\n",
    "\n",
    "    for layer in model.layers:\n",
    "        for attr in ['kernel_regularizer']:\n",
    "            if hasattr(layer, attr):\n",
    "                setattr(layer, attr, regularizer)\n",
    "\n",
    "    # When we change the layers attributes, the change only happens in the model config file\n",
    "    model_json = model.to_json()\n",
    "\n",
    "    # Save the weights before reloading the model.\n",
    "    tmp_weights_path = os.path.join(tempfile.gettempdir(), 'tmp_weights.h5')\n",
    "    model.save_weights(tmp_weights_path)\n",
    "\n",
    "    # load the model from the config\n",
    "    model = tf.keras.models.model_from_json(model_json)\n",
    "    \n",
    "    # Reload the model weights\n",
    "    model.load_weights(tmp_weights_path, by_name=True)\n",
    "    return model\n",
    "\n",
    "# Architecture utils\n",
    "# def get_simclr(hidden_1, \n",
    "#                hidden_2, \n",
    "#                l2_penalty = 10e-6):\n",
    "def simclr_model(input_shape):\n",
    "    \"\"\"\n",
    "    Main function to define the entire network backbone to train\n",
    "    Arguments:\n",
    "        hidden_1-2 : (int) variable to define number of neurons in the projection head dense layer\n",
    "        l2_penalty : (float) to define the amount of l2 penalty applied to each layer's weights\n",
    "    Returns:\n",
    "        final_model : (tf.keras.Model) final model that will be trained \n",
    "    \"\"\"\n",
    "    \n",
    "#     # encoder network\n",
    "#     base_model = tf.keras.applications.ResNet50V2(include_top = False, \n",
    "#                                                   weights = None, \n",
    "#                                                   input_shape = (IMG_H, IMG_W, 3))\n",
    "    \n",
    "#     # defining l2 regularization\n",
    "#     regularizer = tf.keras.regularizers.l2(l2_penalty)\n",
    "#     reg_base_model = add_regularization(base_model,regularizer)\n",
    "#     reg_base_model.trainable = True\n",
    "\n",
    "#     # Joining the entire pipeline using functional API\n",
    "#     inputs = Input((IMG_H, IMG_W, 3))\n",
    "#     h = reg_base_model(inputs, training=True)\n",
    "#     h = GlobalAveragePooling2D()(h)\n",
    "    \n",
    "#     # Non linear projection layer to improve the quality of embeddings being produced\n",
    "#     projection_1 = Dense(hidden_1, kernel_regularizer = regularizer)(h)\n",
    "#     projection_1 = tf.keras.layers.BatchNormalization()(projection_1)\n",
    "#     projection_1 = Activation(\"relu\")(projection_1)\n",
    "#     projection_2 = Dense(hidden_2, kernel_regularizer = regularizer)(projection_1)\n",
    "#     projection_2 = tf.keras.layers.BatchNormalization()(projection_2)\n",
    "#     projection_2 = Activation(\"relu\")(projection_2)\n",
    "\n",
    "    input_img = tf.keras.Input(shape = input_shape)\n",
    "    \n",
    "    Z1 = tfl.Conv2D(16, kernel_size=3, strides=1, padding='same', activation='relu')(input_img)\n",
    "    B1 = tfl.BatchNormalization(axis=-1)(Z1)\n",
    "    Z2 = tfl.Conv2D(16, kernel_size=3, strides=1, padding='same', activation='relu')(B1)\n",
    "    B2 = tfl.BatchNormalization(axis=-1)(Z2)\n",
    "    P1 = tfl.MaxPool2D(pool_size=2, strides=2, padding='valid')(B2)\n",
    "    D1 = tfl.Dropout(0.25)(P1)\n",
    "    # (16, 16, 16)\n",
    "    \n",
    "    Z3 = tfl.Conv2D(32, kernel_size=2, strides=1, padding='valid', activation='relu')(D1)\n",
    "    B3 = tfl.BatchNormalization(axis=-1)(Z3)\n",
    "    Z4 = tfl.Conv2D(32, kernel_size=2, strides=1, padding='valid', activation='relu')(B3)\n",
    "    B4 = tfl.BatchNormalization(axis=-1)(Z4)\n",
    "    P2 = tfl.MaxPool2D(pool_size=2, strides=2, padding='valid')(B4)\n",
    "    D2 = tfl.Dropout(0.25)(P2)\n",
    "    # (7, 7, 32)\n",
    "    \n",
    "    F1 = tfl.Flatten()(D2)\n",
    "    Den1 = tfl.Dense(256, activation='relu')(F1)\n",
    "    Drop1 = tfl.Dropout(0.25)(Den1)\n",
    "    Den2 = tfl.Dense(64, activation='relu')(Drop1)\n",
    "    Drop2 = tfl.Dropout(0.25)(Den2)\n",
    "    outputs = tfl.Dense(10, activation='softmax')(Drop2)\n",
    "    \n",
    "    model = tf.keras.Model(inputs = input_img, outputs = outputs)\n",
    "    return model\n",
    "\n",
    "    # Final Model\n",
    "    final_model = Model(inputs, outputs)\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7079dbfd",
   "metadata": {
    "papermill": {
     "duration": 0.027394,
     "end_time": "2022-04-15T16:58:37.563215",
     "exception": false,
     "start_time": "2022-04-15T16:58:37.535821",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train Step Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57566bc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T16:58:37.622956Z",
     "iopub.status.busy": "2022-04-15T16:58:37.622145Z",
     "iopub.status.idle": "2022-04-15T16:58:37.624073Z",
     "shell.execute_reply": "2022-04-15T16:58:37.624485Z",
     "shell.execute_reply.started": "2022-04-15T16:21:50.150376Z"
    },
    "papermill": {
     "duration": 0.033389,
     "end_time": "2022-04-15T16:58:37.624622",
     "exception": false,
     "start_time": "2022-04-15T16:58:37.591233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mask to remove positive examples from the batch of negative samples\n",
    "# negative_mask = helpers.get_negative_mask(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07d93f7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T16:58:37.696464Z",
     "iopub.status.busy": "2022-04-15T16:58:37.695566Z",
     "iopub.status.idle": "2022-04-15T16:58:37.708585Z",
     "shell.execute_reply": "2022-04-15T16:58:37.710236Z",
     "shell.execute_reply.started": "2022-04-15T16:21:50.163293Z"
    },
    "papermill": {
     "duration": 0.057989,
     "end_time": "2022-04-15T16:58:37.710444",
     "exception": false,
     "start_time": "2022-04-15T16:58:37.652455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(xis, \n",
    "               xjs, \n",
    "               model, \n",
    "               optimizer, \n",
    "               criterion, \n",
    "               temperature):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        zis = model(xis)\n",
    "        zjs = model(xjs)\n",
    "\n",
    "        # normalize projection feature vectors : onto a unit hypersphere\n",
    "        zis = tf.math.l2_normalize(zis, axis=1)\n",
    "        zjs = tf.math.l2_normalize(zjs, axis=1)\n",
    "          \n",
    "        # calculating over set of all positive pairs ( computing all numerators of softmax)\n",
    "        l_pos = sim_func_dim1(zis, zjs)\n",
    "        l_pos = tf.reshape(l_pos, (BATCH_SIZE, 1))\n",
    "\n",
    "        # temperature scaling\n",
    "        l_pos /= temperature\n",
    "        \n",
    "        # make the batch dimension 2*n\n",
    "        negatives = tf.concat([zjs, zis], axis=0)\n",
    "\n",
    "        loss = 0\n",
    "        for positives in [zis, zjs]:\n",
    "            # computing similarity with a data point & all the possible negatives\n",
    "            l_neg = sim_func_dim2(positives, negatives)\n",
    "            \n",
    "            # since each data point is its own class\n",
    "            labels = tf.zeros(BATCH_SIZE, dtype=tf.int32)\n",
    "            \n",
    "            # using the negative mask to remove itself & its positve counterpart to compute negative sim\n",
    "            l_neg = tf.boolean_mask(l_neg, negative_mask)\n",
    "            l_neg = tf.reshape(l_neg, (BATCH_SIZE, -1))\n",
    "\n",
    "            # temperature scaling\n",
    "            l_neg /= temperature\n",
    "\n",
    "            logits = tf.concat([l_pos, l_neg], axis=1) \n",
    "            loss += criterion(y_pred = logits, y_true = labels)\n",
    "\n",
    "        # since for every data point including its augmentation we compute the loss thus divide by 2*BatchSize\n",
    "        loss = loss / (2 * BATCH_SIZE)\n",
    "    \n",
    "    # Compute & apply the gradients on traininable paramters of the model\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    # updated model along with gradients so that we can visualise them on tensorboard.\n",
    "    return loss, gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "076f83ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T16:58:37.795110Z",
     "iopub.status.busy": "2022-04-15T16:58:37.785967Z",
     "iopub.status.idle": "2022-04-15T16:58:37.796951Z",
     "shell.execute_reply": "2022-04-15T16:58:37.797387Z",
     "shell.execute_reply.started": "2022-04-15T16:21:50.180446Z"
    },
    "papermill": {
     "duration": 0.049788,
     "end_time": "2022-04-15T16:58:37.797526",
     "exception": false,
     "start_time": "2022-04-15T16:58:37.747738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_simclr(model, \n",
    "                 train_dataset, \n",
    "                 optimizer, \n",
    "                 criterion,\n",
    "                 temperature=0.1, \n",
    "                 epochs=100,\n",
    "                 num_train_samples_viz = 5,\n",
    "                 num_test_samples_viz = 2):\n",
    "    \n",
    "    \"\"\"\n",
    "      Training the model function\n",
    "    \"\"\"\n",
    "    \n",
    " \n",
    "    print(\"Starting training procedure .... : \")\n",
    "    print(\"Number of steps per epoch : \",len(train_dataset))\n",
    "    \n",
    "    # To measure per epoch time taken\n",
    "    t_start = time.time()\n",
    "    \n",
    "    # Visualisation lists\n",
    "    lr_epoch = []\n",
    "    epoch_wise_loss = []\n",
    "\n",
    "    for epoch in range(0, epochs):\n",
    "        \n",
    "        # Reset loss collection each step\n",
    "        step_wise_loss = []\n",
    "\n",
    "        # Number of grad descent steps in 1 epoch\n",
    "        num_train_steps = len(train_dataset) \n",
    "\n",
    "        # Picking up random batches & taking first image for input check\n",
    "        random_batches_train = random.sample(range(len(train_dataset)),num_train_samples_viz)\n",
    "        cnt = 0\n",
    "\n",
    "        # Arrays for tensorboard visualisation\n",
    "        random_collection_train_sample_1 = []\n",
    "        random_collection_train_sample_2 = []\n",
    "        gradArray = None\n",
    "        loss = None \n",
    "\n",
    "        # Training loop\n",
    "        for image_batch in tqdm(train_dataset):\n",
    "\n",
    "            # Fetching both views for input\n",
    "            a = image_batch[0]\n",
    "            b = image_batch[1]\n",
    "\n",
    "            # Train one batch\n",
    "            loss, gradArray = train_step(a, b, model, optimizer, criterion, temperature)\n",
    "            step_wise_loss.append(loss)\n",
    "\n",
    "            # Check whether to take image from this batch or not\n",
    "            if cnt in random_batches_train:\n",
    "                random_collection_train_sample_1.append(image_batch[0][0])\n",
    "                random_collection_train_sample_2.append(image_batch[1][0])\n",
    "            cnt+=1\n",
    "        \n",
    "        # Average loss throughout the whole process\n",
    "        if not len(epoch_wise_loss):\n",
    "            epoch_wise_loss.append(np.mean(step_wise_loss))\n",
    "        else:\n",
    "            # Adding the mean of previous ones\n",
    "            mean_value = (np.sum(step_wise_loss) + epoch_wise_loss[-1]*(epoch)*num_train_steps)/((epoch+1)*num_train_steps)\n",
    "            epoch_wise_loss.append(mean_value)\n",
    "        \n",
    "        # Printing the loss progression\n",
    "        print(\"\\n epoch: {} | train loss: {:.8f} | lr : {} | {:.4f} mins\"\n",
    "              .format(epoch + 1,epoch_wise_loss[-1],optimizer._decayed_lr(tf.float32).numpy(), (time.time()-t_start)/60.0))    \n",
    "   \n",
    "        # Appending the value of learning rate for warmup + cosine decay visualisation\n",
    "        lr_epoch.append(optimizer._decayed_lr(tf.float32).numpy())\n",
    "        \n",
    "        # Tensorfboard visualisations\n",
    "        tf.summary.experimental.set_step(epoch)\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('LOSS', epoch_wise_loss[-1], step=epoch)\n",
    "            tf.summary.scalar('LEARNING RATE PROGRESSION', lr_epoch[-1], step = epoch)\n",
    "            tf.summary.image('VIEW 1', random_collection_train_sample_1, step=epoch)\n",
    "            tf.summary.image('VIEW 2', random_collection_train_sample_2, step=epoch)\n",
    "\n",
    "            # global variable defined later\n",
    "            for name in layer_names:\n",
    "                tf.summary.histogram(name+\"_gradients\",gradArray[layer_to_index[name]])\n",
    "                \n",
    "            for layer in model.layers:\n",
    "                for tl in layer.trainable_weights:\n",
    "                    if tl.name in layer_names:\n",
    "                        tf.summary.histogram(tl.name+\"_weights\",tl.numpy())\n",
    "        \n",
    "        # saving models \n",
    "        print(\"Saving Base Model.....\")\n",
    "        \n",
    "        # Saving the entire model for checkpointing reasons\n",
    "        model.save(\"./\" + modelNameStr + \".h5\")\n",
    "\n",
    "        # saving the state of optimizer\n",
    "        np.save(\"./\" + modelNameStr + \"_optimizer.npy\", optimizer.get_weights())\n",
    "\n",
    "    \n",
    "    return epoch_wise_loss, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a9d26e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T16:58:37.863077Z",
     "iopub.status.busy": "2022-04-15T16:58:37.861887Z",
     "iopub.status.idle": "2022-04-15T16:58:37.865419Z",
     "shell.execute_reply": "2022-04-15T16:58:37.864963Z",
     "shell.execute_reply.started": "2022-04-15T16:21:50.206512Z"
    },
    "papermill": {
     "duration": 0.039365,
     "end_time": "2022-04-15T16:58:37.865526",
     "exception": false,
     "start_time": "2022-04-15T16:58:37.826161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WarmUp(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self,\n",
    "               initial_learning_rate: float,\n",
    "               decay_schedule_fn: Callable,\n",
    "               warmup_steps: int,\n",
    "               power: float = 1.0,\n",
    "               name: str = None,):\n",
    "    \n",
    "    super().__init__()\n",
    "    self.initial_learning_rate = initial_learning_rate\n",
    "    self.warmup_steps = warmup_steps\n",
    "    self.power = power\n",
    "    self.decay_schedule_fn = decay_schedule_fn\n",
    "    self.name = name\n",
    "\n",
    "  def __call__(self, step):\n",
    "    with tf.name_scope(self.name or \"WarmUp\") as name:\n",
    "        # Implements polynomial warmup. i.e., if global_step < warmup_steps, the\n",
    "        # learning rate will be `global_step/num_warmup_steps * init_lr`.\n",
    "        global_step_float = tf.cast(step, tf.float32)\n",
    "        warmup_steps_float = tf.cast(self.warmup_steps, tf.float32)\n",
    "        warmup_percent_done = global_step_float / warmup_steps_float\n",
    "        warmup_learning_rate = self.initial_learning_rate * tf.math.pow(warmup_percent_done, self.power)\n",
    "        return tf.cond(\n",
    "            global_step_float < warmup_steps_float,\n",
    "            lambda: warmup_learning_rate,\n",
    "            lambda: self.decay_schedule_fn(step - self.warmup_steps),\n",
    "            name=name,\n",
    "        )\n",
    "\n",
    "  def get_config(self):\n",
    "    return {\n",
    "        \"initial_learning_rate\": self.initial_learning_rate,\n",
    "        \"decay_schedule_fn\": self.decay_schedule_fn,\n",
    "        \"warmup_steps\": self.warmup_steps,\n",
    "        \"power\": self.power,\n",
    "        \"name\": self.name,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94c4d63f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T16:58:37.927911Z",
     "iopub.status.busy": "2022-04-15T16:58:37.927258Z",
     "iopub.status.idle": "2022-04-15T16:58:37.933896Z",
     "shell.execute_reply": "2022-04-15T16:58:37.934431Z",
     "shell.execute_reply.started": "2022-04-15T16:21:50.223873Z"
    },
    "papermill": {
     "duration": 0.04118,
     "end_time": "2022-04-15T16:58:37.934556",
     "exception": false,
     "start_time": "2022-04-15T16:58:37.893376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decay Steps :  200030\n"
     ]
    }
   ],
   "source": [
    "# Defining the loss function\n",
    "criterion = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, \n",
    "                                                          reduction=tf.keras.losses.Reduction.SUM)\n",
    "# Number of epochs\n",
    "tot_epochs = 5\n",
    "\n",
    "# temperature in NTXent\n",
    "loss_temp = 0.2\n",
    "\n",
    "# Defining the SimCLR model\n",
    "# Differentiating models via their hyper-parameter values\n",
    "# simclr_2 = simclr_model((32, 32))\n",
    "\n",
    "\n",
    "# optimiser decay schedule\n",
    "decay_steps = (len(df_train))*tot_epochs\n",
    "warmup_steps = (len(df_train))*10\n",
    "initial_lr = 0.5e-3\n",
    "\n",
    "# Cosine decay function\n",
    "lr_decayed_fn = tf.keras.experimental.CosineDecay(initial_learning_rate = initial_lr, \n",
    "                                                  decay_steps = decay_steps)\n",
    "cosine_with_warmUp = WarmUp(initial_learning_rate = initial_lr,\n",
    "                            decay_schedule_fn = lr_decayed_fn,\n",
    "                            warmup_steps = warmup_steps)\n",
    "\n",
    "print(\"Decay Steps : \",decay_steps)\n",
    "optimizer = tf.keras.optimizers.Adam(cosine_with_warmUp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b45f8aa8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T16:58:37.994669Z",
     "iopub.status.busy": "2022-04-15T16:58:37.994136Z",
     "iopub.status.idle": "2022-04-15T16:58:38.315452Z",
     "shell.execute_reply": "2022-04-15T16:58:38.314690Z",
     "shell.execute_reply.started": "2022-04-15T16:21:50.237870Z"
    },
    "papermill": {
     "duration": 0.353134,
     "end_time": "2022-04-15T16:58:38.315599",
     "exception": false,
     "start_time": "2022-04-15T16:58:37.962465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 32, 32, 16)        448       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 16)        2320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 16)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 15, 32)        2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 15, 15, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               401664    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 428,122\n",
      "Trainable params: 427,930\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "simclr = simclr_model((32, 32, 3))\n",
    "simclr.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "simclr.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2ab688",
   "metadata": {
    "papermill": {
     "duration": 0.029716,
     "end_time": "2022-04-15T16:58:38.374952",
     "exception": false,
     "start_time": "2022-04-15T16:58:38.345236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training SimCLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49b50d00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T16:58:38.439830Z",
     "iopub.status.busy": "2022-04-15T16:58:38.438847Z",
     "iopub.status.idle": "2022-04-15T16:58:40.675881Z",
     "shell.execute_reply": "2022-04-15T16:58:40.675393Z",
     "shell.execute_reply.started": "2022-04-15T16:21:50.385023Z"
    },
    "papermill": {
     "duration": 2.271309,
     "end_time": "2022-04-15T16:58:40.676012",
     "exception": false,
     "start_time": "2022-04-15T16:58:38.404703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Composing the Train Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((df_train, y_train_oh)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aace6a3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T16:58:40.747727Z",
     "iopub.status.busy": "2022-04-15T16:58:40.746303Z",
     "iopub.status.idle": "2022-04-15T17:19:45.039878Z",
     "shell.execute_reply": "2022-04-15T17:19:45.039102Z",
     "shell.execute_reply.started": "2022-04-15T16:21:52.556466Z"
    },
    "papermill": {
     "duration": 1264.334766,
     "end_time": "2022-04-15T17:19:45.040197",
     "exception": false,
     "start_time": "2022-04-15T16:58:40.705431",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1251/1251 [==============================] - 13s 5ms/step - loss: 1.7257 - accuracy: 0.3734\n",
      "Epoch 2/10\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 1.3237 - accuracy: 0.5331\n",
      "Epoch 3/10\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 1.1604 - accuracy: 0.5983\n",
      "Epoch 4/10\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 1.0609 - accuracy: 0.6343\n",
      "Epoch 5/10\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.9761 - accuracy: 0.6630\n",
      "Epoch 6/10\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.9113 - accuracy: 0.6832\n",
      "Epoch 7/10\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.8601 - accuracy: 0.7017\n",
      "Epoch 8/10\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.8139 - accuracy: 0.7163\n",
      "Epoch 9/10\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.7706 - accuracy: 0.7320\n",
      "Epoch 10/10\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.7431 - accuracy: 0.7418\n",
      "For  10  Epochs:\n",
      "Log-loss for Train Dataset =  0.5105488669794773\n",
      "Log-loss for Test Dataset =  0.7945075802420395\n",
      "Accuracy for Train Dataset =  0.8186272059191121\n",
      "Accuracy for Test Dataset =  0.7286\n",
      "\n",
      "Epoch 1/20\n",
      "1251/1251 [==============================] - 7s 5ms/step - loss: 1.7076 - accuracy: 0.3879\n",
      "Epoch 2/20\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 1.3023 - accuracy: 0.5414\n",
      "Epoch 3/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 1.1386 - accuracy: 0.6014\n",
      "Epoch 4/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 1.0321 - accuracy: 0.6414\n",
      "Epoch 5/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.9485 - accuracy: 0.6687\n",
      "Epoch 6/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.8815 - accuracy: 0.6945\n",
      "Epoch 7/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.8317 - accuracy: 0.7124\n",
      "Epoch 8/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.7868 - accuracy: 0.7269\n",
      "Epoch 9/20\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.7510 - accuracy: 0.7391\n",
      "Epoch 10/20\n",
      "1251/1251 [==============================] - 7s 5ms/step - loss: 0.7179 - accuracy: 0.7507\n",
      "Epoch 11/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.6806 - accuracy: 0.7652\n",
      "Epoch 12/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.6636 - accuracy: 0.7699\n",
      "Epoch 13/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.6292 - accuracy: 0.7817\n",
      "Epoch 14/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.6059 - accuracy: 0.7859\n",
      "Epoch 15/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.5869 - accuracy: 0.7964\n",
      "Epoch 16/20\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.5719 - accuracy: 0.8034\n",
      "Epoch 17/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.5513 - accuracy: 0.8092\n",
      "Epoch 18/20\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.5379 - accuracy: 0.8126\n",
      "Epoch 19/20\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.5293 - accuracy: 0.8154\n",
      "Epoch 20/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.5075 - accuracy: 0.8243\n",
      "For  20  Epochs:\n",
      "Log-loss for Train Dataset =  0.2797775676229489\n",
      "Log-loss for Test Dataset =  0.7871881479694942\n",
      "Accuracy for Train Dataset =  0.9036144578313253\n",
      "Accuracy for Test Dataset =  0.7574\n",
      "\n",
      "Epoch 1/30\n",
      "1251/1251 [==============================] - 7s 5ms/step - loss: 1.7238 - accuracy: 0.3707\n",
      "Epoch 2/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 1.2793 - accuracy: 0.5487\n",
      "Epoch 3/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 1.0952 - accuracy: 0.6208\n",
      "Epoch 4/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 1.0011 - accuracy: 0.6556\n",
      "Epoch 5/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.9229 - accuracy: 0.6824\n",
      "Epoch 6/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.8680 - accuracy: 0.7019\n",
      "Epoch 7/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.8237 - accuracy: 0.7184\n",
      "Epoch 8/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.7770 - accuracy: 0.7337\n",
      "Epoch 9/30\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.7442 - accuracy: 0.7396\n",
      "Epoch 10/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.7103 - accuracy: 0.7534\n",
      "Epoch 11/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.6788 - accuracy: 0.7630\n",
      "Epoch 12/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.6587 - accuracy: 0.7711\n",
      "Epoch 13/30\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.6321 - accuracy: 0.7823\n",
      "Epoch 14/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.6115 - accuracy: 0.7915\n",
      "Epoch 15/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.5847 - accuracy: 0.7979\n",
      "Epoch 16/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.5740 - accuracy: 0.8012\n",
      "Epoch 17/30\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.5594 - accuracy: 0.8075\n",
      "Epoch 18/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.5421 - accuracy: 0.8132\n",
      "Epoch 19/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5319 - accuracy: 0.8158\n",
      "Epoch 20/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 0.5194 - accuracy: 0.8188\n",
      "Epoch 21/30\n",
      "1251/1251 [==============================] - 8s 6ms/step - loss: 0.5006 - accuracy: 0.8266\n",
      "Epoch 22/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.4906 - accuracy: 0.8301\n",
      "Epoch 23/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.4834 - accuracy: 0.8343\n",
      "Epoch 24/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.4684 - accuracy: 0.8382\n",
      "Epoch 25/30\n",
      "1251/1251 [==============================] - 7s 5ms/step - loss: 0.4621 - accuracy: 0.8407\n",
      "Epoch 26/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.4484 - accuracy: 0.8468\n",
      "Epoch 27/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.4451 - accuracy: 0.8470\n",
      "Epoch 28/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.4362 - accuracy: 0.8493\n",
      "Epoch 29/30\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.4285 - accuracy: 0.8530\n",
      "Epoch 30/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.4193 - accuracy: 0.8565\n",
      "For  30  Epochs:\n",
      "Log-loss for Train Dataset =  0.174615679955807\n",
      "Log-loss for Test Dataset =  0.7651637724221723\n",
      "Accuracy for Train Dataset =  0.9412338149277608\n",
      "Accuracy for Test Dataset =  0.7745\n",
      "\n",
      "Epoch 1/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 1.7001 - accuracy: 0.3828\n",
      "Epoch 2/40\n",
      "1251/1251 [==============================] - 7s 5ms/step - loss: 1.3165 - accuracy: 0.5333\n",
      "Epoch 3/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 1.1414 - accuracy: 0.6035\n",
      "Epoch 4/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 1.0291 - accuracy: 0.6445\n",
      "Epoch 5/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.9451 - accuracy: 0.6705\n",
      "Epoch 6/40\n",
      "1251/1251 [==============================] - 7s 5ms/step - loss: 0.8786 - accuracy: 0.6954\n",
      "Epoch 7/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.8369 - accuracy: 0.7108\n",
      "Epoch 8/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.7927 - accuracy: 0.7251\n",
      "Epoch 9/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.7534 - accuracy: 0.7401\n",
      "Epoch 10/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.7223 - accuracy: 0.7504\n",
      "Epoch 11/40\n",
      "1251/1251 [==============================] - 7s 5ms/step - loss: 0.6921 - accuracy: 0.7598\n",
      "Epoch 12/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.6712 - accuracy: 0.7674\n",
      "Epoch 13/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.6423 - accuracy: 0.7754\n",
      "Epoch 14/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.6212 - accuracy: 0.7853\n",
      "Epoch 15/40\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.6042 - accuracy: 0.7907\n",
      "Epoch 16/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.5768 - accuracy: 0.7988\n",
      "Epoch 17/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.5657 - accuracy: 0.8017\n",
      "Epoch 18/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.5468 - accuracy: 0.8119\n",
      "Epoch 19/40\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.5381 - accuracy: 0.8152\n",
      "Epoch 20/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.5220 - accuracy: 0.8183\n",
      "Epoch 21/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.5085 - accuracy: 0.8227\n",
      "Epoch 22/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.4971 - accuracy: 0.8277\n",
      "Epoch 23/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.4795 - accuracy: 0.8329\n",
      "Epoch 24/40\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.4751 - accuracy: 0.8362\n",
      "Epoch 25/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.4619 - accuracy: 0.8411\n",
      "Epoch 26/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.4565 - accuracy: 0.8433\n",
      "Epoch 27/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.4464 - accuracy: 0.8455\n",
      "Epoch 28/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.4325 - accuracy: 0.8510\n",
      "Epoch 29/40\n",
      "1251/1251 [==============================] - 8s 6ms/step - loss: 0.4267 - accuracy: 0.8518\n",
      "Epoch 30/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.4147 - accuracy: 0.8568\n",
      "Epoch 31/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.4148 - accuracy: 0.8574\n",
      "Epoch 32/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.4046 - accuracy: 0.8598\n",
      "Epoch 33/40\n",
      "1251/1251 [==============================] - 7s 5ms/step - loss: 0.3973 - accuracy: 0.8632\n",
      "Epoch 34/40\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.3951 - accuracy: 0.8644\n",
      "Epoch 35/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.3888 - accuracy: 0.8666\n",
      "Epoch 36/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.3854 - accuracy: 0.8659\n",
      "Epoch 37/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.3773 - accuracy: 0.8695\n",
      "Epoch 38/40\n",
      "1251/1251 [==============================] - 8s 6ms/step - loss: 0.3693 - accuracy: 0.8740\n",
      "Epoch 39/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.3635 - accuracy: 0.8747\n",
      "Epoch 40/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.3608 - accuracy: 0.8770\n",
      "For  40  Epochs:\n",
      "Log-loss for Train Dataset =  0.13320680218779302\n",
      "Log-loss for Test Dataset =  0.8286101706142645\n",
      "Accuracy for Train Dataset =  0.9562565615157727\n",
      "Accuracy for Test Dataset =  0.7706\n",
      "\n",
      "Epoch 1/50\n",
      "1251/1251 [==============================] - 7s 5ms/step - loss: 1.7178 - accuracy: 0.3743\n",
      "Epoch 2/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 1.3058 - accuracy: 0.5373\n",
      "Epoch 3/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 1.1262 - accuracy: 0.6091\n",
      "Epoch 4/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 1.0084 - accuracy: 0.6521\n",
      "Epoch 5/50\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.9325 - accuracy: 0.6768\n",
      "Epoch 6/50\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.8675 - accuracy: 0.7002\n",
      "Epoch 7/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.8193 - accuracy: 0.7167\n",
      "Epoch 8/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.7800 - accuracy: 0.7307\n",
      "Epoch 9/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.7383 - accuracy: 0.7458\n",
      "Epoch 10/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.7101 - accuracy: 0.7545\n",
      "Epoch 11/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.6801 - accuracy: 0.7629\n",
      "Epoch 12/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.6544 - accuracy: 0.7722\n",
      "Epoch 13/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.6330 - accuracy: 0.7806\n",
      "Epoch 14/50\n",
      "1251/1251 [==============================] - 8s 6ms/step - loss: 0.6070 - accuracy: 0.7877\n",
      "Epoch 15/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.5892 - accuracy: 0.7952\n",
      "Epoch 16/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.5748 - accuracy: 0.8010\n",
      "Epoch 17/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.5575 - accuracy: 0.8075\n",
      "Epoch 18/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.5427 - accuracy: 0.8113\n",
      "Epoch 19/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.5217 - accuracy: 0.8193\n",
      "Epoch 20/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.5091 - accuracy: 0.8212\n",
      "Epoch 21/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.5021 - accuracy: 0.8255\n",
      "Epoch 22/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.4903 - accuracy: 0.8295\n",
      "Epoch 23/50\n",
      "1251/1251 [==============================] - 8s 6ms/step - loss: 0.4788 - accuracy: 0.8345\n",
      "Epoch 24/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.4682 - accuracy: 0.8381\n",
      "Epoch 25/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.4551 - accuracy: 0.8415\n",
      "Epoch 26/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.4412 - accuracy: 0.8485\n",
      "Epoch 27/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.4341 - accuracy: 0.8492\n",
      "Epoch 28/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.4299 - accuracy: 0.8511\n",
      "Epoch 29/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.4232 - accuracy: 0.8526\n",
      "Epoch 30/50\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.4120 - accuracy: 0.8569\n",
      "Epoch 31/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.4074 - accuracy: 0.8597\n",
      "Epoch 32/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.4022 - accuracy: 0.8614\n",
      "Epoch 33/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.3969 - accuracy: 0.8626\n",
      "Epoch 34/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.3860 - accuracy: 0.8658\n",
      "Epoch 35/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.3794 - accuracy: 0.8715\n",
      "Epoch 36/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.3736 - accuracy: 0.8721\n",
      "Epoch 37/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.3698 - accuracy: 0.8741\n",
      "Epoch 38/50\n",
      "1251/1251 [==============================] - 7s 5ms/step - loss: 0.3641 - accuracy: 0.8729\n",
      "Epoch 39/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.3563 - accuracy: 0.8775\n",
      "Epoch 40/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.3530 - accuracy: 0.8784\n",
      "Epoch 41/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.3472 - accuracy: 0.8793\n",
      "Epoch 42/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.3535 - accuracy: 0.8796\n",
      "Epoch 43/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.3378 - accuracy: 0.8859\n",
      "Epoch 44/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.3420 - accuracy: 0.8831\n",
      "Epoch 45/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.3318 - accuracy: 0.8854\n",
      "Epoch 46/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.3313 - accuracy: 0.8862\n",
      "Epoch 47/50\n",
      "1251/1251 [==============================] - 8s 6ms/step - loss: 0.3259 - accuracy: 0.8892\n",
      "Epoch 48/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.3217 - accuracy: 0.8899\n",
      "Epoch 49/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.3158 - accuracy: 0.8925\n",
      "Epoch 50/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.3131 - accuracy: 0.8925\n",
      "For  50  Epochs:\n",
      "Log-loss for Train Dataset =  0.10879368283464749\n",
      "Log-loss for Test Dataset =  0.8460932984898265\n",
      "Accuracy for Train Dataset =  0.9655551667249912\n",
      "Accuracy for Test Dataset =  0.7681\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = [10, 20, 30, 40, 50]\n",
    "train_loss, test_loss, train_acc, test_acc = [], [], [], []\n",
    "\n",
    "for epochs in num_epochs:\n",
    "    # Training the Model\n",
    "    simclr = simclr_model((32, 32, 3))\n",
    "    simclr.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "    simclr.fit(train_dataset, epochs = epochs)\n",
    "    \n",
    "    # Predicting on the Train/Test Datasets\n",
    "    preds_train = simclr.predict(df_train)\n",
    "    preds_test = simclr.predict(df_test)\n",
    "\n",
    "    # Finding the Predicted Classes\n",
    "    cls_train = np.argmax(preds_train, axis = 1)\n",
    "    cls_test = np.argmax(preds_test, axis = 1)\n",
    "    \n",
    "    # Finding the Train/Test set Loss\n",
    "    train_loss.append(log_loss(y_train_oh, preds_train))\n",
    "    test_loss.append(log_loss(y_test_oh, preds_test))\n",
    "    train_acc.append(accuracy_score(y_train, cls_train))\n",
    "    test_acc.append(accuracy_score(y_test, cls_test))\n",
    "    \n",
    "    print(\"For \", epochs, \" Epochs:\")\n",
    "    print(\"Log-loss for Train Dataset = \", train_loss[-1])\n",
    "    print(\"Log-loss for Test Dataset = \", test_loss[-1])\n",
    "    print(\"Accuracy for Train Dataset = \", train_acc[-1])\n",
    "    print(\"Accuracy for Test Dataset = \", test_acc[-1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "927f4eb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T17:19:56.845772Z",
     "iopub.status.busy": "2022-04-15T17:19:56.845176Z",
     "iopub.status.idle": "2022-04-15T17:24:12.078044Z",
     "shell.execute_reply": "2022-04-15T17:24:12.077489Z",
     "shell.execute_reply.started": "2022-04-15T16:26:23.333840Z"
    },
    "papermill": {
     "duration": 260.658643,
     "end_time": "2022-04-15T17:24:12.078225",
     "exception": false,
     "start_time": "2022-04-15T17:19:51.419582",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1251/1251 [==============================] - 7s 5ms/step - loss: 1.7029 - accuracy: 0.3800\n",
      "Epoch 2/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 1.3121 - accuracy: 0.5358\n",
      "Epoch 3/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 1.1588 - accuracy: 0.5974\n",
      "Epoch 4/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 1.0609 - accuracy: 0.6348\n",
      "Epoch 5/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.9783 - accuracy: 0.6639\n",
      "Epoch 6/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.9106 - accuracy: 0.6845\n",
      "Epoch 7/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.8551 - accuracy: 0.7045\n",
      "Epoch 8/30\n",
      "1251/1251 [==============================] - 8s 6ms/step - loss: 0.8160 - accuracy: 0.7205\n",
      "Epoch 9/30\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 0.7724 - accuracy: 0.7328\n",
      "Epoch 10/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.7344 - accuracy: 0.7465\n",
      "Epoch 11/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.7056 - accuracy: 0.7554\n",
      "Epoch 12/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.6783 - accuracy: 0.7658\n",
      "Epoch 13/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.6536 - accuracy: 0.7758\n",
      "Epoch 14/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.6297 - accuracy: 0.7805\n",
      "Epoch 15/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.6088 - accuracy: 0.7882\n",
      "Epoch 16/30\n",
      "1251/1251 [==============================] - 7s 5ms/step - loss: 0.5809 - accuracy: 0.7979\n",
      "Epoch 17/30\n",
      "1251/1251 [==============================] - 8s 7ms/step - loss: 0.5716 - accuracy: 0.8036\n",
      "Epoch 18/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.5551 - accuracy: 0.8052\n",
      "Epoch 19/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.5427 - accuracy: 0.8099\n",
      "Epoch 20/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.5304 - accuracy: 0.8148\n",
      "Epoch 21/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.5112 - accuracy: 0.8221\n",
      "Epoch 22/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.4994 - accuracy: 0.8264\n",
      "Epoch 23/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.4896 - accuracy: 0.8297\n",
      "Epoch 24/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.4777 - accuracy: 0.8332\n",
      "Epoch 25/30\n",
      "1251/1251 [==============================] - 8s 6ms/step - loss: 0.4641 - accuracy: 0.8405\n",
      "Epoch 26/30\n",
      "1251/1251 [==============================] - 8s 6ms/step - loss: 0.4527 - accuracy: 0.8440\n",
      "Epoch 27/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.4489 - accuracy: 0.8441\n",
      "Epoch 28/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.4471 - accuracy: 0.8462\n",
      "Epoch 29/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 0.4329 - accuracy: 0.8496\n",
      "Epoch 30/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 0.4264 - accuracy: 0.8522\n"
     ]
    }
   ],
   "source": [
    "# Training the Model with the best hyper-parameter settings\n",
    "ind = np.argmax(test_acc)\n",
    "best_num_epochs = num_epochs[ind]\n",
    "simclr = simclr_model((32, 32, 3))\n",
    "simclr.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "simclr.fit(train_dataset, epochs = best_num_epochs)\n",
    "\n",
    "# Saving the model along with it's weights\n",
    "simclr.save('simclr_model.h5')\n",
    "\n",
    "# Predicting on the Train/Test Datasets\n",
    "preds_train = simclr.predict(df_train)\n",
    "preds_test = simclr.predict(df_test)\n",
    "\n",
    "# Finding the Predicted Classes\n",
    "cls_train = np.argmax(preds_train, axis = 1)\n",
    "cls_test = np.argmax(preds_test, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029f3b51",
   "metadata": {
    "papermill": {
     "duration": 6.891063,
     "end_time": "2022-04-15T17:24:26.001043",
     "exception": false,
     "start_time": "2022-04-15T17:24:19.109980",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41be5232",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T17:24:40.032152Z",
     "iopub.status.busy": "2022-04-15T17:24:40.031331Z",
     "iopub.status.idle": "2022-04-15T17:24:40.128438Z",
     "shell.execute_reply": "2022-04-15T17:24:40.129436Z",
     "shell.execute_reply.started": "2022-04-15T16:29:16.437412Z"
    },
    "papermill": {
     "duration": 7.505171,
     "end_time": "2022-04-15T17:24:40.129643",
     "exception": false,
     "start_time": "2022-04-15T17:24:32.624472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-loss for Train Dataset =  0.17580806362273133\n",
      "Log-loss for Test Dataset =  0.7489407274774214\n",
      "Accuracy for Train Dataset =  0.9449832525121232\n",
      "Accuracy for Test Dataset =  0.7697\n"
     ]
    }
   ],
   "source": [
    "# Finding the Train/Test set Loss\n",
    "print(\"Log-loss for Train Dataset = \", log_loss(y_train_oh, preds_train))\n",
    "print(\"Log-loss for Test Dataset = \", log_loss(y_test_oh, preds_test))\n",
    "print(\"Accuracy for Train Dataset = \", accuracy_score(y_train, cls_train))\n",
    "print(\"Accuracy for Test Dataset = \", accuracy_score(y_test, cls_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1630.615664,
   "end_time": "2022-04-15T17:24:49.880508",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-15T16:57:39.264844",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
