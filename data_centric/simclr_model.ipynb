{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a02eed4",
   "metadata": {
    "papermill": {
     "duration": 0.031784,
     "end_time": "2022-06-06T15:19:48.226895",
     "exception": false,
     "start_time": "2022-06-06T15:19:48.195111",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A Simple Framework for Contrastive Learning of Visual Representations\n",
    "\n",
    "- This notebook is to use the popular SimCLR framework in Tensorflow2.0 along with the strategies used in this paper ([SimCLR](https://arxiv.org/abs/2002.05709)), to learn generalised representations/features for images in a self supervised fashion on CIFAR10. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef42d32",
   "metadata": {
    "papermill": {
     "duration": 0.029753,
     "end_time": "2022-06-06T15:19:48.286710",
     "exception": false,
     "start_time": "2022-06-06T15:19:48.256957",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SimCLR :\n",
    "1. Development of composition of data augmentation strategies to generate pairs of positive input data points that serve as an important component for discriminative learning. This could be considered as the hero element of the whole framework because defining the class of augmentations is clearly responsible towards defining the features the model will consider 2 distinguish between 2 items & features it will be invariant to while trying to find similarities between 2 items.\n",
    " \n",
    "2. Introduction of a non linear projection head that creates a buffer between the output of the encoder network & the input to loss function. This was an elegant solution towards learning generalisable features.\n",
    " \n",
    "3. Use of InfoNCE loss on feature similarity. This loss was introduced in [Improved Deep Metric Learning with\n",
    "Multi-class N-pair Loss Objective](https://www.nec-labs.com/uploads/images/Department-Images/MediaAnalytics/papers/nips16_npairmetriclearning.pdf)  with a major contribution of a proposed objective function that allows joint comparison among more than 1 negative example & reduces the computational load of using only N pairs of examples, instead of (N+1)Ã—N. The authors of SimCLR build on this loss function with the added innovation of usage of normalised encoded vectors & a temperature parameter to penalise hard negative samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef2aaf72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T15:19:48.353357Z",
     "iopub.status.busy": "2022-06-06T15:19:48.351867Z",
     "iopub.status.idle": "2022-06-06T15:19:59.098692Z",
     "shell.execute_reply": "2022-06-06T15:19:59.097493Z",
     "shell.execute_reply.started": "2022-06-06T15:14:35.327687Z"
    },
    "papermill": {
     "duration": 10.780484,
     "end_time": "2022-06-06T15:19:59.098853",
     "exception": false,
     "start_time": "2022-06-06T15:19:48.318369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imutils\r\n",
      "  Downloading imutils-0.5.4.tar.gz (17 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hBuilding wheels for collected packages: imutils\r\n",
      "  Building wheel for imutils (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25860 sha256=944109d29344a38f54431959751eb3ab1eec3e9523a7c2032d9b1c32200ac7fc\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/86/d7/0a/4923351ed1cec5d5e24c1eaf8905567b02a0343b24aa873df2\r\n",
      "Successfully built imutils\r\n",
      "Installing collected packages: imutils\r\n",
      "Successfully installed imutils-0.5.4\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "! pip install imutils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba5413c",
   "metadata": {
    "papermill": {
     "duration": 0.032305,
     "end_time": "2022-06-06T15:19:59.164723",
     "exception": false,
     "start_time": "2022-06-06T15:19:59.132418",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22b436fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T15:19:59.243917Z",
     "iopub.status.busy": "2022-06-06T15:19:59.240615Z",
     "iopub.status.idle": "2022-06-06T15:20:09.334601Z",
     "shell.execute_reply": "2022-06-06T15:20:09.335609Z",
     "shell.execute_reply.started": "2022-06-06T15:18:14.821940Z"
    },
    "papermill": {
     "duration": 10.138815,
     "end_time": "2022-06-06T15:20:09.335817",
     "exception": false,
     "start_time": "2022-06-06T15:19:59.197002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version :  2.6.2\n"
     ]
    }
   ],
   "source": [
    "# Basic Imports \n",
    "\n",
    "# Model mathematics\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "\n",
    "# Plotting libraries\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# Utilities\n",
    "import datetime\n",
    "import os,sys\n",
    "import tempfile\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "sys.path.append(\"/kaggle/input/helper-files\")\n",
    "import random \n",
    "import gc\n",
    "import time\n",
    "import functools\n",
    "from imutils import paths\n",
    "from tqdm import tqdm\n",
    "from typing import Callable\n",
    "from sklearn.metrics import accuracy_score, log_loss, confusion_matrix, f1_score\n",
    "\n",
    "# SimCLR Losses (extra files, will be explained further in the notebook)\n",
    "from semi_super_augPipe import preprocess_image\n",
    "from losses import _dot_simililarity_dim1 as sim_func_dim1, _dot_simililarity_dim2 as sim_func_dim2\n",
    "import helpers\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "# Random seed fixation for experiment result repitition\n",
    "tf.random.set_seed(10)\n",
    "np.random.seed(10)\n",
    "\n",
    "print(\"Tensorflow version : \",tf.__version__)\n",
    "\n",
    "# In-order run function decorators in tf2.0\n",
    "tf.config.run_functions_eagerly(False)\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e404d1df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T15:20:09.451234Z",
     "iopub.status.busy": "2022-06-06T15:20:09.450387Z",
     "iopub.status.idle": "2022-06-06T15:20:09.452569Z",
     "shell.execute_reply": "2022-06-06T15:20:09.451937Z",
     "shell.execute_reply.started": "2022-06-06T15:14:57.639158Z"
    },
    "papermill": {
     "duration": 0.062882,
     "end_time": "2022-06-06T15:20:09.452725",
     "exception": false,
     "start_time": "2022-06-06T15:20:09.389843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting the seeds\n",
    "SEED = 0\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88502dbd",
   "metadata": {
    "papermill": {
     "duration": 0.053129,
     "end_time": "2022-06-06T15:20:09.559047",
     "exception": false,
     "start_time": "2022-06-06T15:20:09.505918",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Paths & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6ee80a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T15:20:09.674394Z",
     "iopub.status.busy": "2022-06-06T15:20:09.673637Z",
     "iopub.status.idle": "2022-06-06T15:20:09.675673Z",
     "shell.execute_reply": "2022-06-06T15:20:09.676052Z",
     "shell.execute_reply.started": "2022-06-06T15:14:57.651257Z"
    },
    "papermill": {
     "duration": 0.064215,
     "end_time": "2022-06-06T15:20:09.676215",
     "exception": false,
     "start_time": "2022-06-06T15:20:09.612000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# These are the usual ipython objects\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Defining a function to list the memory consumed\n",
    "# Only outputs variables taking at least 1MB space\n",
    "def list_storage(inp_dir):\n",
    "    # Get a sorted list of the objects and their sizes\n",
    "    vars_defined = [x for x in inp_dir if not x.startswith('_') and x not in sys.modules and x not in ipython_vars]\n",
    "    sto = sorted([(x, sys.getsizeof(globals().get(x))) for x in vars_defined], key=lambda x: x[1], reverse=True)\n",
    "    sto = [(x[0], str(round((x[1] / 2**20), 2)) + ' MB') for x in sto if x[1] >= 2**20]\n",
    "    print(tabulate(sto, headers = ['Variable', 'Storage (in MB)']))\n",
    "\n",
    "# In order to use this function, use the below line of code\n",
    "# list_storage(dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "806d235a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T15:20:09.745530Z",
     "iopub.status.busy": "2022-06-06T15:20:09.745010Z",
     "iopub.status.idle": "2022-06-06T15:20:35.659933Z",
     "shell.execute_reply": "2022-06-06T15:20:35.659407Z",
     "shell.execute_reply.started": "2022-06-06T15:14:57.662344Z"
    },
    "papermill": {
     "duration": 25.951209,
     "end_time": "2022-06-06T15:20:35.660077",
     "exception": false,
     "start_time": "2022-06-06T15:20:09.708868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing the Labelled Dataset\n",
    "df_train = pd.read_csv(\"../input/cifar10/train_lab_x.csv\")\n",
    "y_train = pd.read_csv(\"../input/cifar10/train_lab_y.csv\")\n",
    "\n",
    "# Importing the Test Dataset\n",
    "df_test = pd.read_csv(\"../input/cifar10/test_x.csv\")\n",
    "y_test = pd.read_csv(\"../input/cifar10/test_y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "205d74ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T15:20:35.738522Z",
     "iopub.status.busy": "2022-06-06T15:20:35.737374Z",
     "iopub.status.idle": "2022-06-06T15:20:38.848355Z",
     "shell.execute_reply": "2022-06-06T15:20:38.847920Z",
     "shell.execute_reply.started": "2022-06-06T15:15:25.768781Z"
    },
    "papermill": {
     "duration": 3.154162,
     "end_time": "2022-06-06T15:20:38.848478",
     "exception": false,
     "start_time": "2022-06-06T15:20:35.694316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40006, 3072) (40006, 1)\n",
      "(40006, 3, 32, 32)\n",
      "[0]\n",
      "(40006, 32, 32, 3) (40006, 10)\n",
      "(10000, 3072) (10000, 1)\n",
      "(10000, 3, 32, 32)\n",
      "(10000, 32, 32, 3) (10000, 10)\n",
      "Tensorflow version :  2.6.2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 108x108 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = np.array(df_train)\n",
    "y_train = np.array(y_train)\n",
    "print(df_train.shape, y_train.shape)\n",
    "\n",
    "# Reshaping the dataset\n",
    "df_train = np.reshape(df_train, (-1, 3, 32, 32))\n",
    "print(df_train.shape)\n",
    "\n",
    "# Visualizing a single image\n",
    "ind = 11\n",
    "example = df_train[ind, : , : , : ]\n",
    "example = example.transpose((1, 2, 0))\n",
    "plt.figure(figsize=(1.5, 1.5))\n",
    "plt.imshow(example)\n",
    "print(y_train[ind])\n",
    "\n",
    "# Creating a random permutation\n",
    "perm = np.random.permutation(df_train.shape[0])\n",
    "\n",
    "# Shuffling the training dataset\n",
    "df_train = df_train[perm, : , : , : ]\n",
    "y_train = y_train[perm]\n",
    "\n",
    "# Reshaping, rescaling and one-hot encoding\n",
    "df_train = np.transpose(np.array(df_train), (0, 2, 3, 1))\n",
    "df_train = df_train / 255\n",
    "y_train_oh = tf.one_hot(np.ravel(y_train), depth = 10)\n",
    "\n",
    "print(df_train.shape, y_train_oh.shape)\n",
    "\n",
    "df_test = np.array(df_test)\n",
    "y_test = np.array(y_test)\n",
    "print(df_test.shape, y_test.shape)\n",
    "\n",
    "# Reshaping the dataset\n",
    "df_test = np.reshape(df_test, (-1, 3, 32, 32))\n",
    "print(df_test.shape)\n",
    "\n",
    "# Reshaping, rescaling and one-hot encoding\n",
    "df_test = np.transpose(np.array(df_test), (0, 2, 3, 1))\n",
    "df_test = df_test / 255\n",
    "y_test_oh = tf.one_hot(np.ravel(y_test), depth = 10)\n",
    "print(df_test.shape, y_test_oh.shape)\n",
    "\n",
    "# Random seed fixation for experiment result repitition\n",
    "tf.random.set_seed(10)\n",
    "np.random.seed(10)\n",
    "\n",
    "print(\"Tensorflow version : \",tf.__version__)\n",
    "\n",
    "# In-order run function decorators in tf2.0\n",
    "tf.config.run_functions_eagerly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c63437e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T15:20:38.922050Z",
     "iopub.status.busy": "2022-06-06T15:20:38.921337Z",
     "iopub.status.idle": "2022-06-06T15:20:38.923338Z",
     "shell.execute_reply": "2022-06-06T15:20:38.923746Z",
     "shell.execute_reply.started": "2022-06-06T15:15:28.879579Z"
    },
    "papermill": {
     "duration": 0.040739,
     "end_time": "2022-06-06T15:20:38.923874",
     "exception": false,
     "start_time": "2022-06-06T15:20:38.883135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Image training properties \n",
    "IMG_H, IMG_W = (32, 32)\n",
    "\n",
    "# How powerful you want the colour augmentations to be\n",
    "color_jitter_strength = 0.3\n",
    "\n",
    "# Minimum crop area you want\n",
    "minimum_object_coverage = 0.7\n",
    "\n",
    "# Range of crop area\n",
    "area_range = (minimum_object_coverage, 1.0)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbdb636",
   "metadata": {
    "papermill": {
     "duration": 0.034279,
     "end_time": "2022-06-06T15:20:38.992437",
     "exception": false,
     "start_time": "2022-06-06T15:20:38.958158",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee77fcc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T15:20:39.068640Z",
     "iopub.status.busy": "2022-06-06T15:20:39.067855Z",
     "iopub.status.idle": "2022-06-06T15:20:39.070178Z",
     "shell.execute_reply": "2022-06-06T15:20:39.069781Z",
     "shell.execute_reply.started": "2022-06-06T15:15:28.891253Z"
    },
    "papermill": {
     "duration": 0.0435,
     "end_time": "2022-06-06T15:20:39.070280",
     "exception": false,
     "start_time": "2022-06-06T15:20:39.026780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cosine_sim_1d = tf.keras.losses.CosineSimilarity(axis=1, reduction=tf.keras.losses.Reduction.NONE)\n",
    "cosine_sim_2d = tf.keras.losses.CosineSimilarity(axis=2, reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "\n",
    "def _cosine_simililarity_dim1(x, y):\n",
    "    v = cosine_sim_1d(x, y)\n",
    "    return v\n",
    "\n",
    "\n",
    "def _cosine_simililarity_dim2(x, y):\n",
    "    # x shape: (N, 1, C)\n",
    "    # y shape: (1, 2N, C)\n",
    "    # v shape: (N, 2N)\n",
    "    v = cosine_sim_2d(tf.expand_dims(x, 1), tf.expand_dims(y, 0))\n",
    "    return v\n",
    "\n",
    "\n",
    "def _dot_simililarity_dim1(x, y):\n",
    "    # x shape: (N, 1, C)\n",
    "    # y shape: (N, C, 1)\n",
    "    # v shape: (N, 1, 1)\n",
    "    v = tf.matmul(tf.expand_dims(x, 1), tf.expand_dims(y, 2))\n",
    "    return v\n",
    "\n",
    "\n",
    "def _dot_simililarity_dim2(x, y):\n",
    "    v = tf.tensordot(tf.expand_dims(x, 1), tf.expand_dims(tf.transpose(y), 0), axes=2)\n",
    "    # x shape: (N, 1, C)\n",
    "    # y shape: (1, C, 2N)\n",
    "    # v shape: (N, 2N)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be83ee87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T15:20:39.157529Z",
     "iopub.status.busy": "2022-06-06T15:20:39.141755Z",
     "iopub.status.idle": "2022-06-06T15:20:39.184484Z",
     "shell.execute_reply": "2022-06-06T15:20:39.184068Z",
     "shell.execute_reply.started": "2022-06-06T15:15:28.906059Z"
    },
    "papermill": {
     "duration": 0.080152,
     "end_time": "2022-06-06T15:20:39.184603",
     "exception": false,
     "start_time": "2022-06-06T15:20:39.104451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "# Constants to control augmentation strength\n",
    "FLAGS_color_jitter_strength = 0.5\n",
    "FLAGS_min_object_covered = 0.3 # min area in the specified boudning box to cover\n",
    "FLAGS_area_range = (0.3, 1.0) # the cropped area must contain a fraction of the supplied image within this range\n",
    "\n",
    "# pipe for random rotations\n",
    "aug_layers = tf.keras.Sequential([tf.keras.layers.RandomRotation(0.1),])\n",
    "\n",
    "def set_aug_strength_value(cjs, m_obj_cov, a_range): \n",
    "    \"\"\"\n",
    "    Function to setup the value of the augmentation variables\n",
    "    Arguments:\n",
    "        cjs : (float) strength of color jittering to apply to vary the color histograms\n",
    "        m_obj_cov : (float)  min area in the specified boudning box to cover\n",
    "        a_range : (tuple) the cropped area must contain a fraction of the supplied image within this range\n",
    "    Returns :\n",
    "        None, only sets the values\n",
    "    \"\"\"\n",
    "    global FLAGS_color_jitter_strength\n",
    "    FLAGS_color_jitter_strength = cjs\n",
    "\n",
    "    global FLAGS_min_object_covered\n",
    "    FLAGS_min_object_covered = m_obj_cov\n",
    "\n",
    "    global FLAGS_area_range\n",
    "    FLAGS_area_range = a_range\n",
    "\n",
    "def random_apply(func, p, x):\n",
    "    \"\"\"\n",
    "    Function that acts as a wrapper to apply augmentations randomly\n",
    "    -> tf cond is a functions whose first argument is a predicate, when it is true then it returns first function (i.e. apply augmentation) else dont apply augmentation\n",
    "    -> tf less return true if arg1 < arg2 \n",
    "    -> tf random uniform picks a number between 0 & 1 randomly\n",
    "    Arguments:\n",
    "        func : (function) augmentation to apply randomly\n",
    "        p : (float) probability value [0-1]\n",
    "        x : (tf.image) input type\n",
    "    Returns:\n",
    "        image after applying the specified augmentation stochastically\n",
    "    \"\"\" \n",
    "    return tf.cond(tf.less(tf.random.uniform([], minval=0, maxval=1, dtype=tf.float32), tf.cast(p, tf.float32)),\n",
    "                    lambda: func(x),\n",
    "                    lambda: x)\n",
    "\n",
    "def distorted_bounding_box_crop(image, \n",
    "                                bbox, \n",
    "                                min_object_covered=0.1, \n",
    "                                aspect_ratio_range=(0.75, 1.33), \n",
    "                                area_range=(0.05, 1.0), \n",
    "                                max_attempts=100, \n",
    "                                scope=None):\n",
    "    \"\"\"\n",
    "    Generates cropped_image using one of the bboxes randomly distorted.\n",
    "    See `tf.image.sample_distorted_bounding_box` for more documentation.\n",
    "    Arguments:\n",
    "        image: `Tensor` of image data.\n",
    "        bbox: `Tensor` of bounding boxes arranged `[1, num_boxes, coords]` where each coordinate is [0, 1) and the coordinates are arranged\n",
    "                as `[ymin, xmin, ymax, xmax]`. If num_boxes is 0 then use the whole image.\n",
    "        min_object_covered: An optional `float`. Defaults to `0.1`. The cropped area of the image must contain at least this fraction of any bounding box supplied.\n",
    "        aspect_ratio_range: An optional list of `float`s. The cropped area of the image must have an aspect ratio = width / height within this range.\n",
    "        area_range: An optional list of `float`s. The cropped area of the image must contain a fraction of the supplied image within in this range.\n",
    "        max_attempts: An optional `int`. Number of attempts at generating a cropped region of the image of the specified constraints. After `max_attempts` failures, return the entire image.\n",
    "        scope: Optional `str` for name scope.\n",
    "    Returns:\n",
    "        (cropped image `Tensor`, distorted bbox `Tensor`).\n",
    "        \"\"\"\n",
    "    with tf.name_scope('distorted_bounding_box_crop'):\n",
    "        shape = tf.shape(image)\n",
    "        sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(shape,\n",
    "                                                                                bounding_boxes=bbox,\n",
    "                                                                                min_object_covered=min_object_covered,\n",
    "                                                                                aspect_ratio_range=aspect_ratio_range,\n",
    "                                                                                area_range=area_range,\n",
    "                                                                                max_attempts=max_attempts,\n",
    "                                                                                use_image_if_no_bounding_boxes=True)\n",
    "        bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n",
    "\n",
    "        # Crop the image to the specified bounding box.\n",
    "        offset_y, offset_x, _ = tf.unstack(bbox_begin)\n",
    "        target_height, target_width, _ = tf.unstack(bbox_size)\n",
    "        image = tf.image.crop_to_bounding_box(image, offset_y, offset_x, target_height, target_width)\n",
    "        return image\n",
    "\n",
    "def crop_and_resize(image, height, width):\n",
    "    \"\"\"\n",
    "    Make a random crop and resize it to height `height` and width `width`.\n",
    "    Arguments:\n",
    "        image: Tensor representing the image.\n",
    "        height: Desired image height.\n",
    "        width: Desired image width.\n",
    "    Returns:\n",
    "        A `height` x `width` x channels Tensor holding a random crop of `image`.\n",
    "    \"\"\"\n",
    "    \n",
    "    bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])\n",
    "    aspect_ratio = width / height\n",
    "    # Flag values are the global ones & they are set by set_aug_strength_value\n",
    "    image = distorted_bounding_box_crop(image,\n",
    "                                        bbox,\n",
    "                                        min_object_covered = FLAGS_min_object_covered ,\n",
    "                                        aspect_ratio_range=(3. / 4 * aspect_ratio, 4. / 3. * aspect_ratio),\n",
    "                                        area_range= FLAGS_area_range,\n",
    "                                        max_attempts=100,\n",
    "                                        scope=None)\n",
    "    \n",
    "    return tf.image.resize([image], [height, width], method = \"bicubic\")[0]\n",
    "\n",
    "def random_crop_with_resize(image, height, width, p=1.0):\n",
    "    \"\"\"\n",
    "    Randomly crop and resize an image.\n",
    "    Arguments:\n",
    "        image: `Tensor` representing an image of arbitrary size.\n",
    "        height: Height of output image.\n",
    "        width: Width of output image.\n",
    "        p: Probability of applying this transformation.\n",
    "    Returns:\n",
    "        A preprocessed image `Tensor`.\n",
    "    \"\"\"\n",
    "    def _transform(image):  # pylint: disable=missing-docstring\n",
    "        image = crop_and_resize(image, height, width)\n",
    "        return image\n",
    "    \n",
    "    return random_apply(_transform, p=p, x=image)\n",
    "\n",
    "def color_distortion(image, s=1.0):\n",
    "    \n",
    "    # image is a tensor with value range in [0, 1].\n",
    "    # s is the strength of color distortion.\n",
    "    def color_jitter(x):\n",
    "        # one can also shuffle the order of following augmentations\n",
    "        # each time they are applied.\n",
    "        x = tf.image.random_brightness(x, max_delta=0.8*s)\n",
    "        x = tf.image.random_contrast(x, lower=1-0.8*s, upper=1+0.8*s)\n",
    "        x = tf.image.random_saturation(x, lower=1-0.8*s, upper=1+0.8*s)\n",
    "        x = tf.image.random_hue(x, max_delta=0.2*s)\n",
    "        x = tf.clip_by_value(x, 0, 1)\n",
    "        return x\n",
    "\n",
    "    def color_drop(x):\n",
    "        image = tf.image.rgb_to_grayscale(x)\n",
    "        image = tf.tile(image, [1, 1, 3])\n",
    "        return image\n",
    "    \n",
    "    def non_lin_wrap(x):\n",
    "        \n",
    "        input_img = tf.image.convert_image_dtype(tf.expand_dims(x, 0), tf.dtypes.float32)\n",
    "        flow_shape = [1, input_img.shape[1], input_img.shape[2], 2]\n",
    "        init_flows = tf.random.normal(shape=flow_shape) * 2.0\n",
    "        dense_img_warp = tfa.image.dense_image_warp(input_img, init_flows)\n",
    "        dense_img_warp = tf.squeeze(dense_img_warp, 0)\n",
    "        return dense_img_warp\n",
    "        \n",
    "    def random_rotate(x):\n",
    "        # either apply 90 rotation or +/- 10 skew\n",
    "        return tf.cond(tf.less(tf.random.uniform([], minval=0, maxval=1, dtype=tf.float32), tf.cast(0.5, tf.float32)), \n",
    "                       true_fn = lambda : tf.image.rot90(x, k=1), \n",
    "                       false_fn = lambda : aug_layers(x))\n",
    "    \n",
    "    def random_gauss_noise(x):\n",
    "        noise = tf.random.normal(shape = (x.shape), mean = 0.0, stddev = 0.05, dtype = tf.float32) \n",
    "        return x + noise \n",
    "        \n",
    "    def gauss_filter(x):\n",
    "        return tfa.image.gaussian_filter2d(x, (10,10), sigma = 6.0)\n",
    "    \n",
    "    def channel_randomisation(x):\n",
    "        channels = tf.unstack (x, axis=-1)\n",
    "        c_order = tf.random.shuffle([0,1,2])\n",
    "        \n",
    "        return tf.stack([tf.gather(channels, c_order[0]), \n",
    "                         tf.gather(channels, c_order[1]), \n",
    "                         tf.gather(channels, c_order[2])], \n",
    "                         axis=-1)\n",
    "    \n",
    "    def random_overlay_color_masks(x):\n",
    "        r = tf.random.uniform(shape=[], minval=0, maxval=1)\n",
    "        g = tf.random.uniform(shape=[], minval=0, maxval=1)\n",
    "        b = tf.random.uniform(shape=[], minval=0, maxval=1)\n",
    "        r_channel = tf.fill([x.shape[0],x.shape[1]], r) \n",
    "        g_channel = tf.fill([x.shape[0],x.shape[1]], g) \n",
    "        b_channel = tf.fill([x.shape[0],x.shape[1]], b)\n",
    "        \n",
    "        mask_img = tf.stack([r_channel, g_channel, b_channel], axis=-1)\n",
    "        \n",
    "        alpha = tf.random.uniform(shape=[], minval=0.7, maxval=1)\n",
    "        beta = tf.random.uniform(shape=[], minval=0.1, maxval=0.5)\n",
    "        return alpha*x + beta * mask_img \n",
    "\n",
    "    # randomly apply transformation with probability p.\n",
    "    image = random_apply(color_jitter, p=0.8, x = image)\n",
    "    image = random_apply(color_drop, p=0.4, x = image)\n",
    "    image = random_apply(non_lin_wrap, p=0.1, x = image)\n",
    "    image = random_apply(gauss_filter, p = 0.1, x = image)\n",
    "    image = random_apply(random_rotate, p = 0.2, x = image)\n",
    "    image = random_apply(channel_randomisation, p = 0.2, x = image)\n",
    "    image = random_apply(random_gauss_noise, p = 0.2, x = image)\n",
    "    image = random_apply(random_overlay_color_masks, p = 0.4, x = image)\n",
    "    return image\n",
    "\n",
    "def preprocess_for_train(image, \n",
    "                        height, \n",
    "                        width, \n",
    "                        color_distort = True, \n",
    "                        crop = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Preprocesses the given image for training.\n",
    "    Arguments:\n",
    "        image  : (tf.image) representing an image of arbitrary size.\n",
    "        height : (int) height of the image to be returned\n",
    "        width  : (int) width of the image to be returned\n",
    "        color_distort : (bool) whether to apply the color distortion.\n",
    "        crop :(bool) Whether to crop the image.\n",
    "    Returns:\n",
    "        A preprocessed image `Tensor`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Applying image crop randomly \n",
    "    if crop:\n",
    "        image = random_crop_with_resize(image, height, width)\n",
    "    \n",
    "\n",
    "    # Applying color distortion, it becomes necessary when dealing with random crops\n",
    "    if color_distort:\n",
    "        image = color_distortion(image, s = FLAGS_color_jitter_strength)\n",
    "    \n",
    "    # Safely reshaping the image for batch formation & model inference\n",
    "    image = tf.reshape(image, [height, width, 3])\n",
    "\n",
    "    # Clipping steps for image normalisation\n",
    "    image = tf.clip_by_value(image, 0., 1.)\n",
    "\n",
    "    return image\n",
    "\n",
    "def preprocess_image(image, \n",
    "                     height, \n",
    "                     width, \n",
    "                     color_distort = True, \n",
    "                     cjs = 0.5, \n",
    "                     m_obj_cov = 0.3, \n",
    "                     a_range = (0.3,1.0)):\n",
    "    \"\"\"\n",
    "    Preprocesses the given image.\n",
    "    Arguments:\n",
    "        image  : (numpy array) representing an image of arbitrary size.\n",
    "        height : (int) height of the image to be returned\n",
    "        width  : (int) width of the image to be returned\n",
    "        color_distort : (bool) whether to apply the color distortion.\n",
    "        cjs : (float) color jitter strength & the default value is the base line we have\n",
    "        m_obj_cov : (float)  min area in the specified boudning box to cover\n",
    "        a_range : (tuple) the cropped area must contain a fraction of the supplied image within this range\n",
    "    Returns:\n",
    "        A preprocessed image tf.image of range [0, 1] normalised.\n",
    "        \"\"\"\n",
    "\n",
    "    # updating global values of the augmentation strength control parameters\n",
    "    set_aug_strength_value(cjs, m_obj_cov, a_range)\n",
    "    # Np to tensor & data-type setup\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "\n",
    "    return preprocess_for_train(image, \n",
    "                                height, \n",
    "                                width, \n",
    "                                color_distort = True,\n",
    "                                crop = True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fcb9a83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T15:20:39.260932Z",
     "iopub.status.busy": "2022-06-06T15:20:39.259423Z",
     "iopub.status.idle": "2022-06-06T15:20:39.261523Z",
     "shell.execute_reply": "2022-06-06T15:20:39.261969Z",
     "shell.execute_reply.started": "2022-06-06T15:15:28.960797Z"
    },
    "papermill": {
     "duration": 0.042691,
     "end_time": "2022-06-06T15:20:39.262089",
     "exception": false,
     "start_time": "2022-06-06T15:20:39.219398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "@tf.function\n",
    "def input_image_loader(image):\n",
    "    image_norm = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    \n",
    "    # The IMG_H & IMG_W are constants that will be read only once during graph tracing step\n",
    "    image_norm = tf.image.resize(image_norm, size=[IMG_H, IMG_W])\n",
    "    \n",
    "\n",
    "    aug_image_1 = preprocess_image(image = image_norm, \n",
    "                                    height = IMG_H, \n",
    "                                    width  = IMG_W, \n",
    "                                    cjs = color_jitter_strength,\n",
    "                                    m_obj_cov = minimum_object_coverage,\n",
    "                                    a_range = area_range)\n",
    "\n",
    "    aug_image_2 = preprocess_image(image = image_norm, \n",
    "                                    height = IMG_H, \n",
    "                                    width  = IMG_W, \n",
    "                                    cjs = color_jitter_strength,\n",
    "                                    m_obj_cov = minimum_object_coverage,\n",
    "                                    a_range = area_range)\n",
    "    # view 1 & view 2\n",
    "    return aug_image_1, aug_image_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4de791fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T15:20:39.337746Z",
     "iopub.status.busy": "2022-06-06T15:20:39.336930Z",
     "iopub.status.idle": "2022-06-06T15:20:45.444420Z",
     "shell.execute_reply": "2022-06-06T15:20:45.443941Z",
     "shell.execute_reply.started": "2022-06-06T15:18:24.245113Z"
    },
    "papermill": {
     "duration": 6.14826,
     "end_time": "2022-06-06T15:20:45.444552",
     "exception": false,
     "start_time": "2022-06-06T15:20:39.296292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TFDS\n",
    "train_tensor = tf.data.Dataset.from_tensor_slices(df_train)\n",
    "train_ds_shuffled = train_tensor.shuffle(len(train_tensor))\n",
    "final_train_ds = (train_ds_shuffled\n",
    "                .map(input_image_loader, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "                .batch(BATCH_SIZE, drop_remainder=True)\n",
    "                .prefetch(tf.data.experimental.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f131de4",
   "metadata": {
    "papermill": {
     "duration": 0.033991,
     "end_time": "2022-06-06T15:20:45.513333",
     "exception": false,
     "start_time": "2022-06-06T15:20:45.479342",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a792acb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T15:20:45.590499Z",
     "iopub.status.busy": "2022-06-06T15:20:45.589697Z",
     "iopub.status.idle": "2022-06-06T15:20:47.509404Z",
     "shell.execute_reply": "2022-06-06T15:20:47.509837Z",
     "shell.execute_reply.started": "2022-06-06T15:18:35.072128Z"
    },
    "papermill": {
     "duration": 1.962375,
     "end_time": "2022-06-06T15:20:47.509998",
     "exception": false,
     "start_time": "2022-06-06T15:20:45.547623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batch in final_train_ds.take(1):\n",
    "    pass\n",
    "\n",
    "image_to_viz = 3\n",
    "plt.figure(figsize=(7, 7))\n",
    "ax1 = plt.subplot(2, 2, 1)\n",
    "plt.imshow(batch[0][image_to_viz].numpy().astype('float32'), interpolation = 'none', vmin = 0, vmax = 1)\n",
    "ax1.grid(False)\n",
    "ax2 = plt.subplot(2, 2, 2)\n",
    "plt.imshow(batch[1][image_to_viz].numpy().astype('float32'), interpolation = 'none', vmin = 0, vmax = 1)\n",
    "ax2.grid(False)\n",
    "\n",
    "ax3 = plt.subplot(2, 2, 3)\n",
    "plt.hist(batch[0][image_to_viz].numpy().ravel())\n",
    "ax4 = plt.subplot(2, 2, 4, sharey = ax3, sharex=ax3)\n",
    "plt.hist(batch[1][image_to_viz].numpy().ravel())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6d86c4",
   "metadata": {
    "papermill": {
     "duration": 0.035005,
     "end_time": "2022-06-06T15:20:47.580497",
     "exception": false,
     "start_time": "2022-06-06T15:20:47.545492",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bedb2a",
   "metadata": {
    "papermill": {
     "duration": 0.034998,
     "end_time": "2022-06-06T15:20:47.651422",
     "exception": false,
     "start_time": "2022-06-06T15:20:47.616424",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Model helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3ae4304",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T15:20:47.725337Z",
     "iopub.status.busy": "2022-06-06T15:20:47.724456Z",
     "iopub.status.idle": "2022-06-06T15:20:47.745100Z",
     "shell.execute_reply": "2022-06-06T15:20:47.744677Z",
     "shell.execute_reply.started": "2022-06-06T15:18:39.594243Z"
    },
    "papermill": {
     "duration": 0.058663,
     "end_time": "2022-06-06T15:20:47.745211",
     "exception": false,
     "start_time": "2022-06-06T15:20:47.686548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# adding l2 reg\n",
    "def add_regularization(model, regularizer = tf.keras.regularizers.l2(0.0001)):\n",
    "    \"\"\"\n",
    "    Helper function to add l2 regularisation to each layer of a either a preTrained or \n",
    "    randomly initialised built in model\n",
    "    Arguments:\n",
    "        model : (keras.model) input model \n",
    "        regularizer : ( tf.keras.regularizers.l2) object from keras that defines a l2 regularizer\n",
    "    Returns:\n",
    "        model : all layers contain the \"regularizer\" object & incase we pass a pretrained model then the \n",
    "                original weights are preserved\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(regularizer, tf.keras.regularizers.Regularizer):\n",
    "        print(\"Regularizer must be a subclass of tf.keras.regularizers.Regularizer\")\n",
    "        return model\n",
    "\n",
    "    for layer in model.layers:\n",
    "        for attr in ['kernel_regularizer']:\n",
    "            if hasattr(layer, attr):\n",
    "                setattr(layer, attr, regularizer)\n",
    "\n",
    "    # When we change the layers attributes, the change only happens in the model config file\n",
    "    model_json = model.to_json()\n",
    "\n",
    "    # Save the weights before reloading the model.\n",
    "    tmp_weights_path = os.path.join(tempfile.gettempdir(), 'tmp_weights.h5')\n",
    "    model.save_weights(tmp_weights_path)\n",
    "\n",
    "    # load the model from the config\n",
    "    model = tf.keras.models.model_from_json(model_json)\n",
    "    \n",
    "    # Reload the model weights\n",
    "    model.load_weights(tmp_weights_path, by_name=True)\n",
    "    return model\n",
    "\n",
    "# Architecture utils\n",
    "def get_simclr(hidden_1, \n",
    "               hidden_2, \n",
    "               l2_penalty = 10e-6):\n",
    "    \"\"\"\n",
    "    Main function to define the entire network backbone to train\n",
    "    Arguments:\n",
    "        hidden_1-2 : (int) variable to define number of neurons in the projection head dense layer\n",
    "        l2_penalty : (float) to define the amount of l2 penalty applied to each layer's weights\n",
    "    Returns:\n",
    "        final_model : (tf.keras.Model) final model that will be trained \n",
    "    \"\"\"\n",
    "    \n",
    "    # encoder network\n",
    "    base_model = tf.keras.applications.ResNet50V2(include_top = False, \n",
    "                                                  weights = None, \n",
    "                                                  input_shape = (IMG_H, IMG_W, 3))\n",
    "    \n",
    "    # defining l2 regularization\n",
    "    regularizer = tf.keras.regularizers.l2(l2_penalty)\n",
    "    reg_base_model = add_regularization(base_model,regularizer)\n",
    "    reg_base_model.trainable = True\n",
    "\n",
    "    # Joining the entire pipeline using functional API\n",
    "    inputs = Input((IMG_H, IMG_W, 3))\n",
    "    h = reg_base_model(inputs, training=True)\n",
    "    h = GlobalAveragePooling2D()(h)\n",
    "    \n",
    "    # Non linear projection layer to improve the quality of embeddings being produced\n",
    "    projection_1 = Dense(hidden_1, kernel_regularizer = regularizer)(h)\n",
    "    projection_1 = tf.keras.layers.BatchNormalization()(projection_1)\n",
    "    projection_1 = Activation(\"relu\")(projection_1)\n",
    "    projection_2 = Dense(hidden_2, kernel_regularizer = regularizer)(projection_1)\n",
    "    projection_2 = tf.keras.layers.BatchNormalization()(projection_2)\n",
    "    projection_2 = Activation(\"relu\")(projection_2)\n",
    "\n",
    "    # Final Model\n",
    "    final_model = Model(inputs, projection_2)\n",
    "    return final_model\n",
    "\n",
    "def simclr_model(input_shape):\n",
    "    \"\"\"\n",
    "    Main function to define the entire network backbone to train\n",
    "    Arguments:\n",
    "        hidden_1-2 : (int) variable to define number of neurons in the projection head dense layer\n",
    "        l2_penalty : (float) to define the amount of l2 penalty applied to each layer's weights\n",
    "    Returns:\n",
    "        final_model : (tf.keras.Model) final model that will be trained \n",
    "    \"\"\"\n",
    "    \n",
    "#     # encoder network\n",
    "#     base_model = tf.keras.applications.ResNet50V2(include_top = False, \n",
    "#                                                   weights = None, \n",
    "#                                                   input_shape = (IMG_H, IMG_W, 3))\n",
    "    \n",
    "#     # defining l2 regularization\n",
    "#     regularizer = tf.keras.regularizers.l2(l2_penalty)\n",
    "#     reg_base_model = add_regularization(base_model,regularizer)\n",
    "#     reg_base_model.trainable = True\n",
    "\n",
    "#     # Joining the entire pipeline using functional API\n",
    "#     inputs = Input((IMG_H, IMG_W, 3))\n",
    "#     h = reg_base_model(inputs, training=True)\n",
    "#     h = GlobalAveragePooling2D()(h)\n",
    "    \n",
    "#     # Non linear projection layer to improve the quality of embeddings being produced\n",
    "#     projection_1 = Dense(hidden_1, kernel_regularizer = regularizer)(h)\n",
    "#     projection_1 = tf.keras.layers.BatchNormalization()(projection_1)\n",
    "#     projection_1 = Activation(\"relu\")(projection_1)\n",
    "#     projection_2 = Dense(hidden_2, kernel_regularizer = regularizer)(projection_1)\n",
    "#     projection_2 = tf.keras.layers.BatchNormalization()(projection_2)\n",
    "#     projection_2 = Activation(\"relu\")(projection_2)\n",
    "\n",
    "    input_img = tf.keras.Input(shape = input_shape)\n",
    "    \n",
    "    Z1 = tfl.Conv2D(16, kernel_size=3, strides=1, padding='same', activation='relu')(input_img)\n",
    "    B1 = tfl.BatchNormalization(axis=-1)(Z1)\n",
    "    Z2 = tfl.Conv2D(16, kernel_size=3, strides=1, padding='same', activation='relu')(B1)\n",
    "    B2 = tfl.BatchNormalization(axis=-1)(Z2)\n",
    "    P1 = tfl.MaxPool2D(pool_size=2, strides=2, padding='valid')(B2)\n",
    "    D1 = tfl.Dropout(0.25)(P1)\n",
    "    # (16, 16, 16)\n",
    "    \n",
    "    Z3 = tfl.Conv2D(32, kernel_size=2, strides=1, padding='valid', activation='relu')(D1)\n",
    "    B3 = tfl.BatchNormalization(axis=-1)(Z3)\n",
    "    Z4 = tfl.Conv2D(32, kernel_size=2, strides=1, padding='valid', activation='relu')(B3)\n",
    "    B4 = tfl.BatchNormalization(axis=-1)(Z4)\n",
    "    P2 = tfl.MaxPool2D(pool_size=2, strides=2, padding='valid')(B4)\n",
    "    D2 = tfl.Dropout(0.25)(P2)\n",
    "    # (7, 7, 32)\n",
    "    \n",
    "    F1 = tfl.Flatten()(D2)\n",
    "    Den1 = tfl.Dense(256, activation='relu')(F1)\n",
    "    Drop1 = tfl.Dropout(0.25)(Den1)\n",
    "    Den2 = tfl.Dense(64, activation='relu')(Drop1)\n",
    "    Drop2 = tfl.Dropout(0.25)(Den2)\n",
    "    outputs = tfl.Dense(10, activation='softmax')(Drop2)\n",
    "    \n",
    "    model = tf.keras.Model(inputs = input_img, outputs = outputs)\n",
    "    return model\n",
    "\n",
    "    # Final Model\n",
    "    final_model = Model(inputs, outputs)\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317985b9",
   "metadata": {
    "papermill": {
     "duration": 0.035164,
     "end_time": "2022-06-06T15:20:47.816008",
     "exception": false,
     "start_time": "2022-06-06T15:20:47.780844",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Defining a negative Mask \n",
    "\n",
    "This function has been define in the other files attached & the main goal of it is to define a mask when we compute loss of negative comparisons. Here is the output of this code visualised : \n",
    "\n",
    "**This negative mask will allow us to ignore the positive pairs in an augmented batch & help us focus only on the negative stuff !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32c25d69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T15:20:47.893755Z",
     "iopub.status.busy": "2022-06-06T15:20:47.892169Z",
     "iopub.status.idle": "2022-06-06T15:20:47.894340Z",
     "shell.execute_reply": "2022-06-06T15:20:47.894774Z",
     "shell.execute_reply.started": "2022-06-06T15:18:43.194043Z"
    },
    "papermill": {
     "duration": 0.043552,
     "end_time": "2022-06-06T15:20:47.894907",
     "exception": false,
     "start_time": "2022-06-06T15:20:47.851355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_negative_mask(batch_size):\n",
    "    # return a mask that removes the similarity score of equal/similar images.\n",
    "    # this function ensures that only distinct pair of images get their similarity scores\n",
    "    # passed as negative examples\n",
    "    negative_mask = np.ones((batch_size, 2 * batch_size), dtype=bool)\n",
    "    for i in range(batch_size):\n",
    "        negative_mask[i, i] = 0\n",
    "        negative_mask[i, i + batch_size] = 0\n",
    "    return tf.constant(negative_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd3bd445",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T15:20:47.970331Z",
     "iopub.status.busy": "2022-06-06T15:20:47.969500Z",
     "iopub.status.idle": "2022-06-06T15:20:47.971333Z",
     "shell.execute_reply": "2022-06-06T15:20:47.971772Z",
     "shell.execute_reply.started": "2022-06-06T15:18:53.805132Z"
    },
    "papermill": {
     "duration": 0.041652,
     "end_time": "2022-06-06T15:20:47.971905",
     "exception": false,
     "start_time": "2022-06-06T15:20:47.930253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mask to remove positive examples from the batch of negative samples\n",
    "negative_mask = get_negative_mask(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3df323e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T15:20:48.080026Z",
     "iopub.status.busy": "2022-06-06T15:20:48.048370Z",
     "iopub.status.idle": "2022-06-06T15:20:48.787826Z",
     "shell.execute_reply": "2022-06-06T15:20:48.787315Z",
     "shell.execute_reply.started": "2022-06-06T15:18:55.228604Z"
    },
    "papermill": {
     "duration": 0.78082,
     "end_time": "2022-06-06T15:20:48.787966",
     "exception": false,
     "start_time": "2022-06-06T15:20:48.007146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! rm -rf ./logs\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fbac653",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T15:20:48.871958Z",
     "iopub.status.busy": "2022-06-06T15:20:48.870338Z",
     "iopub.status.idle": "2022-06-06T15:20:48.872599Z",
     "shell.execute_reply": "2022-06-06T15:20:48.873028Z",
     "shell.execute_reply.started": "2022-06-06T15:18:57.119954Z"
    },
    "papermill": {
     "duration": 0.049434,
     "end_time": "2022-06-06T15:20:48.873146",
     "exception": false,
     "start_time": "2022-06-06T15:20:48.823712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(xis, \n",
    "               xjs, \n",
    "               model, \n",
    "               optimizer, \n",
    "               criterion, \n",
    "               temperature):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        zis = model(xis)\n",
    "        zjs = model(xjs)\n",
    "\n",
    "        # normalize projection feature vectors : onto a unit hypersphere\n",
    "        zis = tf.math.l2_normalize(zis, axis=1)\n",
    "        zjs = tf.math.l2_normalize(zjs, axis=1)\n",
    "          \n",
    "        # calculating over set of all positive pairs ( computing all numerators of softmax)\n",
    "        l_pos = sim_func_dim1(zis, zjs)\n",
    "        l_pos = tf.reshape(l_pos, (BATCH_SIZE, 1))\n",
    "\n",
    "        # temperature scaling\n",
    "        l_pos /= temperature\n",
    "        \n",
    "        # make the batch dimension 2*n\n",
    "        negatives = tf.concat([zjs, zis], axis=0)\n",
    "\n",
    "        loss = 0\n",
    "        for positives in [zis, zjs]:\n",
    "            # computing similarity with a data point & all the possible negatives\n",
    "            l_neg = sim_func_dim2(positives, negatives)\n",
    "            \n",
    "            # since each data point is its own class\n",
    "            labels = tf.zeros(BATCH_SIZE, dtype=tf.int32)\n",
    "            \n",
    "            # using the negative mask to remove itself & its positve counterpart to compute negative sim\n",
    "            l_neg = tf.boolean_mask(l_neg, negative_mask)\n",
    "            l_neg = tf.reshape(l_neg, (BATCH_SIZE, -1))\n",
    "\n",
    "            # temperature scaling\n",
    "            l_neg /= temperature\n",
    "\n",
    "            logits = tf.concat([l_pos, l_neg], axis=1) \n",
    "            loss += criterion(y_pred = logits, y_true = labels)\n",
    "\n",
    "        # since for every data point including its augmentation we compute the loss thus divide by 2*BatchSize\n",
    "        loss = loss / (2 * BATCH_SIZE)\n",
    "    \n",
    "    # Compute & apply the gradients on traininable paramters of the model\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    # updated model along with gradients so that we can visualise them on tensorboard.\n",
    "    return loss, gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7bbe10",
   "metadata": {
    "papermill": {
     "duration": 0.034989,
     "end_time": "2022-06-06T15:20:48.943324",
     "exception": false,
     "start_time": "2022-06-06T15:20:48.908335",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e0935de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T15:20:49.027504Z",
     "iopub.status.busy": "2022-06-06T15:20:49.026666Z",
     "iopub.status.idle": "2022-06-06T15:20:49.028486Z",
     "shell.execute_reply": "2022-06-06T15:20:49.029006Z",
     "shell.execute_reply.started": "2022-06-06T15:18:59.415952Z"
    },
    "papermill": {
     "duration": 0.050101,
     "end_time": "2022-06-06T15:20:49.029124",
     "exception": false,
     "start_time": "2022-06-06T15:20:48.979023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_simclr(model, \n",
    "                 train_dataset, \n",
    "                 optimizer, \n",
    "                 criterion,\n",
    "                 temperature=0.1, \n",
    "                 epochs=100,\n",
    "                 num_train_samples_viz = 5,\n",
    "                 num_test_samples_viz = 2,\n",
    "                metrics = 'accuracy'):\n",
    "    \n",
    "    \"\"\"\n",
    "      Training the model function\n",
    "    \"\"\"\n",
    "    \n",
    " \n",
    "    print(\"Starting training procedure .... : \")\n",
    "    print(\"Number of steps per epoch : \",len(train_dataset))\n",
    "    \n",
    "    # To measure per epoch time taken\n",
    "    t_start = time.time()\n",
    "    \n",
    "    # Visualisation lists\n",
    "    lr_epoch = []\n",
    "    epoch_wise_loss = []\n",
    "\n",
    "    for epoch in range(0, epochs):\n",
    "        \n",
    "        # Reset loss collection each step\n",
    "        step_wise_loss = []\n",
    "\n",
    "        # Number of grad descent steps in 1 epoch\n",
    "        num_train_steps = len(train_dataset) \n",
    "\n",
    "        # Picking up random batches & taking first image for input check\n",
    "        random_batches_train = random.sample(range(len(train_dataset)),num_train_samples_viz)\n",
    "        cnt = 0\n",
    "\n",
    "        # Arrays for tensorboard visualisation\n",
    "        random_collection_train_sample_1 = []\n",
    "        random_collection_train_sample_2 = []\n",
    "        gradArray = None\n",
    "        loss = None \n",
    "\n",
    "        # Training loop\n",
    "        for image_batch in tqdm(train_dataset):\n",
    "\n",
    "            # Fetching both views for input\n",
    "            a = image_batch[0]\n",
    "            b = image_batch[1]\n",
    "\n",
    "            # Train one batch\n",
    "            loss, gradArray = train_step(a, b, model, optimizer, criterion, temperature)\n",
    "            step_wise_loss.append(loss)\n",
    "\n",
    "            # Check whether to take image from this batch or not\n",
    "            if cnt in random_batches_train:\n",
    "                random_collection_train_sample_1.append(image_batch[0][0])\n",
    "                random_collection_train_sample_2.append(image_batch[1][0])\n",
    "            cnt+=1\n",
    "        \n",
    "        # Average loss throughout the whole process\n",
    "        if not len(epoch_wise_loss):\n",
    "            epoch_wise_loss.append(np.mean(step_wise_loss))\n",
    "        else:\n",
    "            # Adding the mean of previous ones\n",
    "            mean_value = (np.sum(step_wise_loss) + epoch_wise_loss[-1]*(epoch)*num_train_steps)/((epoch+1)*num_train_steps)\n",
    "            epoch_wise_loss.append(mean_value)\n",
    "        \n",
    "        # Printing the loss progression\n",
    "        print(\"\\n epoch: {} | train loss: {:.8f} | lr : {} | {:.4f} mins\"\n",
    "              .format(epoch + 1,epoch_wise_loss[-1],optimizer._decayed_lr(tf.float32).numpy(), (time.time()-t_start)/60.0))    \n",
    "   \n",
    "        # Appending the value of learning rate for warmup + cosine decay visualisation\n",
    "        lr_epoch.append(optimizer._decayed_lr(tf.float32).numpy())\n",
    "        \n",
    "        # saving models \n",
    "        print(\"Saving Base Model.....\")\n",
    "        \n",
    "        # Saving the entire model for checkpointing reasons\n",
    "        model.save(\"./\" + modelNameStr + \".h5\")\n",
    "\n",
    "        # saving the state of optimizer\n",
    "        np.save(\"./\" + modelNameStr + \"_optimizer.npy\", optimizer.get_weights())\n",
    "\n",
    "    \n",
    "    return epoch_wise_loss, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ad1764",
   "metadata": {
    "papermill": {
     "duration": 0.034821,
     "end_time": "2022-06-06T15:20:49.099337",
     "exception": false,
     "start_time": "2022-06-06T15:20:49.064516",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Optimiser schedule\n",
    "\n",
    "This is a custom learning rate scheduler written according to SimCLR authors. The learning rate starts from a low value to an initial value linearly after which the cosine decay schedule beigns. The warmup_steps indicate number of epochs taken by optimiser to increase linearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "154d5a85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T15:20:49.179299Z",
     "iopub.status.busy": "2022-06-06T15:20:49.178451Z",
     "iopub.status.idle": "2022-06-06T15:20:49.180305Z",
     "shell.execute_reply": "2022-06-06T15:20:49.180781Z",
     "shell.execute_reply.started": "2022-06-06T15:19:02.363179Z"
    },
    "papermill": {
     "duration": 0.04644,
     "end_time": "2022-06-06T15:20:49.180899",
     "exception": false,
     "start_time": "2022-06-06T15:20:49.134459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WarmUp(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self,\n",
    "               initial_learning_rate: float,\n",
    "               decay_schedule_fn: Callable,\n",
    "               warmup_steps: int,\n",
    "               power: float = 1.0,\n",
    "               name: str = None,):\n",
    "    \n",
    "    super().__init__()\n",
    "    self.initial_learning_rate = initial_learning_rate\n",
    "    self.warmup_steps = warmup_steps\n",
    "    self.power = power\n",
    "    self.decay_schedule_fn = decay_schedule_fn\n",
    "    self.name = name\n",
    "\n",
    "  def __call__(self, step):\n",
    "    with tf.name_scope(self.name or \"WarmUp\") as name:\n",
    "        # Implements polynomial warmup. i.e., if global_step < warmup_steps, the\n",
    "        # learning rate will be `global_step/num_warmup_steps * init_lr`.\n",
    "        global_step_float = tf.cast(step, tf.float32)\n",
    "        warmup_steps_float = tf.cast(self.warmup_steps, tf.float32)\n",
    "        warmup_percent_done = global_step_float / warmup_steps_float\n",
    "        warmup_learning_rate = self.initial_learning_rate * tf.math.pow(warmup_percent_done, self.power)\n",
    "        return tf.cond(\n",
    "            global_step_float < warmup_steps_float,\n",
    "            lambda: warmup_learning_rate,\n",
    "            lambda: self.decay_schedule_fn(step - self.warmup_steps),\n",
    "            name=name,\n",
    "        )\n",
    "\n",
    "  def get_config(self):\n",
    "    return {\n",
    "        \"initial_learning_rate\": self.initial_learning_rate,\n",
    "        \"decay_schedule_fn\": self.decay_schedule_fn,\n",
    "        \"warmup_steps\": self.warmup_steps,\n",
    "        \"power\": self.power,\n",
    "        \"name\": self.name,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345cff2d",
   "metadata": {
    "papermill": {
     "duration": 0.034996,
     "end_time": "2022-06-06T15:20:49.251065",
     "exception": false,
     "start_time": "2022-06-06T15:20:49.216069",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aac8bcde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T15:20:49.326342Z",
     "iopub.status.busy": "2022-06-06T15:20:49.325524Z",
     "iopub.status.idle": "2022-06-06T15:20:49.327972Z",
     "shell.execute_reply": "2022-06-06T15:20:49.327491Z",
     "shell.execute_reply.started": "2022-06-06T15:19:05.276318Z"
    },
    "papermill": {
     "duration": 0.041809,
     "end_time": "2022-06-06T15:20:49.328077",
     "exception": false,
     "start_time": "2022-06-06T15:20:49.286268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as tfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc8584db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T15:20:49.414292Z",
     "iopub.status.busy": "2022-06-06T15:20:49.413489Z",
     "iopub.status.idle": "2022-06-06T15:20:52.434862Z",
     "shell.execute_reply": "2022-06-06T15:20:52.435426Z",
     "shell.execute_reply.started": "2022-06-06T15:19:07.254666Z"
    },
    "papermill": {
     "duration": 3.072466,
     "end_time": "2022-06-06T15:20:52.435618",
     "exception": false,
     "start_time": "2022-06-06T15:20:49.363152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelName :  ./resNet_simCLR_decay_10e6_color_0.3_min_obj_0.7_avgPool_loss_tmp_0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decay Steps :  6250\n",
      "['conv5_block2_2_bn/beta:0', 'conv5_block3_preact_bn/gamma:0', 'conv5_block3_1_bn/beta:0', 'conv5_block3_3_conv/kernel:0', 'dense/kernel:0', 'dense_1/kernel:0']\n"
     ]
    }
   ],
   "source": [
    "# Defining the loss function\n",
    "criterion = tf.keras.losses.CategoricalCrossentropy(reduction=tf.keras.losses.Reduction.SUM)\n",
    "# Number of epochs\n",
    "tot_epochs = 5\n",
    "\n",
    "# temperature in NTXent\n",
    "loss_temp = 0.2\n",
    "\n",
    "# Defining the SimCLR model\n",
    "# Differentiating models via their hyper-parameter values\n",
    "modelNameStr = \"./resNet_simCLR_decay_10e6_color_\"+str(color_jitter_strength)+\"_min_obj_\"+str(minimum_object_coverage)+\"_avgPool_loss_tmp_\"+str(loss_temp)\n",
    "print(\"ModelName : \",modelNameStr) \n",
    "simclr_2 = get_simclr(32, 32)\n",
    "\n",
    "\n",
    "# optimiser decay schedule\n",
    "decay_steps = (len(final_train_ds))*tot_epochs\n",
    "warmup_steps = (len(final_train_ds))*10\n",
    "initial_lr = 0.5e-3\n",
    "\n",
    "# Cosine decay function\n",
    "lr_decayed_fn = tf.keras.experimental.CosineDecay(initial_learning_rate = initial_lr, \n",
    "                                                  decay_steps = decay_steps)\n",
    "cosine_with_warmUp = WarmUp(initial_learning_rate = initial_lr,\n",
    "                            decay_schedule_fn = lr_decayed_fn,\n",
    "                            warmup_steps = warmup_steps)\n",
    "\n",
    "print(\"Decay Steps : \",decay_steps)\n",
    "optimizer = tf.keras.optimizers.Adam(cosine_with_warmUp)\n",
    "\n",
    "# Learning a layer to number mapping for kernel weight & gradient visualisation of last few CNN layers & projection head\n",
    "tot = []\n",
    "for i in simclr_2.layers:\n",
    "  for j in i.trainable_weights:\n",
    "    tot.append(j.name)\n",
    "cnt = 0\n",
    "\n",
    "layer_names = [tot[-23], tot[-20], tot[-16], tot[-12], tot[-8], tot[-4]]\n",
    "print(layer_names)\n",
    "index_to_layer = {}\n",
    "for i in simclr_2.layers:\n",
    "  for j in i.trainable_weights:\n",
    "    index_to_layer[cnt] = j.name\n",
    "    cnt+=1\n",
    "layer_to_index = {j:i for i,j in index_to_layer.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beaebf6",
   "metadata": {
    "papermill": {
     "duration": 0.037607,
     "end_time": "2022-06-06T15:20:52.511878",
     "exception": false,
     "start_time": "2022-06-06T15:20:52.474271",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training SimCLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0afd7a27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T15:20:52.595393Z",
     "iopub.status.busy": "2022-06-06T15:20:52.594600Z",
     "iopub.status.idle": "2022-06-06T15:20:52.687212Z",
     "shell.execute_reply": "2022-06-06T15:20:52.686758Z",
     "shell.execute_reply.started": "2022-06-06T15:19:12.351071Z"
    },
    "papermill": {
     "duration": 0.137887,
     "end_time": "2022-06-06T15:20:52.687321",
     "exception": false,
     "start_time": "2022-06-06T15:20:52.549434",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "simclr = simclr_model((32, 32, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3abd2216",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T15:20:52.767809Z",
     "iopub.status.busy": "2022-06-06T15:20:52.767001Z",
     "iopub.status.idle": "2022-06-06T15:20:54.821905Z",
     "shell.execute_reply": "2022-06-06T15:20:54.821396Z",
     "shell.execute_reply.started": "2022-06-06T15:19:13.714976Z"
    },
    "papermill": {
     "duration": 2.096319,
     "end_time": "2022-06-06T15:20:54.822027",
     "exception": false,
     "start_time": "2022-06-06T15:20:52.725708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Composing the Train Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((df_train, y_train_oh)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a3ba8a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T15:20:54.915342Z",
     "iopub.status.busy": "2022-06-06T15:20:54.908521Z",
     "iopub.status.idle": "2022-06-06T15:42:22.822825Z",
     "shell.execute_reply": "2022-06-06T15:42:22.822057Z"
    },
    "papermill": {
     "duration": 1287.963967,
     "end_time": "2022-06-06T15:42:22.823018",
     "exception": false,
     "start_time": "2022-06-06T15:20:54.859051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1251/1251 [==============================] - 13s 5ms/step - loss: 55.0307 - accuracy: 0.3767\n",
      "Epoch 2/10\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 42.0113 - accuracy: 0.5381\n",
      "Epoch 3/10\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 36.6391 - accuracy: 0.5999\n",
      "Epoch 4/10\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 33.3647 - accuracy: 0.6390\n",
      "Epoch 5/10\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 30.7396 - accuracy: 0.6660\n",
      "Epoch 6/10\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 28.9765 - accuracy: 0.6882\n",
      "Epoch 7/10\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 27.2225 - accuracy: 0.7029\n",
      "Epoch 8/10\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 25.6425 - accuracy: 0.7256\n",
      "Epoch 9/10\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 24.5417 - accuracy: 0.7350\n",
      "Epoch 10/10\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 23.3916 - accuracy: 0.7455\n",
      "For  10  Epochs:\n",
      "Log-loss for Train Dataset =  0.4981248274490955\n",
      "Log-loss for Test Dataset =  0.7804772136236402\n",
      "Accuracy for Train Dataset =  0.8197520371944208\n",
      "Accuracy for Test Dataset =  0.7409\n",
      "\n",
      "Epoch 1/20\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 55.1713 - accuracy: 0.3744\n",
      "Epoch 2/20\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 42.2068 - accuracy: 0.5291\n",
      "Epoch 3/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 36.0317 - accuracy: 0.6093\n",
      "Epoch 4/20\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 32.4435 - accuracy: 0.6502\n",
      "Epoch 5/20\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 30.0646 - accuracy: 0.6769\n",
      "Epoch 6/20\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 28.0249 - accuracy: 0.6977\n",
      "Epoch 7/20\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 26.5307 - accuracy: 0.7134\n",
      "Epoch 8/20\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 25.0965 - accuracy: 0.7305\n",
      "Epoch 9/20\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 23.9187 - accuracy: 0.7436\n",
      "Epoch 10/20\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 22.7644 - accuracy: 0.7527\n",
      "Epoch 11/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 21.9649 - accuracy: 0.7624\n",
      "Epoch 12/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 20.9925 - accuracy: 0.7715\n",
      "Epoch 13/20\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 20.2217 - accuracy: 0.7822\n",
      "Epoch 14/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 19.5053 - accuracy: 0.7881\n",
      "Epoch 15/20\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 19.2212 - accuracy: 0.7927\n",
      "Epoch 16/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 18.4192 - accuracy: 0.8000\n",
      "Epoch 17/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 17.8094 - accuracy: 0.8081\n",
      "Epoch 18/20\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 17.4644 - accuracy: 0.8124\n",
      "Epoch 19/20\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 17.0622 - accuracy: 0.8151\n",
      "Epoch 20/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 16.6025 - accuracy: 0.8201\n",
      "For  20  Epochs:\n",
      "Log-loss for Train Dataset =  0.26167158639741794\n",
      "Log-loss for Test Dataset =  0.7631017223428378\n",
      "Accuracy for Train Dataset =  0.9153127030945358\n",
      "Accuracy for Test Dataset =  0.757\n",
      "\n",
      "Epoch 1/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 53.8133 - accuracy: 0.3898\n",
      "Epoch 2/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 41.4190 - accuracy: 0.5408\n",
      "Epoch 3/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 36.4432 - accuracy: 0.6046\n",
      "Epoch 4/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 33.3600 - accuracy: 0.6422\n",
      "Epoch 5/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 30.8281 - accuracy: 0.6677\n",
      "Epoch 6/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 29.2169 - accuracy: 0.6850\n",
      "Epoch 7/30\n",
      "1251/1251 [==============================] - 7s 5ms/step - loss: 27.4700 - accuracy: 0.7034\n",
      "Epoch 8/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 25.9530 - accuracy: 0.7195\n",
      "Epoch 9/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 24.9041 - accuracy: 0.7327\n",
      "Epoch 10/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 23.7830 - accuracy: 0.7405\n",
      "Epoch 11/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 22.8459 - accuracy: 0.7518\n",
      "Epoch 12/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 21.8951 - accuracy: 0.7606\n",
      "Epoch 13/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 21.2236 - accuracy: 0.7700\n",
      "Epoch 14/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 20.4677 - accuracy: 0.7758\n",
      "Epoch 15/30\n",
      "1251/1251 [==============================] - 7s 5ms/step - loss: 19.8566 - accuracy: 0.7844\n",
      "Epoch 16/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 19.2279 - accuracy: 0.7914\n",
      "Epoch 17/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 18.4605 - accuracy: 0.7986\n",
      "Epoch 18/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 18.1962 - accuracy: 0.8012\n",
      "Epoch 19/30\n",
      "1251/1251 [==============================] - 7s 5ms/step - loss: 17.5941 - accuracy: 0.8076\n",
      "Epoch 20/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 17.1610 - accuracy: 0.8123\n",
      "Epoch 21/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 16.6741 - accuracy: 0.8187\n",
      "Epoch 22/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 16.4473 - accuracy: 0.8201\n",
      "Epoch 23/30\n",
      "1251/1251 [==============================] - 7s 5ms/step - loss: 16.1610 - accuracy: 0.8268\n",
      "Epoch 24/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 15.8498 - accuracy: 0.8300\n",
      "Epoch 25/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 15.4130 - accuracy: 0.8325\n",
      "Epoch 26/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 15.1528 - accuracy: 0.8346\n",
      "Epoch 27/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 14.7802 - accuracy: 0.8404\n",
      "Epoch 28/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 14.3193 - accuracy: 0.8423\n",
      "Epoch 29/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 14.0864 - accuracy: 0.8482\n",
      "Epoch 30/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 13.8943 - accuracy: 0.8499\n",
      "For  30  Epochs:\n",
      "Log-loss for Train Dataset =  0.20101969464724104\n",
      "Log-loss for Test Dataset =  0.7836331584847569\n",
      "Accuracy for Train Dataset =  0.9370594410838374\n",
      "Accuracy for Test Dataset =  0.7573\n",
      "\n",
      "Epoch 1/40\n",
      "1251/1251 [==============================] - 7s 5ms/step - loss: 53.8079 - accuracy: 0.3914\n",
      "Epoch 2/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 41.4956 - accuracy: 0.5404\n",
      "Epoch 3/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 36.5164 - accuracy: 0.6015\n",
      "Epoch 4/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 33.2109 - accuracy: 0.6382\n",
      "Epoch 5/40\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 30.6445 - accuracy: 0.6666\n",
      "Epoch 6/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 28.5572 - accuracy: 0.6899\n",
      "Epoch 7/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 26.7601 - accuracy: 0.7094\n",
      "Epoch 8/40\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 25.2060 - accuracy: 0.7282\n",
      "Epoch 9/40\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 23.8794 - accuracy: 0.7407\n",
      "Epoch 10/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 22.8527 - accuracy: 0.7513\n",
      "Epoch 11/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 21.8315 - accuracy: 0.7639\n",
      "Epoch 12/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 20.9354 - accuracy: 0.7732\n",
      "Epoch 13/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 20.2372 - accuracy: 0.7804\n",
      "Epoch 14/40\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 19.2976 - accuracy: 0.7916\n",
      "Epoch 15/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 18.8351 - accuracy: 0.7949\n",
      "Epoch 16/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 18.1077 - accuracy: 0.8029\n",
      "Epoch 17/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 17.5500 - accuracy: 0.8105\n",
      "Epoch 18/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 17.1589 - accuracy: 0.8131\n",
      "Epoch 19/40\n",
      "1251/1251 [==============================] - 7s 5ms/step - loss: 16.5027 - accuracy: 0.8218\n",
      "Epoch 20/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 16.1046 - accuracy: 0.8245\n",
      "Epoch 21/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 15.8315 - accuracy: 0.8276\n",
      "Epoch 22/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 15.2492 - accuracy: 0.8341\n",
      "Epoch 23/40\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 15.0968 - accuracy: 0.8355\n",
      "Epoch 24/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 14.6505 - accuracy: 0.8406\n",
      "Epoch 25/40\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 14.2632 - accuracy: 0.8427\n",
      "Epoch 26/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 14.1185 - accuracy: 0.8455\n",
      "Epoch 27/40\n",
      "1251/1251 [==============================] - 8s 6ms/step - loss: 13.6500 - accuracy: 0.8533\n",
      "Epoch 28/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 13.6035 - accuracy: 0.8528\n",
      "Epoch 29/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 13.2906 - accuracy: 0.8557\n",
      "Epoch 30/40\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 13.2712 - accuracy: 0.8561\n",
      "Epoch 31/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 12.7318 - accuracy: 0.8628\n",
      "Epoch 32/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 12.6465 - accuracy: 0.8656\n",
      "Epoch 33/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 12.3624 - accuracy: 0.8671\n",
      "Epoch 34/40\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 12.2663 - accuracy: 0.8666\n",
      "Epoch 35/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 12.1965 - accuracy: 0.8679\n",
      "Epoch 36/40\n",
      "1251/1251 [==============================] - 8s 7ms/step - loss: 11.7731 - accuracy: 0.8727\n",
      "Epoch 37/40\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 11.7285 - accuracy: 0.8729\n",
      "Epoch 38/40\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 11.4154 - accuracy: 0.8773\n",
      "Epoch 39/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 11.4145 - accuracy: 0.8769\n",
      "Epoch 40/40\n",
      "1251/1251 [==============================] - 8s 7ms/step - loss: 11.2740 - accuracy: 0.8796\n",
      "For  40  Epochs:\n",
      "Log-loss for Train Dataset =  0.13532269112984305\n",
      "Log-loss for Test Dataset =  0.8082677006913427\n",
      "Accuracy for Train Dataset =  0.9586811978203269\n",
      "Accuracy for Test Dataset =  0.7663\n",
      "\n",
      "Epoch 1/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 54.4496 - accuracy: 0.3844\n",
      "Epoch 2/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 41.5007 - accuracy: 0.5446\n",
      "Epoch 3/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 36.1016 - accuracy: 0.6051\n",
      "Epoch 4/50\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 32.9760 - accuracy: 0.6430\n",
      "Epoch 5/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 30.3039 - accuracy: 0.6710\n",
      "Epoch 6/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 28.4033 - accuracy: 0.6927\n",
      "Epoch 7/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 26.7808 - accuracy: 0.7103\n",
      "Epoch 8/50\n",
      "1251/1251 [==============================] - 8s 6ms/step - loss: 25.3955 - accuracy: 0.7274\n",
      "Epoch 9/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 24.2816 - accuracy: 0.7365\n",
      "Epoch 10/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 23.3403 - accuracy: 0.7472\n",
      "Epoch 11/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 22.3421 - accuracy: 0.7595\n",
      "Epoch 12/50\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 21.3213 - accuracy: 0.7700\n",
      "Epoch 13/50\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 20.5874 - accuracy: 0.7767\n",
      "Epoch 14/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 19.8611 - accuracy: 0.7888\n",
      "Epoch 15/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 19.3166 - accuracy: 0.7905\n",
      "Epoch 16/50\n",
      "1251/1251 [==============================] - 8s 6ms/step - loss: 18.7319 - accuracy: 0.7973\n",
      "Epoch 17/50\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 18.0010 - accuracy: 0.8077\n",
      "Epoch 18/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 17.8241 - accuracy: 0.8058\n",
      "Epoch 19/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 17.3665 - accuracy: 0.8126\n",
      "Epoch 20/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 16.7680 - accuracy: 0.8190\n",
      "Epoch 21/50\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 16.2822 - accuracy: 0.8245\n",
      "Epoch 22/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 16.1456 - accuracy: 0.8252\n",
      "Epoch 23/50\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 15.3716 - accuracy: 0.8327\n",
      "Epoch 24/50\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 15.2913 - accuracy: 0.8356\n",
      "Epoch 25/50\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 14.8543 - accuracy: 0.8412\n",
      "Epoch 26/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 14.6403 - accuracy: 0.8405\n",
      "Epoch 27/50\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 14.2910 - accuracy: 0.8436\n",
      "Epoch 28/50\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 14.1172 - accuracy: 0.8491\n",
      "Epoch 29/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 13.6542 - accuracy: 0.8527\n",
      "Epoch 30/50\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 13.3237 - accuracy: 0.8590\n",
      "Epoch 31/50\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 13.3083 - accuracy: 0.8570\n",
      "Epoch 32/50\n",
      "1251/1251 [==============================] - 8s 7ms/step - loss: 13.2066 - accuracy: 0.8574\n",
      "Epoch 33/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 12.9972 - accuracy: 0.8605\n",
      "Epoch 34/50\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 12.7046 - accuracy: 0.8641\n",
      "Epoch 35/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 12.3876 - accuracy: 0.8676\n",
      "Epoch 36/50\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 12.1445 - accuracy: 0.8696\n",
      "Epoch 37/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 11.9738 - accuracy: 0.8712\n",
      "Epoch 38/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 11.8255 - accuracy: 0.8733\n",
      "Epoch 39/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 11.7329 - accuracy: 0.8749\n",
      "Epoch 40/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 11.4509 - accuracy: 0.8775\n",
      "Epoch 41/50\n",
      "1251/1251 [==============================] - 8s 6ms/step - loss: 11.4838 - accuracy: 0.8773\n",
      "Epoch 42/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 11.1759 - accuracy: 0.8792\n",
      "Epoch 43/50\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 11.0890 - accuracy: 0.8811\n",
      "Epoch 44/50\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 11.0110 - accuracy: 0.8822\n",
      "Epoch 45/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 11.0150 - accuracy: 0.8828\n",
      "Epoch 46/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 10.7378 - accuracy: 0.8859\n",
      "Epoch 47/50\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 10.3571 - accuracy: 0.8892\n",
      "Epoch 48/50\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 10.5283 - accuracy: 0.8882\n",
      "Epoch 49/50\n",
      "1251/1251 [==============================] - 8s 7ms/step - loss: 10.5433 - accuracy: 0.8889\n",
      "Epoch 50/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 10.1809 - accuracy: 0.8922\n",
      "For  50  Epochs:\n",
      "Log-loss for Train Dataset =  0.10341768808126076\n",
      "Log-loss for Test Dataset =  0.8806747725668551\n",
      "Accuracy for Train Dataset =  0.9678548217767335\n",
      "Accuracy for Test Dataset =  0.7622\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = [10, 20, 30, 40, 50]\n",
    "train_loss, test_loss, train_acc, test_acc = [], [], [], []\n",
    "\n",
    "for epochs in num_epochs:\n",
    "    # Training the Model\n",
    "    simclr = simclr_model((32, 32, 3))\n",
    "    simclr.compile(optimizer='adam', loss=criterion, metrics='accuracy')\n",
    "    simclr.fit(train_dataset, epochs = epochs)\n",
    "    \n",
    "    # Predicting on the Train/Test Datasets\n",
    "    preds_train = simclr.predict(df_train)\n",
    "    preds_test = simclr.predict(df_test)\n",
    "\n",
    "    # Finding the Predicted Classes\n",
    "    cls_train = np.argmax(preds_train, axis = 1)\n",
    "    cls_test = np.argmax(preds_test, axis = 1)\n",
    "    \n",
    "    # Finding the Train/Test set Loss\n",
    "    train_loss.append(log_loss(y_train_oh, preds_train))\n",
    "    test_loss.append(log_loss(y_test_oh, preds_test))\n",
    "    train_acc.append(accuracy_score(y_train, cls_train))\n",
    "    test_acc.append(accuracy_score(y_test, cls_test))\n",
    "    \n",
    "    print(\"For \", epochs, \" Epochs:\")\n",
    "    print(\"Log-loss for Train Dataset = \", train_loss[-1])\n",
    "    print(\"Log-loss for Test Dataset = \", test_loss[-1])\n",
    "    print(\"Accuracy for Train Dataset = \", train_acc[-1])\n",
    "    print(\"Accuracy for Test Dataset = \", test_acc[-1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c83ca999",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T15:42:34.030176Z",
     "iopub.status.busy": "2022-06-06T15:42:34.028974Z",
     "iopub.status.idle": "2022-06-06T15:48:32.346417Z",
     "shell.execute_reply": "2022-06-06T15:48:32.345889Z",
     "shell.execute_reply.started": "2022-06-06T15:15:32.530960Z"
    },
    "papermill": {
     "duration": 363.787287,
     "end_time": "2022-06-06T15:48:32.346554",
     "exception": false,
     "start_time": "2022-06-06T15:42:28.559267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 55.2684 - accuracy: 0.3809\n",
      "Epoch 2/40\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 42.3523 - accuracy: 0.5287\n",
      "Epoch 3/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 36.3108 - accuracy: 0.6043\n",
      "Epoch 4/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 32.5973 - accuracy: 0.6487\n",
      "Epoch 5/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 30.0062 - accuracy: 0.6782\n",
      "Epoch 6/40\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 28.1599 - accuracy: 0.6965\n",
      "Epoch 7/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 26.5431 - accuracy: 0.7139\n",
      "Epoch 8/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 25.1150 - accuracy: 0.7293\n",
      "Epoch 9/40\n",
      "1251/1251 [==============================] - 7s 5ms/step - loss: 23.6379 - accuracy: 0.7443\n",
      "Epoch 10/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 22.5395 - accuracy: 0.7569\n",
      "Epoch 11/40\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 21.6237 - accuracy: 0.7676\n",
      "Epoch 12/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 20.6329 - accuracy: 0.7771\n",
      "Epoch 13/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 19.9065 - accuracy: 0.7859\n",
      "Epoch 14/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 19.1601 - accuracy: 0.7916\n",
      "Epoch 15/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 18.7820 - accuracy: 0.7961\n",
      "Epoch 16/40\n",
      "1251/1251 [==============================] - 8s 6ms/step - loss: 18.0325 - accuracy: 0.8048\n",
      "Epoch 17/40\n",
      "1251/1251 [==============================] - 7s 5ms/step - loss: 17.5526 - accuracy: 0.8131\n",
      "Epoch 18/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 16.9916 - accuracy: 0.8141\n",
      "Epoch 19/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 16.5480 - accuracy: 0.8207\n",
      "Epoch 20/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 16.2695 - accuracy: 0.8250\n",
      "Epoch 21/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 15.7398 - accuracy: 0.8292\n",
      "Epoch 22/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 15.3700 - accuracy: 0.8335\n",
      "Epoch 23/40\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 15.1073 - accuracy: 0.8356\n",
      "Epoch 24/40\n",
      "1251/1251 [==============================] - 8s 7ms/step - loss: 14.5043 - accuracy: 0.8419\n",
      "Epoch 25/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 14.4646 - accuracy: 0.8436\n",
      "Epoch 26/40\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 13.9436 - accuracy: 0.8495\n",
      "Epoch 27/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 13.7359 - accuracy: 0.8515\n",
      "Epoch 28/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 13.2965 - accuracy: 0.8574\n",
      "Epoch 29/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 13.2084 - accuracy: 0.8589\n",
      "Epoch 30/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 13.0084 - accuracy: 0.8605\n",
      "Epoch 31/40\n",
      "1251/1251 [==============================] - 8s 6ms/step - loss: 12.5873 - accuracy: 0.8643\n",
      "Epoch 32/40\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 12.7842 - accuracy: 0.8613\n",
      "Epoch 33/40\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 12.3428 - accuracy: 0.8680\n",
      "Epoch 34/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 12.2246 - accuracy: 0.8684\n",
      "Epoch 35/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 11.7184 - accuracy: 0.8749\n",
      "Epoch 36/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 11.7572 - accuracy: 0.8734\n",
      "Epoch 37/40\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 11.2998 - accuracy: 0.8779\n",
      "Epoch 38/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 11.4990 - accuracy: 0.8789\n",
      "Epoch 39/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 11.1109 - accuracy: 0.8814\n",
      "Epoch 40/40\n",
      "1251/1251 [==============================] - 7s 5ms/step - loss: 11.1208 - accuracy: 0.8832\n"
     ]
    }
   ],
   "source": [
    "# Training the Model with the best hyper-parameter settings\n",
    "ind = np.argmax(test_acc)\n",
    "best_num_epochs = num_epochs[ind]\n",
    "simclr = simclr_model((32, 32, 3))\n",
    "simclr.compile(optimizer='adam', loss=criterion, metrics='accuracy')\n",
    "simclr.fit(train_dataset, epochs = best_num_epochs)\n",
    "\n",
    "# Saving the model along with it's weights\n",
    "simclr.save('simclr_model.h5')\n",
    "\n",
    "# Predicting on the Train/Test Datasets\n",
    "preds_train = simclr.predict(df_train)\n",
    "preds_test = simclr.predict(df_test)\n",
    "\n",
    "# Finding the Predicted Classes\n",
    "cls_train = np.argmax(preds_train, axis = 1)\n",
    "cls_test = np.argmax(preds_test, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f4d7d81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T15:48:46.888138Z",
     "iopub.status.busy": "2022-06-06T15:48:46.887319Z",
     "iopub.status.idle": "2022-06-06T15:48:46.978711Z",
     "shell.execute_reply": "2022-06-06T15:48:46.978231Z",
     "shell.execute_reply.started": "2022-06-06T15:15:32.532869Z"
    },
    "papermill": {
     "duration": 7.493615,
     "end_time": "2022-06-06T15:48:46.978833",
     "exception": false,
     "start_time": "2022-06-06T15:48:39.485218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-loss for Train Dataset =  0.0940608659091713\n",
      "Log-loss for Test Dataset =  0.7863617719537916\n",
      "Weighted F1 Score for Train Dataset =  0.9726344595681928\n",
      "Weighted F1 Score for Test Dataset =  0.7733116996857851\n",
      "Accuracy for Train Dataset =  0.9726541018847172\n",
      "Accuracy for Test Dataset =  0.7747\n"
     ]
    }
   ],
   "source": [
    "# Finding the Train/Test set Loss\n",
    "print(\"Log-loss for Train Dataset = \", log_loss(y_train_oh, preds_train))\n",
    "print(\"Log-loss for Test Dataset = \", log_loss(y_test_oh, preds_test))\n",
    "print(\"Weighted F1 Score for Train Dataset = \", f1_score(y_train, cls_train, average = 'weighted'))\n",
    "print(\"Weighted F1 Score for Test Dataset = \", f1_score(y_test, cls_test, average = 'weighted'))\n",
    "print(\"Accuracy for Train Dataset = \", accuracy_score(y_train, cls_train))\n",
    "print(\"Accuracy for Test Dataset = \", accuracy_score(y_test, cls_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7611d759",
   "metadata": {
    "papermill": {
     "duration": 7.203221,
     "end_time": "2022-06-06T15:49:01.373085",
     "exception": false,
     "start_time": "2022-06-06T15:48:54.169864",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### References\n",
    "[Reference 1](https://github.com/google-research/simclr)\n",
    "<br>\n",
    "[Reference 2](https://github.com/sayakpaul/SimCLR-in-TensorFlow-2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1771.043527,
   "end_time": "2022-06-06T15:49:11.250903",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-06T15:19:40.207376",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
