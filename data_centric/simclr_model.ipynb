{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2263a433",
   "metadata": {
    "papermill": {
     "duration": 0.028606,
     "end_time": "2022-06-05T17:09:20.231349",
     "exception": false,
     "start_time": "2022-06-05T17:09:20.202743",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A Simple Framework for Contrastive Learning of Visual Representations\n",
    "\n",
    "- This notebook is to use the popular SimCLR framework in Tensorflow2.0 along with the strategies used in this paper ([SimCLR](https://arxiv.org/abs/2002.05709)), to learn generalised representations/features for images in a self supervised fashion on CIFAR10. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4819e044",
   "metadata": {
    "papermill": {
     "duration": 0.026813,
     "end_time": "2022-06-05T17:09:20.285951",
     "exception": false,
     "start_time": "2022-06-05T17:09:20.259138",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SimCLR :\n",
    "1. Development of composition of data augmentation strategies to generate pairs of positive input data points that serve as an important component for discriminative learning. This could be considered as the hero element of the whole framework because defining the class of augmentations is clearly responsible towards defining the features the model will consider 2 distinguish between 2 items & features it will be invariant to while trying to find similarities between 2 items.\n",
    " \n",
    "2. Introduction of a non linear projection head that creates a buffer between the output of the encoder network & the input to loss function. This was an elegant solution towards learning generalisable features.\n",
    " \n",
    "3. Use of InfoNCE loss on feature similarity. This loss was introduced in [Improved Deep Metric Learning with\n",
    "Multi-class N-pair Loss Objective](https://www.nec-labs.com/uploads/images/Department-Images/MediaAnalytics/papers/nips16_npairmetriclearning.pdf)  with a major contribution of a proposed objective function that allows joint comparison among more than 1 negative example & reduces the computational load of using only N pairs of examples, instead of (N+1)Ã—N. The authors of SimCLR build on this loss function with the added innovation of usage of normalised encoded vectors & a temperature parameter to penalise hard negative samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d33de77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T17:09:20.350615Z",
     "iopub.status.busy": "2022-06-05T17:09:20.349788Z",
     "iopub.status.idle": "2022-06-05T17:09:31.540499Z",
     "shell.execute_reply": "2022-06-05T17:09:31.539927Z",
     "shell.execute_reply.started": "2022-06-05T16:59:24.297588Z"
    },
    "papermill": {
     "duration": 11.227879,
     "end_time": "2022-06-05T17:09:31.540660",
     "exception": false,
     "start_time": "2022-06-05T17:09:20.312781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imutils\r\n",
      "  Downloading imutils-0.5.4.tar.gz (17 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hBuilding wheels for collected packages: imutils\r\n",
      "  Building wheel for imutils (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25860 sha256=8204199279a94e560c364cdb5146daeb93aff7d782c3334fb1cfae46da731992\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/86/d7/0a/4923351ed1cec5d5e24c1eaf8905567b02a0343b24aa873df2\r\n",
      "Successfully built imutils\r\n",
      "Installing collected packages: imutils\r\n",
      "Successfully installed imutils-0.5.4\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "! pip install imutils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12d975d",
   "metadata": {
    "papermill": {
     "duration": 0.029366,
     "end_time": "2022-06-05T17:09:31.600472",
     "exception": false,
     "start_time": "2022-06-05T17:09:31.571106",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ecaa767",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T17:09:31.695180Z",
     "iopub.status.busy": "2022-06-05T17:09:31.685691Z",
     "iopub.status.idle": "2022-06-05T17:09:41.634709Z",
     "shell.execute_reply": "2022-06-05T17:09:41.635268Z",
     "shell.execute_reply.started": "2022-06-05T16:59:35.716253Z"
    },
    "papermill": {
     "duration": 9.982246,
     "end_time": "2022-06-05T17:09:41.635438",
     "exception": false,
     "start_time": "2022-06-05T17:09:31.653192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version :  2.6.2\n"
     ]
    }
   ],
   "source": [
    "# Basic Imports \n",
    "\n",
    "# Model mathematics\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "\n",
    "# Plotting libraries\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# Utilities\n",
    "import datetime\n",
    "import os,sys\n",
    "import tempfile\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "sys.path.append(\"/kaggle/input/helper-files\")\n",
    "import random \n",
    "import gc\n",
    "import time\n",
    "import functools\n",
    "from imutils import paths\n",
    "from tqdm import tqdm\n",
    "from typing import Callable\n",
    "from sklearn.metrics import accuracy_score, log_loss, confusion_matrix, f1_score\n",
    "\n",
    "# SimCLR Losses (extra files, will be explained further in the notebook)\n",
    "from semi_super_augPipe import preprocess_image\n",
    "from losses import _dot_simililarity_dim1 as sim_func_dim1, _dot_simililarity_dim2 as sim_func_dim2\n",
    "import helpers\n",
    "\n",
    "\n",
    "# Random seed fixation for experiment result repitition\n",
    "tf.random.set_seed(10)\n",
    "np.random.seed(10)\n",
    "\n",
    "print(\"Tensorflow version : \",tf.__version__)\n",
    "\n",
    "# In-order run function decorators in tf2.0\n",
    "tf.config.run_functions_eagerly(False)\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "866ecd46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T17:09:41.700281Z",
     "iopub.status.busy": "2022-06-05T17:09:41.699548Z",
     "iopub.status.idle": "2022-06-05T17:09:41.702013Z",
     "shell.execute_reply": "2022-06-05T17:09:41.701605Z",
     "shell.execute_reply.started": "2022-06-05T16:59:44.726170Z"
    },
    "papermill": {
     "duration": 0.036545,
     "end_time": "2022-06-05T17:09:41.702127",
     "exception": false,
     "start_time": "2022-06-05T17:09:41.665582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting the seeds\n",
    "SEED = 0\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbcefc3",
   "metadata": {
    "papermill": {
     "duration": 0.029657,
     "end_time": "2022-06-05T17:09:41.761957",
     "exception": false,
     "start_time": "2022-06-05T17:09:41.732300",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Paths & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b98529",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T17:09:41.834767Z",
     "iopub.status.busy": "2022-06-05T17:09:41.833092Z",
     "iopub.status.idle": "2022-06-05T17:09:41.835398Z",
     "shell.execute_reply": "2022-06-05T17:09:41.835814Z",
     "shell.execute_reply.started": "2022-06-05T16:59:44.743414Z"
    },
    "papermill": {
     "duration": 0.043072,
     "end_time": "2022-06-05T17:09:41.835967",
     "exception": false,
     "start_time": "2022-06-05T17:09:41.792895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# These are the usual ipython objects\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Defining a function to list the memory consumed\n",
    "# Only outputs variables taking at least 1MB space\n",
    "def list_storage(inp_dir):\n",
    "    # Get a sorted list of the objects and their sizes\n",
    "    vars_defined = [x for x in inp_dir if not x.startswith('_') and x not in sys.modules and x not in ipython_vars]\n",
    "    sto = sorted([(x, sys.getsizeof(globals().get(x))) for x in vars_defined], key=lambda x: x[1], reverse=True)\n",
    "    sto = [(x[0], str(round((x[1] / 2**20), 2)) + ' MB') for x in sto if x[1] >= 2**20]\n",
    "    print(tabulate(sto, headers = ['Variable', 'Storage (in MB)']))\n",
    "\n",
    "# In order to use this function, use the below line of code\n",
    "# list_storage(dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36a09a00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T17:09:41.916498Z",
     "iopub.status.busy": "2022-06-05T17:09:41.915863Z",
     "iopub.status.idle": "2022-06-05T17:10:08.833294Z",
     "shell.execute_reply": "2022-06-05T17:10:08.832395Z",
     "shell.execute_reply.started": "2022-06-05T16:59:44.755462Z"
    },
    "papermill": {
     "duration": 26.956306,
     "end_time": "2022-06-05T17:10:08.833446",
     "exception": false,
     "start_time": "2022-06-05T17:09:41.877140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing the Labelled Dataset\n",
    "df_train = pd.read_csv(\"../input/cifar10/train_lab_x.csv\")\n",
    "y_train = pd.read_csv(\"../input/cifar10/train_lab_y.csv\")\n",
    "\n",
    "# Importing the Test Dataset\n",
    "df_test = pd.read_csv(\"../input/cifar10/test_x.csv\")\n",
    "y_test = pd.read_csv(\"../input/cifar10/test_y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a672065f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T17:10:08.916131Z",
     "iopub.status.busy": "2022-06-05T17:10:08.914786Z",
     "iopub.status.idle": "2022-06-05T17:10:12.036627Z",
     "shell.execute_reply": "2022-06-05T17:10:12.036166Z",
     "shell.execute_reply.started": "2022-06-05T17:00:11.200478Z"
    },
    "papermill": {
     "duration": 3.168495,
     "end_time": "2022-06-05T17:10:12.036745",
     "exception": false,
     "start_time": "2022-06-05T17:10:08.868250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40006, 3072) (40006, 1)\n",
      "(40006, 3, 32, 32)\n",
      "[0]\n",
      "(40006, 32, 32, 3) (40006, 10)\n",
      "(10000, 3072) (10000, 1)\n",
      "(10000, 3, 32, 32)\n",
      "(10000, 32, 32, 3) (10000, 10)\n",
      "Tensorflow version :  2.6.2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 108x108 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = np.array(df_train)\n",
    "y_train = np.array(y_train)\n",
    "print(df_train.shape, y_train.shape)\n",
    "\n",
    "# Reshaping the dataset\n",
    "df_train = np.reshape(df_train, (-1, 3, 32, 32))\n",
    "print(df_train.shape)\n",
    "\n",
    "# Visualizing a single image\n",
    "ind = 11\n",
    "example = df_train[ind, : , : , : ]\n",
    "example = example.transpose((1, 2, 0))\n",
    "plt.figure(figsize=(1.5, 1.5))\n",
    "plt.imshow(example)\n",
    "print(y_train[ind])\n",
    "\n",
    "# Creating a random permutation\n",
    "perm = np.random.permutation(df_train.shape[0])\n",
    "\n",
    "# Shuffling the training dataset\n",
    "df_train = df_train[perm, : , : , : ]\n",
    "y_train = y_train[perm]\n",
    "\n",
    "# Reshaping, rescaling and one-hot encoding\n",
    "df_train = np.transpose(np.array(df_train), (0, 2, 3, 1))\n",
    "df_train = df_train / 255\n",
    "y_train_oh = tf.one_hot(np.ravel(y_train), depth = 10)\n",
    "\n",
    "print(df_train.shape, y_train_oh.shape)\n",
    "\n",
    "df_test = np.array(df_test)\n",
    "y_test = np.array(y_test)\n",
    "print(df_test.shape, y_test.shape)\n",
    "\n",
    "# Reshaping the dataset\n",
    "df_test = np.reshape(df_test, (-1, 3, 32, 32))\n",
    "print(df_test.shape)\n",
    "\n",
    "# Reshaping, rescaling and one-hot encoding\n",
    "df_test = np.transpose(np.array(df_test), (0, 2, 3, 1))\n",
    "df_test = df_test / 255\n",
    "y_test_oh = tf.one_hot(np.ravel(y_test), depth = 10)\n",
    "print(df_test.shape, y_test_oh.shape)\n",
    "\n",
    "# Random seed fixation for experiment result repitition\n",
    "tf.random.set_seed(10)\n",
    "np.random.seed(10)\n",
    "\n",
    "print(\"Tensorflow version : \",tf.__version__)\n",
    "\n",
    "# In-order run function decorators in tf2.0\n",
    "tf.config.run_functions_eagerly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8560c6e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T17:10:12.104989Z",
     "iopub.status.busy": "2022-06-05T17:10:12.104349Z",
     "iopub.status.idle": "2022-06-05T17:10:12.107991Z",
     "shell.execute_reply": "2022-06-05T17:10:12.107578Z",
     "shell.execute_reply.started": "2022-06-05T17:00:14.124605Z"
    },
    "papermill": {
     "duration": 0.039441,
     "end_time": "2022-06-05T17:10:12.108099",
     "exception": false,
     "start_time": "2022-06-05T17:10:12.068658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Image training properties \n",
    "IMG_H, IMG_W = (32, 32)\n",
    "\n",
    "# How powerful you want the colour augmentations to be\n",
    "color_jitter_strength = 0.3\n",
    "\n",
    "# Minimum crop area you want\n",
    "minimum_object_coverage = 0.7\n",
    "\n",
    "# Range of crop area\n",
    "area_range = (minimum_object_coverage, 1.0)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c01a6c",
   "metadata": {
    "papermill": {
     "duration": 0.031315,
     "end_time": "2022-06-05T17:10:12.170851",
     "exception": false,
     "start_time": "2022-06-05T17:10:12.139536",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01e073b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T17:10:12.241572Z",
     "iopub.status.busy": "2022-06-05T17:10:12.240026Z",
     "iopub.status.idle": "2022-06-05T17:10:12.242164Z",
     "shell.execute_reply": "2022-06-05T17:10:12.242566Z",
     "shell.execute_reply.started": "2022-06-05T17:00:14.130891Z"
    },
    "papermill": {
     "duration": 0.040149,
     "end_time": "2022-06-05T17:10:12.242684",
     "exception": false,
     "start_time": "2022-06-05T17:10:12.202535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "@tf.function\n",
    "def input_image_loader(image):\n",
    "    image_norm = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    \n",
    "    # The IMG_H & IMG_W are constants that will be read only once during graph tracing step\n",
    "    image_norm = tf.image.resize(image_norm, size=[IMG_H, IMG_W])\n",
    "    \n",
    "\n",
    "    aug_image_1 = preprocess_image(image = image_norm, \n",
    "                                    height = IMG_H, \n",
    "                                    width  = IMG_W, \n",
    "                                    cjs = color_jitter_strength,\n",
    "                                    m_obj_cov = minimum_object_coverage,\n",
    "                                    a_range = area_range)\n",
    "\n",
    "    aug_image_2 = preprocess_image(image = image_norm, \n",
    "                                    height = IMG_H, \n",
    "                                    width  = IMG_W, \n",
    "                                    cjs = color_jitter_strength,\n",
    "                                    m_obj_cov = minimum_object_coverage,\n",
    "                                    a_range = area_range)\n",
    "    # view 1 & view 2\n",
    "    return aug_image_1, aug_image_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3540277",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T17:10:12.311636Z",
     "iopub.status.busy": "2022-06-05T17:10:12.310750Z",
     "iopub.status.idle": "2022-06-05T17:10:18.224231Z",
     "shell.execute_reply": "2022-06-05T17:10:18.224649Z",
     "shell.execute_reply.started": "2022-06-05T17:00:14.140954Z"
    },
    "papermill": {
     "duration": 5.950556,
     "end_time": "2022-06-05T17:10:18.224811",
     "exception": false,
     "start_time": "2022-06-05T17:10:12.274255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TFDS\n",
    "train_tensor = tf.data.Dataset.from_tensor_slices(df_train)\n",
    "train_ds_shuffled = train_tensor.shuffle(len(train_tensor))\n",
    "final_train_ds = (train_ds_shuffled\n",
    "                .map(input_image_loader, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "                .batch(BATCH_SIZE, drop_remainder=True)\n",
    "                .prefetch(tf.data.experimental.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0480881",
   "metadata": {
    "papermill": {
     "duration": 0.031634,
     "end_time": "2022-06-05T17:10:18.288631",
     "exception": false,
     "start_time": "2022-06-05T17:10:18.256997",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f8ee416",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T17:10:18.360675Z",
     "iopub.status.busy": "2022-06-05T17:10:18.360101Z",
     "iopub.status.idle": "2022-06-05T17:10:20.152191Z",
     "shell.execute_reply": "2022-06-05T17:10:20.152580Z",
     "shell.execute_reply.started": "2022-06-05T17:00:20.282871Z"
    },
    "papermill": {
     "duration": 1.832462,
     "end_time": "2022-06-05T17:10:20.152731",
     "exception": false,
     "start_time": "2022-06-05T17:10:18.320269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batch in final_train_ds.take(1):\n",
    "    pass\n",
    "\n",
    "image_to_viz = 3\n",
    "plt.figure(figsize=(7, 7))\n",
    "ax1 = plt.subplot(2, 2, 1)\n",
    "plt.imshow(batch[0][image_to_viz].numpy().astype('float32'), interpolation = 'none', vmin = 0, vmax = 1)\n",
    "ax1.grid(False)\n",
    "ax2 = plt.subplot(2, 2, 2)\n",
    "plt.imshow(batch[1][image_to_viz].numpy().astype('float32'), interpolation = 'none', vmin = 0, vmax = 1)\n",
    "ax2.grid(False)\n",
    "\n",
    "ax3 = plt.subplot(2, 2, 3)\n",
    "plt.hist(batch[0][image_to_viz].numpy().ravel())\n",
    "ax4 = plt.subplot(2, 2, 4, sharey = ax3, sharex=ax3)\n",
    "plt.hist(batch[1][image_to_viz].numpy().ravel())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77869fa6",
   "metadata": {
    "papermill": {
     "duration": 0.033089,
     "end_time": "2022-06-05T17:10:20.218495",
     "exception": false,
     "start_time": "2022-06-05T17:10:20.185406",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ff79a1",
   "metadata": {
    "papermill": {
     "duration": 0.032047,
     "end_time": "2022-06-05T17:10:20.282823",
     "exception": false,
     "start_time": "2022-06-05T17:10:20.250776",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Model helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff816acf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T17:10:20.372337Z",
     "iopub.status.busy": "2022-06-05T17:10:20.370616Z",
     "iopub.status.idle": "2022-06-05T17:10:20.372925Z",
     "shell.execute_reply": "2022-06-05T17:10:20.373354Z",
     "shell.execute_reply.started": "2022-06-05T17:00:22.182960Z"
    },
    "papermill": {
     "duration": 0.057086,
     "end_time": "2022-06-05T17:10:20.373485",
     "exception": false,
     "start_time": "2022-06-05T17:10:20.316399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# adding l2 reg\n",
    "def add_regularization(model, regularizer = tf.keras.regularizers.l2(0.0001)):\n",
    "    \"\"\"\n",
    "    Helper function to add l2 regularisation to each layer of a either a preTrained or \n",
    "    randomly initialised built in model\n",
    "    Arguments:\n",
    "        model : (keras.model) input model \n",
    "        regularizer : ( tf.keras.regularizers.l2) object from keras that defines a l2 regularizer\n",
    "    Returns:\n",
    "        model : all layers contain the \"regularizer\" object & incase we pass a pretrained model then the \n",
    "                original weights are preserved\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(regularizer, tf.keras.regularizers.Regularizer):\n",
    "        print(\"Regularizer must be a subclass of tf.keras.regularizers.Regularizer\")\n",
    "        return model\n",
    "\n",
    "    for layer in model.layers:\n",
    "        for attr in ['kernel_regularizer']:\n",
    "            if hasattr(layer, attr):\n",
    "                setattr(layer, attr, regularizer)\n",
    "\n",
    "    # When we change the layers attributes, the change only happens in the model config file\n",
    "    model_json = model.to_json()\n",
    "\n",
    "    # Save the weights before reloading the model.\n",
    "    tmp_weights_path = os.path.join(tempfile.gettempdir(), 'tmp_weights.h5')\n",
    "    model.save_weights(tmp_weights_path)\n",
    "\n",
    "    # load the model from the config\n",
    "    model = tf.keras.models.model_from_json(model_json)\n",
    "    \n",
    "    # Reload the model weights\n",
    "    model.load_weights(tmp_weights_path, by_name=True)\n",
    "    return model\n",
    "\n",
    "# Architecture utils\n",
    "def get_simclr(hidden_1, \n",
    "               hidden_2, \n",
    "               l2_penalty = 10e-6):\n",
    "    \"\"\"\n",
    "    Main function to define the entire network backbone to train\n",
    "    Arguments:\n",
    "        hidden_1-2 : (int) variable to define number of neurons in the projection head dense layer\n",
    "        l2_penalty : (float) to define the amount of l2 penalty applied to each layer's weights\n",
    "    Returns:\n",
    "        final_model : (tf.keras.Model) final model that will be trained \n",
    "    \"\"\"\n",
    "    \n",
    "    # encoder network\n",
    "    base_model = tf.keras.applications.ResNet50V2(include_top = False, \n",
    "                                                  weights = None, \n",
    "                                                  input_shape = (IMG_H, IMG_W, 3))\n",
    "    \n",
    "    # defining l2 regularization\n",
    "    regularizer = tf.keras.regularizers.l2(l2_penalty)\n",
    "    reg_base_model = add_regularization(base_model,regularizer)\n",
    "    reg_base_model.trainable = True\n",
    "\n",
    "    # Joining the entire pipeline using functional API\n",
    "    inputs = Input((IMG_H, IMG_W, 3))\n",
    "    h = reg_base_model(inputs, training=True)\n",
    "    h = GlobalAveragePooling2D()(h)\n",
    "    \n",
    "    # Non linear projection layer to improve the quality of embeddings being produced\n",
    "    projection_1 = Dense(hidden_1, kernel_regularizer = regularizer)(h)\n",
    "    projection_1 = tf.keras.layers.BatchNormalization()(projection_1)\n",
    "    projection_1 = Activation(\"relu\")(projection_1)\n",
    "    projection_2 = Dense(hidden_2, kernel_regularizer = regularizer)(projection_1)\n",
    "    projection_2 = tf.keras.layers.BatchNormalization()(projection_2)\n",
    "    projection_2 = Activation(\"relu\")(projection_2)\n",
    "\n",
    "    # Final Model\n",
    "    final_model = Model(inputs, projection_2)\n",
    "    return final_model\n",
    "\n",
    "def simclr_model(input_shape):\n",
    "    \"\"\"\n",
    "    Main function to define the entire network backbone to train\n",
    "    Arguments:\n",
    "        hidden_1-2 : (int) variable to define number of neurons in the projection head dense layer\n",
    "        l2_penalty : (float) to define the amount of l2 penalty applied to each layer's weights\n",
    "    Returns:\n",
    "        final_model : (tf.keras.Model) final model that will be trained \n",
    "    \"\"\"\n",
    "    \n",
    "#     # encoder network\n",
    "#     base_model = tf.keras.applications.ResNet50V2(include_top = False, \n",
    "#                                                   weights = None, \n",
    "#                                                   input_shape = (IMG_H, IMG_W, 3))\n",
    "    \n",
    "#     # defining l2 regularization\n",
    "#     regularizer = tf.keras.regularizers.l2(l2_penalty)\n",
    "#     reg_base_model = add_regularization(base_model,regularizer)\n",
    "#     reg_base_model.trainable = True\n",
    "\n",
    "#     # Joining the entire pipeline using functional API\n",
    "#     inputs = Input((IMG_H, IMG_W, 3))\n",
    "#     h = reg_base_model(inputs, training=True)\n",
    "#     h = GlobalAveragePooling2D()(h)\n",
    "    \n",
    "#     # Non linear projection layer to improve the quality of embeddings being produced\n",
    "#     projection_1 = Dense(hidden_1, kernel_regularizer = regularizer)(h)\n",
    "#     projection_1 = tf.keras.layers.BatchNormalization()(projection_1)\n",
    "#     projection_1 = Activation(\"relu\")(projection_1)\n",
    "#     projection_2 = Dense(hidden_2, kernel_regularizer = regularizer)(projection_1)\n",
    "#     projection_2 = tf.keras.layers.BatchNormalization()(projection_2)\n",
    "#     projection_2 = Activation(\"relu\")(projection_2)\n",
    "\n",
    "    input_img = tf.keras.Input(shape = input_shape)\n",
    "    \n",
    "    Z1 = tfl.Conv2D(16, kernel_size=3, strides=1, padding='same', activation='relu')(input_img)\n",
    "    B1 = tfl.BatchNormalization(axis=-1)(Z1)\n",
    "    Z2 = tfl.Conv2D(16, kernel_size=3, strides=1, padding='same', activation='relu')(B1)\n",
    "    B2 = tfl.BatchNormalization(axis=-1)(Z2)\n",
    "    P1 = tfl.MaxPool2D(pool_size=2, strides=2, padding='valid')(B2)\n",
    "    D1 = tfl.Dropout(0.25)(P1)\n",
    "    # (16, 16, 16)\n",
    "    \n",
    "    Z3 = tfl.Conv2D(32, kernel_size=2, strides=1, padding='valid', activation='relu')(D1)\n",
    "    B3 = tfl.BatchNormalization(axis=-1)(Z3)\n",
    "    Z4 = tfl.Conv2D(32, kernel_size=2, strides=1, padding='valid', activation='relu')(B3)\n",
    "    B4 = tfl.BatchNormalization(axis=-1)(Z4)\n",
    "    P2 = tfl.MaxPool2D(pool_size=2, strides=2, padding='valid')(B4)\n",
    "    D2 = tfl.Dropout(0.25)(P2)\n",
    "    # (7, 7, 32)\n",
    "    \n",
    "    F1 = tfl.Flatten()(D2)\n",
    "    Den1 = tfl.Dense(256, activation='relu')(F1)\n",
    "    Drop1 = tfl.Dropout(0.25)(Den1)\n",
    "    Den2 = tfl.Dense(64, activation='relu')(Drop1)\n",
    "    Drop2 = tfl.Dropout(0.25)(Den2)\n",
    "    outputs = tfl.Dense(10, activation='softmax')(Drop2)\n",
    "    \n",
    "    model = tf.keras.Model(inputs = input_img, outputs = outputs)\n",
    "    return model\n",
    "\n",
    "    # Final Model\n",
    "    final_model = Model(inputs, outputs)\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d288207",
   "metadata": {
    "papermill": {
     "duration": 0.033789,
     "end_time": "2022-06-05T17:10:20.449881",
     "exception": false,
     "start_time": "2022-06-05T17:10:20.416092",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Defining a negative Mask \n",
    "\n",
    "This function has been define in the other files attached & the main goal of it is to define a mask when we compute loss of negative comparisons. Here is the output of this code visualised : \n",
    "\n",
    "**This negative mask will allow us to ignore the positive pairs in an augmented batch & help us focus only on the negative stuff !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70e063b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T17:10:20.519286Z",
     "iopub.status.busy": "2022-06-05T17:10:20.518512Z",
     "iopub.status.idle": "2022-06-05T17:10:20.521014Z",
     "shell.execute_reply": "2022-06-05T17:10:20.520555Z",
     "shell.execute_reply.started": "2022-06-05T17:00:22.207099Z"
    },
    "papermill": {
     "duration": 0.038869,
     "end_time": "2022-06-05T17:10:20.521168",
     "exception": false,
     "start_time": "2022-06-05T17:10:20.482299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mask to remove positive examples from the batch of negative samples\n",
    "negative_mask = helpers.get_negative_mask(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0667dfa5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T17:10:20.591060Z",
     "iopub.status.busy": "2022-06-05T17:10:20.590465Z",
     "iopub.status.idle": "2022-06-05T17:10:21.360987Z",
     "shell.execute_reply": "2022-06-05T17:10:21.360057Z",
     "shell.execute_reply.started": "2022-06-05T17:00:22.220204Z"
    },
    "papermill": {
     "duration": 0.807834,
     "end_time": "2022-06-05T17:10:21.361124",
     "exception": false,
     "start_time": "2022-06-05T17:10:20.553290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! rm -rf ./logs\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "549759be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T17:10:21.440688Z",
     "iopub.status.busy": "2022-06-05T17:10:21.439893Z",
     "iopub.status.idle": "2022-06-05T17:10:21.442357Z",
     "shell.execute_reply": "2022-06-05T17:10:21.442765Z",
     "shell.execute_reply.started": "2022-06-05T17:00:22.939798Z"
    },
    "papermill": {
     "duration": 0.047169,
     "end_time": "2022-06-05T17:10:21.442891",
     "exception": false,
     "start_time": "2022-06-05T17:10:21.395722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(xis, \n",
    "               xjs, \n",
    "               model, \n",
    "               optimizer, \n",
    "               criterion, \n",
    "               temperature):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        zis = model(xis)\n",
    "        zjs = model(xjs)\n",
    "\n",
    "        # normalize projection feature vectors : onto a unit hypersphere\n",
    "        zis = tf.math.l2_normalize(zis, axis=1)\n",
    "        zjs = tf.math.l2_normalize(zjs, axis=1)\n",
    "          \n",
    "        # calculating over set of all positive pairs ( computing all numerators of softmax)\n",
    "        l_pos = sim_func_dim1(zis, zjs)\n",
    "        l_pos = tf.reshape(l_pos, (BATCH_SIZE, 1))\n",
    "\n",
    "        # temperature scaling\n",
    "        l_pos /= temperature\n",
    "        \n",
    "        # make the batch dimension 2*n\n",
    "        negatives = tf.concat([zjs, zis], axis=0)\n",
    "\n",
    "        loss = 0\n",
    "        for positives in [zis, zjs]:\n",
    "            # computing similarity with a data point & all the possible negatives\n",
    "            l_neg = sim_func_dim2(positives, negatives)\n",
    "            \n",
    "            # since each data point is its own class\n",
    "            labels = tf.zeros(BATCH_SIZE, dtype=tf.int32)\n",
    "            \n",
    "            # using the negative mask to remove itself & its positve counterpart to compute negative sim\n",
    "            l_neg = tf.boolean_mask(l_neg, negative_mask)\n",
    "            l_neg = tf.reshape(l_neg, (BATCH_SIZE, -1))\n",
    "\n",
    "            # temperature scaling\n",
    "            l_neg /= temperature\n",
    "\n",
    "            logits = tf.concat([l_pos, l_neg], axis=1) \n",
    "            loss += criterion(y_pred = logits, y_true = labels)\n",
    "\n",
    "        # since for every data point including its augmentation we compute the loss thus divide by 2*BatchSize\n",
    "        loss = loss / (2 * BATCH_SIZE)\n",
    "    \n",
    "    # Compute & apply the gradients on traininable paramters of the model\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    # updated model along with gradients so that we can visualise them on tensorboard.\n",
    "    return loss, gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee349d4e",
   "metadata": {
    "papermill": {
     "duration": 0.032377,
     "end_time": "2022-06-05T17:10:21.507763",
     "exception": false,
     "start_time": "2022-06-05T17:10:21.475386",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ea7c20a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T17:10:21.586324Z",
     "iopub.status.busy": "2022-06-05T17:10:21.585566Z",
     "iopub.status.idle": "2022-06-05T17:10:21.587601Z",
     "shell.execute_reply": "2022-06-05T17:10:21.587981Z",
     "shell.execute_reply.started": "2022-06-05T17:00:22.953719Z"
    },
    "papermill": {
     "duration": 0.047732,
     "end_time": "2022-06-05T17:10:21.588106",
     "exception": false,
     "start_time": "2022-06-05T17:10:21.540374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_simclr(model, \n",
    "                 train_dataset, \n",
    "                 optimizer, \n",
    "                 criterion,\n",
    "                 temperature=0.1, \n",
    "                 epochs=100,\n",
    "                 num_train_samples_viz = 5,\n",
    "                 num_test_samples_viz = 2,\n",
    "                metrics = 'accuracy'):\n",
    "    \n",
    "    \"\"\"\n",
    "      Training the model function\n",
    "    \"\"\"\n",
    "    \n",
    " \n",
    "    print(\"Starting training procedure .... : \")\n",
    "    print(\"Number of steps per epoch : \",len(train_dataset))\n",
    "    \n",
    "    # To measure per epoch time taken\n",
    "    t_start = time.time()\n",
    "    \n",
    "    # Visualisation lists\n",
    "    lr_epoch = []\n",
    "    epoch_wise_loss = []\n",
    "\n",
    "    for epoch in range(0, epochs):\n",
    "        \n",
    "        # Reset loss collection each step\n",
    "        step_wise_loss = []\n",
    "\n",
    "        # Number of grad descent steps in 1 epoch\n",
    "        num_train_steps = len(train_dataset) \n",
    "\n",
    "        # Picking up random batches & taking first image for input check\n",
    "        random_batches_train = random.sample(range(len(train_dataset)),num_train_samples_viz)\n",
    "        cnt = 0\n",
    "\n",
    "        # Arrays for tensorboard visualisation\n",
    "        random_collection_train_sample_1 = []\n",
    "        random_collection_train_sample_2 = []\n",
    "        gradArray = None\n",
    "        loss = None \n",
    "\n",
    "        # Training loop\n",
    "        for image_batch in tqdm(train_dataset):\n",
    "\n",
    "            # Fetching both views for input\n",
    "            a = image_batch[0]\n",
    "            b = image_batch[1]\n",
    "\n",
    "            # Train one batch\n",
    "            loss, gradArray = train_step(a, b, model, optimizer, criterion, temperature)\n",
    "            step_wise_loss.append(loss)\n",
    "\n",
    "            # Check whether to take image from this batch or not\n",
    "            if cnt in random_batches_train:\n",
    "                random_collection_train_sample_1.append(image_batch[0][0])\n",
    "                random_collection_train_sample_2.append(image_batch[1][0])\n",
    "            cnt+=1\n",
    "        \n",
    "        # Average loss throughout the whole process\n",
    "        if not len(epoch_wise_loss):\n",
    "            epoch_wise_loss.append(np.mean(step_wise_loss))\n",
    "        else:\n",
    "            # Adding the mean of previous ones\n",
    "            mean_value = (np.sum(step_wise_loss) + epoch_wise_loss[-1]*(epoch)*num_train_steps)/((epoch+1)*num_train_steps)\n",
    "            epoch_wise_loss.append(mean_value)\n",
    "        \n",
    "        # Printing the loss progression\n",
    "        print(\"\\n epoch: {} | train loss: {:.8f} | lr : {} | {:.4f} mins\"\n",
    "              .format(epoch + 1,epoch_wise_loss[-1],optimizer._decayed_lr(tf.float32).numpy(), (time.time()-t_start)/60.0))    \n",
    "   \n",
    "        # Appending the value of learning rate for warmup + cosine decay visualisation\n",
    "        lr_epoch.append(optimizer._decayed_lr(tf.float32).numpy())\n",
    "        \n",
    "        # saving models \n",
    "        print(\"Saving Base Model.....\")\n",
    "        \n",
    "        # Saving the entire model for checkpointing reasons\n",
    "        model.save(\"./\" + modelNameStr + \".h5\")\n",
    "\n",
    "        # saving the state of optimizer\n",
    "        np.save(\"./\" + modelNameStr + \"_optimizer.npy\", optimizer.get_weights())\n",
    "\n",
    "    \n",
    "    return epoch_wise_loss, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8599b7",
   "metadata": {
    "papermill": {
     "duration": 0.032637,
     "end_time": "2022-06-05T17:10:21.653141",
     "exception": false,
     "start_time": "2022-06-05T17:10:21.620504",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Optimiser schedule\n",
    "\n",
    "This is a custom learning rate scheduler written according to SimCLR authors. The learning rate starts from a low value to an initial value linearly after which the cosine decay schedule beigns. The warmup_steps indicate number of epochs taken by optimiser to increase linearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aeb40136",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T17:10:21.729313Z",
     "iopub.status.busy": "2022-06-05T17:10:21.722758Z",
     "iopub.status.idle": "2022-06-05T17:10:21.731775Z",
     "shell.execute_reply": "2022-06-05T17:10:21.731329Z",
     "shell.execute_reply.started": "2022-06-05T17:00:22.973837Z"
    },
    "papermill": {
     "duration": 0.046445,
     "end_time": "2022-06-05T17:10:21.731906",
     "exception": false,
     "start_time": "2022-06-05T17:10:21.685461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WarmUp(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self,\n",
    "               initial_learning_rate: float,\n",
    "               decay_schedule_fn: Callable,\n",
    "               warmup_steps: int,\n",
    "               power: float = 1.0,\n",
    "               name: str = None,):\n",
    "    \n",
    "    super().__init__()\n",
    "    self.initial_learning_rate = initial_learning_rate\n",
    "    self.warmup_steps = warmup_steps\n",
    "    self.power = power\n",
    "    self.decay_schedule_fn = decay_schedule_fn\n",
    "    self.name = name\n",
    "\n",
    "  def __call__(self, step):\n",
    "    with tf.name_scope(self.name or \"WarmUp\") as name:\n",
    "        # Implements polynomial warmup. i.e., if global_step < warmup_steps, the\n",
    "        # learning rate will be `global_step/num_warmup_steps * init_lr`.\n",
    "        global_step_float = tf.cast(step, tf.float32)\n",
    "        warmup_steps_float = tf.cast(self.warmup_steps, tf.float32)\n",
    "        warmup_percent_done = global_step_float / warmup_steps_float\n",
    "        warmup_learning_rate = self.initial_learning_rate * tf.math.pow(warmup_percent_done, self.power)\n",
    "        return tf.cond(\n",
    "            global_step_float < warmup_steps_float,\n",
    "            lambda: warmup_learning_rate,\n",
    "            lambda: self.decay_schedule_fn(step - self.warmup_steps),\n",
    "            name=name,\n",
    "        )\n",
    "\n",
    "  def get_config(self):\n",
    "    return {\n",
    "        \"initial_learning_rate\": self.initial_learning_rate,\n",
    "        \"decay_schedule_fn\": self.decay_schedule_fn,\n",
    "        \"warmup_steps\": self.warmup_steps,\n",
    "        \"power\": self.power,\n",
    "        \"name\": self.name,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ae1f42",
   "metadata": {
    "papermill": {
     "duration": 0.033637,
     "end_time": "2022-06-05T17:10:21.800047",
     "exception": false,
     "start_time": "2022-06-05T17:10:21.766410",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4777acbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T17:10:21.872241Z",
     "iopub.status.busy": "2022-06-05T17:10:21.871387Z",
     "iopub.status.idle": "2022-06-05T17:10:21.873980Z",
     "shell.execute_reply": "2022-06-05T17:10:21.873508Z",
     "shell.execute_reply.started": "2022-06-05T17:00:22.986799Z"
    },
    "papermill": {
     "duration": 0.040928,
     "end_time": "2022-06-05T17:10:21.874101",
     "exception": false,
     "start_time": "2022-06-05T17:10:21.833173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as tfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff4dbeca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T17:10:21.962518Z",
     "iopub.status.busy": "2022-06-05T17:10:21.961085Z",
     "iopub.status.idle": "2022-06-05T17:10:24.900770Z",
     "shell.execute_reply": "2022-06-05T17:10:24.901286Z",
     "shell.execute_reply.started": "2022-06-05T17:00:23.000257Z"
    },
    "papermill": {
     "duration": 2.992603,
     "end_time": "2022-06-05T17:10:24.901449",
     "exception": false,
     "start_time": "2022-06-05T17:10:21.908846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelName :  ./resNet_simCLR_decay_10e6_color_0.3_min_obj_0.7_avgPool_loss_tmp_0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decay Steps :  6250\n",
      "['conv5_block2_2_bn/beta:0', 'conv5_block3_preact_bn/gamma:0', 'conv5_block3_1_bn/beta:0', 'conv5_block3_3_conv/kernel:0', 'dense/kernel:0', 'dense_1/kernel:0']\n"
     ]
    }
   ],
   "source": [
    "# Defining the loss function\n",
    "criterion = tf.keras.losses.CategoricalCrossentropy(reduction=tf.keras.losses.Reduction.SUM)\n",
    "# Number of epochs\n",
    "tot_epochs = 5\n",
    "\n",
    "# temperature in NTXent\n",
    "loss_temp = 0.2\n",
    "\n",
    "# Defining the SimCLR model\n",
    "# Differentiating models via their hyper-parameter values\n",
    "modelNameStr = \"./resNet_simCLR_decay_10e6_color_\"+str(color_jitter_strength)+\"_min_obj_\"+str(minimum_object_coverage)+\"_avgPool_loss_tmp_\"+str(loss_temp)\n",
    "print(\"ModelName : \",modelNameStr) \n",
    "simclr_2 = get_simclr(32, 32)\n",
    "\n",
    "\n",
    "# optimiser decay schedule\n",
    "decay_steps = (len(final_train_ds))*tot_epochs\n",
    "warmup_steps = (len(final_train_ds))*10\n",
    "initial_lr = 0.5e-3\n",
    "\n",
    "# Cosine decay function\n",
    "lr_decayed_fn = tf.keras.experimental.CosineDecay(initial_learning_rate = initial_lr, \n",
    "                                                  decay_steps = decay_steps)\n",
    "cosine_with_warmUp = WarmUp(initial_learning_rate = initial_lr,\n",
    "                            decay_schedule_fn = lr_decayed_fn,\n",
    "                            warmup_steps = warmup_steps)\n",
    "\n",
    "print(\"Decay Steps : \",decay_steps)\n",
    "optimizer = tf.keras.optimizers.Adam(cosine_with_warmUp)\n",
    "\n",
    "# Learning a layer to number mapping for kernel weight & gradient visualisation of last few CNN layers & projection head\n",
    "tot = []\n",
    "for i in simclr_2.layers:\n",
    "  for j in i.trainable_weights:\n",
    "    tot.append(j.name)\n",
    "cnt = 0\n",
    "\n",
    "layer_names = [tot[-23], tot[-20], tot[-16], tot[-12], tot[-8], tot[-4]]\n",
    "print(layer_names)\n",
    "index_to_layer = {}\n",
    "for i in simclr_2.layers:\n",
    "  for j in i.trainable_weights:\n",
    "    index_to_layer[cnt] = j.name\n",
    "    cnt+=1\n",
    "layer_to_index = {j:i for i,j in index_to_layer.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d65d63b",
   "metadata": {
    "papermill": {
     "duration": 0.0355,
     "end_time": "2022-06-05T17:10:24.974088",
     "exception": false,
     "start_time": "2022-06-05T17:10:24.938588",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training SimCLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd062f73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T17:10:25.049240Z",
     "iopub.status.busy": "2022-06-05T17:10:25.048395Z",
     "iopub.status.idle": "2022-06-05T17:10:25.141156Z",
     "shell.execute_reply": "2022-06-05T17:10:25.140386Z",
     "shell.execute_reply.started": "2022-06-05T17:00:26.070999Z"
    },
    "papermill": {
     "duration": 0.133604,
     "end_time": "2022-06-05T17:10:25.141277",
     "exception": false,
     "start_time": "2022-06-05T17:10:25.007673",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "simclr = simclr_model((32, 32, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88789cd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T17:10:25.213795Z",
     "iopub.status.busy": "2022-06-05T17:10:25.212878Z",
     "iopub.status.idle": "2022-06-05T17:10:27.315957Z",
     "shell.execute_reply": "2022-06-05T17:10:27.315397Z",
     "shell.execute_reply.started": "2022-06-05T17:00:26.176772Z"
    },
    "papermill": {
     "duration": 2.14053,
     "end_time": "2022-06-05T17:10:27.316085",
     "exception": false,
     "start_time": "2022-06-05T17:10:25.175555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Composing the Train Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((df_train, y_train_oh)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "578498b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T17:10:27.408191Z",
     "iopub.status.busy": "2022-06-05T17:10:27.405324Z",
     "iopub.status.idle": "2022-06-05T17:31:33.713389Z",
     "shell.execute_reply": "2022-06-05T17:31:33.687302Z",
     "shell.execute_reply.started": "2022-06-05T17:00:28.315199Z"
    },
    "papermill": {
     "duration": 1266.361728,
     "end_time": "2022-06-05T17:31:33.713568",
     "exception": false,
     "start_time": "2022-06-05T17:10:27.351840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1251/1251 [==============================] - 13s 5ms/step - loss: 55.5166 - accuracy: 0.3729\n",
      "Epoch 2/10\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 43.0959 - accuracy: 0.5196\n",
      "Epoch 3/10\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 37.5066 - accuracy: 0.5884\n",
      "Epoch 4/10\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 33.9613 - accuracy: 0.6308\n",
      "Epoch 5/10\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 31.2375 - accuracy: 0.6615\n",
      "Epoch 6/10\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 28.9578 - accuracy: 0.6821\n",
      "Epoch 7/10\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 27.4477 - accuracy: 0.7053\n",
      "Epoch 8/10\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 25.7280 - accuracy: 0.7193\n",
      "Epoch 9/10\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 24.4795 - accuracy: 0.7342\n",
      "Epoch 10/10\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 23.4492 - accuracy: 0.7436\n",
      "For  10  Epochs:\n",
      "Log-loss for Train Dataset =  0.6377223281929053\n",
      "Log-loss for Test Dataset =  0.9216722027031481\n",
      "Accuracy for Train Dataset =  0.7730090486427036\n",
      "Accuracy for Test Dataset =  0.6966\n",
      "\n",
      "Epoch 1/20\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 55.9803 - accuracy: 0.3691\n",
      "Epoch 2/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 42.6864 - accuracy: 0.5253\n",
      "Epoch 3/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 36.2585 - accuracy: 0.6031\n",
      "Epoch 4/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 32.3280 - accuracy: 0.6503\n",
      "Epoch 5/20\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 30.0060 - accuracy: 0.6769\n",
      "Epoch 6/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 27.8497 - accuracy: 0.6992\n",
      "Epoch 7/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 26.1526 - accuracy: 0.7170\n",
      "Epoch 8/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 24.7673 - accuracy: 0.7324\n",
      "Epoch 9/20\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 23.7119 - accuracy: 0.7446\n",
      "Epoch 10/20\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 22.6580 - accuracy: 0.7548\n",
      "Epoch 11/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 21.8641 - accuracy: 0.7639\n",
      "Epoch 12/20\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 20.9151 - accuracy: 0.7749\n",
      "Epoch 13/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 20.1833 - accuracy: 0.7814\n",
      "Epoch 14/20\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 19.3878 - accuracy: 0.7892\n",
      "Epoch 15/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 18.8858 - accuracy: 0.7961\n",
      "Epoch 16/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 18.3450 - accuracy: 0.8009\n",
      "Epoch 17/20\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 17.7280 - accuracy: 0.8092\n",
      "Epoch 18/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 17.2639 - accuracy: 0.8074\n",
      "Epoch 19/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 16.9649 - accuracy: 0.8164\n",
      "Epoch 20/20\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 16.2397 - accuracy: 0.8247\n",
      "For  20  Epochs:\n",
      "Log-loss for Train Dataset =  0.28496615163892286\n",
      "Log-loss for Test Dataset =  0.7694937183838158\n",
      "Accuracy for Train Dataset =  0.904139379093136\n",
      "Accuracy for Test Dataset =  0.7563\n",
      "\n",
      "Epoch 1/30\n",
      "1251/1251 [==============================] - 7s 4ms/step - loss: 53.9896 - accuracy: 0.3876\n",
      "Epoch 2/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 41.4406 - accuracy: 0.5448\n",
      "Epoch 3/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 36.2408 - accuracy: 0.6065\n",
      "Epoch 4/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 33.0877 - accuracy: 0.6405\n",
      "Epoch 5/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 30.4132 - accuracy: 0.6703\n",
      "Epoch 6/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 28.5695 - accuracy: 0.6927\n",
      "Epoch 7/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 26.9990 - accuracy: 0.7063\n",
      "Epoch 8/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 25.6096 - accuracy: 0.7217\n",
      "Epoch 9/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 24.2326 - accuracy: 0.7373\n",
      "Epoch 10/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 23.4554 - accuracy: 0.7473\n",
      "Epoch 11/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 22.4332 - accuracy: 0.7560\n",
      "Epoch 12/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 21.6398 - accuracy: 0.7653\n",
      "Epoch 13/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 20.9639 - accuracy: 0.7745\n",
      "Epoch 14/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 20.2677 - accuracy: 0.7806\n",
      "Epoch 15/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 19.4425 - accuracy: 0.7875\n",
      "Epoch 16/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 19.0197 - accuracy: 0.7948\n",
      "Epoch 17/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 18.3219 - accuracy: 0.8013\n",
      "Epoch 18/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 17.7819 - accuracy: 0.8071\n",
      "Epoch 19/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 17.3587 - accuracy: 0.8133\n",
      "Epoch 20/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 17.0807 - accuracy: 0.8134\n",
      "Epoch 21/30\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 16.4904 - accuracy: 0.8190\n",
      "Epoch 22/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 16.3898 - accuracy: 0.8208\n",
      "Epoch 23/30\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 15.9631 - accuracy: 0.8253\n",
      "Epoch 24/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 15.3780 - accuracy: 0.8309\n",
      "Epoch 25/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 15.0802 - accuracy: 0.8367\n",
      "Epoch 26/30\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 14.9441 - accuracy: 0.8370\n",
      "Epoch 27/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 14.7812 - accuracy: 0.8370\n",
      "Epoch 28/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 14.2730 - accuracy: 0.8440\n",
      "Epoch 29/30\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 14.0458 - accuracy: 0.8481\n",
      "Epoch 30/30\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 13.7169 - accuracy: 0.8525\n",
      "For  30  Epochs:\n",
      "Log-loss for Train Dataset =  0.18412422715169466\n",
      "Log-loss for Test Dataset =  0.7550472002215609\n",
      "Accuracy for Train Dataset =  0.9400339949007649\n",
      "Accuracy for Test Dataset =  0.7713\n",
      "\n",
      "Epoch 1/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 53.8479 - accuracy: 0.3937\n",
      "Epoch 2/40\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 41.3547 - accuracy: 0.5472\n",
      "Epoch 3/40\n",
      "1251/1251 [==============================] - 7s 5ms/step - loss: 36.1862 - accuracy: 0.6051\n",
      "Epoch 4/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 32.9837 - accuracy: 0.6429\n",
      "Epoch 5/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 30.5462 - accuracy: 0.6676\n",
      "Epoch 6/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 28.5022 - accuracy: 0.6901\n",
      "Epoch 7/40\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 26.8688 - accuracy: 0.7063\n",
      "Epoch 8/40\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 25.4405 - accuracy: 0.7260\n",
      "Epoch 9/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 24.0670 - accuracy: 0.7364\n",
      "Epoch 10/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 23.1630 - accuracy: 0.7498\n",
      "Epoch 11/40\n",
      "1251/1251 [==============================] - 7s 5ms/step - loss: 22.1324 - accuracy: 0.7594\n",
      "Epoch 12/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 21.3329 - accuracy: 0.7685\n",
      "Epoch 13/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 20.7156 - accuracy: 0.7743\n",
      "Epoch 14/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 19.7623 - accuracy: 0.7828\n",
      "Epoch 15/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 19.1340 - accuracy: 0.7922\n",
      "Epoch 16/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 18.5641 - accuracy: 0.7989\n",
      "Epoch 17/40\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 18.1705 - accuracy: 0.8014\n",
      "Epoch 18/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 17.6445 - accuracy: 0.8080\n",
      "Epoch 19/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 16.8708 - accuracy: 0.8141\n",
      "Epoch 20/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 16.8014 - accuracy: 0.8171\n",
      "Epoch 21/40\n",
      "1251/1251 [==============================] - 7s 5ms/step - loss: 16.2812 - accuracy: 0.8223\n",
      "Epoch 22/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 15.7957 - accuracy: 0.8290\n",
      "Epoch 23/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 15.6153 - accuracy: 0.8313\n",
      "Epoch 24/40\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 15.0336 - accuracy: 0.8355\n",
      "Epoch 25/40\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 14.9741 - accuracy: 0.8358\n",
      "Epoch 26/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 14.4954 - accuracy: 0.8441\n",
      "Epoch 27/40\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 14.2668 - accuracy: 0.8460\n",
      "Epoch 28/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 13.8515 - accuracy: 0.8495\n",
      "Epoch 29/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 13.6444 - accuracy: 0.8519\n",
      "Epoch 30/40\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 13.4409 - accuracy: 0.8538\n",
      "Epoch 31/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 13.1663 - accuracy: 0.8576\n",
      "Epoch 32/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 12.9142 - accuracy: 0.8602\n",
      "Epoch 33/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 12.7104 - accuracy: 0.8612\n",
      "Epoch 34/40\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 12.4671 - accuracy: 0.8647\n",
      "Epoch 35/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 12.4930 - accuracy: 0.8656\n",
      "Epoch 36/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 12.3396 - accuracy: 0.8676\n",
      "Epoch 37/40\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 12.1442 - accuracy: 0.8680\n",
      "Epoch 38/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 11.8358 - accuracy: 0.8720\n",
      "Epoch 39/40\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 11.5393 - accuracy: 0.8752\n",
      "Epoch 40/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 11.6456 - accuracy: 0.8756\n",
      "For  40  Epochs:\n",
      "Log-loss for Train Dataset =  0.10971822520428647\n",
      "Log-loss for Test Dataset =  0.7660678950378238\n",
      "Accuracy for Train Dataset =  0.9664300354946758\n",
      "Accuracy for Test Dataset =  0.7779\n",
      "\n",
      "Epoch 1/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 54.4889 - accuracy: 0.3844\n",
      "Epoch 2/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 41.5602 - accuracy: 0.5446\n",
      "Epoch 3/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 36.2576 - accuracy: 0.6065\n",
      "Epoch 4/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 32.8513 - accuracy: 0.6468\n",
      "Epoch 5/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 30.1587 - accuracy: 0.6758\n",
      "Epoch 6/50\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 28.2515 - accuracy: 0.6927\n",
      "Epoch 7/50\n",
      "1251/1251 [==============================] - 8s 6ms/step - loss: 26.9032 - accuracy: 0.7091\n",
      "Epoch 8/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 25.2782 - accuracy: 0.7270\n",
      "Epoch 9/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 24.2596 - accuracy: 0.7375\n",
      "Epoch 10/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 23.2015 - accuracy: 0.7485\n",
      "Epoch 11/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 22.2747 - accuracy: 0.7589\n",
      "Epoch 12/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 21.1280 - accuracy: 0.7715\n",
      "Epoch 13/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 20.8154 - accuracy: 0.7727\n",
      "Epoch 14/50\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 19.8693 - accuracy: 0.7841\n",
      "Epoch 15/50\n",
      "1251/1251 [==============================] - 8s 6ms/step - loss: 19.3018 - accuracy: 0.7890\n",
      "Epoch 16/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 18.5107 - accuracy: 0.7995\n",
      "Epoch 17/50\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 18.0740 - accuracy: 0.8055\n",
      "Epoch 18/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 17.6874 - accuracy: 0.8068\n",
      "Epoch 19/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 16.9279 - accuracy: 0.8141\n",
      "Epoch 20/50\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 16.7329 - accuracy: 0.8172\n",
      "Epoch 21/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 16.0802 - accuracy: 0.8246\n",
      "Epoch 22/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 15.7430 - accuracy: 0.8300\n",
      "Epoch 23/50\n",
      "1251/1251 [==============================] - 7s 5ms/step - loss: 15.3201 - accuracy: 0.8337\n",
      "Epoch 24/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 14.9837 - accuracy: 0.8377\n",
      "Epoch 25/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 14.6342 - accuracy: 0.8419\n",
      "Epoch 26/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 14.3396 - accuracy: 0.8464\n",
      "Epoch 27/50\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 14.1320 - accuracy: 0.8471\n",
      "Epoch 28/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 13.9866 - accuracy: 0.8483\n",
      "Epoch 29/50\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 13.6317 - accuracy: 0.8522\n",
      "Epoch 30/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 13.2603 - accuracy: 0.8574\n",
      "Epoch 31/50\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 13.1111 - accuracy: 0.8570\n",
      "Epoch 32/50\n",
      "1251/1251 [==============================] - 9s 7ms/step - loss: 12.8635 - accuracy: 0.8618\n",
      "Epoch 33/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 12.8369 - accuracy: 0.8636\n",
      "Epoch 34/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 12.5396 - accuracy: 0.8660\n",
      "Epoch 35/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 12.2599 - accuracy: 0.8673\n",
      "Epoch 36/50\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 12.0593 - accuracy: 0.8697\n",
      "Epoch 37/50\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 12.2084 - accuracy: 0.8708\n",
      "Epoch 38/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 11.7568 - accuracy: 0.8745\n",
      "Epoch 39/50\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 11.6038 - accuracy: 0.8749\n",
      "Epoch 40/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 11.2656 - accuracy: 0.8792\n",
      "Epoch 41/50\n",
      "1251/1251 [==============================] - 9s 7ms/step - loss: 11.2033 - accuracy: 0.8795\n",
      "Epoch 42/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 11.1291 - accuracy: 0.8832\n",
      "Epoch 43/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 11.0802 - accuracy: 0.8815\n",
      "Epoch 44/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 10.8614 - accuracy: 0.8840\n",
      "Epoch 45/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 10.9041 - accuracy: 0.8829\n",
      "Epoch 46/50\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 10.8062 - accuracy: 0.8857\n",
      "Epoch 47/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 10.5572 - accuracy: 0.8890\n",
      "Epoch 48/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 10.4041 - accuracy: 0.8893\n",
      "Epoch 49/50\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 10.2090 - accuracy: 0.8918\n",
      "Epoch 50/50\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 10.2691 - accuracy: 0.8915\n",
      "For  50  Epochs:\n",
      "Log-loss for Train Dataset =  0.07899878684439585\n",
      "Log-loss for Test Dataset =  0.8049816520081635\n",
      "Accuracy for Train Dataset =  0.9802029695545668\n",
      "Accuracy for Test Dataset =  0.7691\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = [10, 20, 30, 40, 50]\n",
    "train_loss, test_loss, train_acc, test_acc = [], [], [], []\n",
    "\n",
    "for epochs in num_epochs:\n",
    "    # Training the Model\n",
    "    simclr = simclr_model((32, 32, 3))\n",
    "    simclr.compile(optimizer='adam', loss=criterion, metrics='accuracy')\n",
    "    simclr.fit(train_dataset, epochs = epochs)\n",
    "    \n",
    "    # Predicting on the Train/Test Datasets\n",
    "    preds_train = simclr.predict(df_train)\n",
    "    preds_test = simclr.predict(df_test)\n",
    "\n",
    "    # Finding the Predicted Classes\n",
    "    cls_train = np.argmax(preds_train, axis = 1)\n",
    "    cls_test = np.argmax(preds_test, axis = 1)\n",
    "    \n",
    "    # Finding the Train/Test set Loss\n",
    "    train_loss.append(log_loss(y_train_oh, preds_train))\n",
    "    test_loss.append(log_loss(y_test_oh, preds_test))\n",
    "    train_acc.append(accuracy_score(y_train, cls_train))\n",
    "    test_acc.append(accuracy_score(y_test, cls_test))\n",
    "    \n",
    "    print(\"For \", epochs, \" Epochs:\")\n",
    "    print(\"Log-loss for Train Dataset = \", train_loss[-1])\n",
    "    print(\"Log-loss for Test Dataset = \", test_loss[-1])\n",
    "    print(\"Accuracy for Train Dataset = \", train_acc[-1])\n",
    "    print(\"Accuracy for Test Dataset = \", test_acc[-1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "394ca468",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T17:31:44.820465Z",
     "iopub.status.busy": "2022-06-05T17:31:44.819313Z",
     "iopub.status.idle": "2022-06-05T17:37:11.112378Z",
     "shell.execute_reply": "2022-06-05T17:37:11.111779Z",
     "shell.execute_reply.started": "2022-06-05T17:08:55.502882Z"
    },
    "papermill": {
     "duration": 331.899405,
     "end_time": "2022-06-05T17:37:11.112539",
     "exception": false,
     "start_time": "2022-06-05T17:31:39.213134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 55.1439 - accuracy: 0.3800\n",
      "Epoch 2/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 42.6439 - accuracy: 0.5273\n",
      "Epoch 3/40\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 36.6384 - accuracy: 0.5972\n",
      "Epoch 4/40\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 32.9181 - accuracy: 0.6452\n",
      "Epoch 5/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 30.1822 - accuracy: 0.6731\n",
      "Epoch 6/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 28.3895 - accuracy: 0.6953\n",
      "Epoch 7/40\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 26.7854 - accuracy: 0.7105\n",
      "Epoch 8/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 25.4946 - accuracy: 0.7232\n",
      "Epoch 9/40\n",
      "1251/1251 [==============================] - 8s 6ms/step - loss: 24.0173 - accuracy: 0.7388\n",
      "Epoch 10/40\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 22.8498 - accuracy: 0.7522\n",
      "Epoch 11/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 21.9871 - accuracy: 0.7614\n",
      "Epoch 12/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 21.1493 - accuracy: 0.7729\n",
      "Epoch 13/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 20.3366 - accuracy: 0.7795\n",
      "Epoch 14/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 19.6713 - accuracy: 0.7856\n",
      "Epoch 15/40\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 18.6511 - accuracy: 0.7970\n",
      "Epoch 16/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 18.4460 - accuracy: 0.8013\n",
      "Epoch 17/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 17.7636 - accuracy: 0.8062\n",
      "Epoch 18/40\n",
      "1251/1251 [==============================] - 8s 7ms/step - loss: 17.1954 - accuracy: 0.8132\n",
      "Epoch 19/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 16.8137 - accuracy: 0.8181\n",
      "Epoch 20/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 16.2873 - accuracy: 0.8239\n",
      "Epoch 21/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 15.9387 - accuracy: 0.8287\n",
      "Epoch 22/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 15.6180 - accuracy: 0.8303\n",
      "Epoch 23/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 15.1929 - accuracy: 0.8384\n",
      "Epoch 24/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 15.0280 - accuracy: 0.8395\n",
      "Epoch 25/40\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 14.7059 - accuracy: 0.8420\n",
      "Epoch 26/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 14.2282 - accuracy: 0.8473\n",
      "Epoch 27/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 14.0436 - accuracy: 0.8487\n",
      "Epoch 28/40\n",
      "1251/1251 [==============================] - 8s 6ms/step - loss: 13.8822 - accuracy: 0.8506\n",
      "Epoch 29/40\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 13.4955 - accuracy: 0.8542\n",
      "Epoch 30/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 13.2496 - accuracy: 0.8568\n",
      "Epoch 31/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 12.9666 - accuracy: 0.8586\n",
      "Epoch 32/40\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 12.9225 - accuracy: 0.8610\n",
      "Epoch 33/40\n",
      "1251/1251 [==============================] - 5s 4ms/step - loss: 12.6439 - accuracy: 0.8641\n",
      "Epoch 34/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 12.4195 - accuracy: 0.8681\n",
      "Epoch 35/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 12.2484 - accuracy: 0.8684\n",
      "Epoch 36/40\n",
      "1251/1251 [==============================] - 8s 6ms/step - loss: 12.0313 - accuracy: 0.8706\n",
      "Epoch 37/40\n",
      "1251/1251 [==============================] - 7s 6ms/step - loss: 11.9140 - accuracy: 0.8712\n",
      "Epoch 38/40\n",
      "1251/1251 [==============================] - 6s 5ms/step - loss: 11.6782 - accuracy: 0.8758\n",
      "Epoch 39/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 11.5641 - accuracy: 0.8754\n",
      "Epoch 40/40\n",
      "1251/1251 [==============================] - 6s 4ms/step - loss: 11.5537 - accuracy: 0.8754\n"
     ]
    }
   ],
   "source": [
    "# Training the Model with the best hyper-parameter settings\n",
    "ind = np.argmax(test_acc)\n",
    "best_num_epochs = num_epochs[ind]\n",
    "simclr = simclr_model((32, 32, 3))\n",
    "simclr.compile(optimizer='adam', loss=criterion, metrics='accuracy')\n",
    "simclr.fit(train_dataset, epochs = best_num_epochs)\n",
    "\n",
    "# Saving the model along with it's weights\n",
    "simclr.save('simclr_model.h5')\n",
    "\n",
    "# Predicting on the Train/Test Datasets\n",
    "preds_train = simclr.predict(df_train)\n",
    "preds_test = simclr.predict(df_test)\n",
    "\n",
    "# Finding the Predicted Classes\n",
    "cls_train = np.argmax(preds_train, axis = 1)\n",
    "cls_test = np.argmax(preds_test, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b64a7052",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T17:37:25.949951Z",
     "iopub.status.busy": "2022-06-05T17:37:25.948949Z",
     "iopub.status.idle": "2022-06-05T17:37:26.041182Z",
     "shell.execute_reply": "2022-06-05T17:37:26.040722Z",
     "shell.execute_reply.started": "2022-06-05T17:08:55.504947Z"
    },
    "papermill": {
     "duration": 7.822321,
     "end_time": "2022-06-05T17:37:26.041314",
     "exception": false,
     "start_time": "2022-06-05T17:37:18.218993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-loss for Train Dataset =  0.09429792322013773\n",
      "Log-loss for Test Dataset =  0.7584075884043998\n",
      "Weighted F1 Score for Train Dataset =  0.9752070028081523\n",
      "Weighted F1 Score for Test Dataset =  0.7754929877446057\n",
      "Accuracy for Train Dataset =  0.9751787231915213\n",
      "Accuracy for Test Dataset =  0.7749\n"
     ]
    }
   ],
   "source": [
    "# Finding the Train/Test set Loss\n",
    "print(\"Log-loss for Train Dataset = \", log_loss(y_train_oh, preds_train))\n",
    "print(\"Log-loss for Test Dataset = \", log_loss(y_test_oh, preds_test))\n",
    "print(\"Weighted F1 Score for Train Dataset = \", f1_score(y_train, cls_train, average = 'weighted'))\n",
    "print(\"Weighted F1 Score for Test Dataset = \", f1_score(y_test, cls_test, average = 'weighted'))\n",
    "print(\"Accuracy for Train Dataset = \", accuracy_score(y_train, cls_train))\n",
    "print(\"Accuracy for Test Dataset = \", accuracy_score(y_test, cls_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2be25ec",
   "metadata": {
    "papermill": {
     "duration": 7.31579,
     "end_time": "2022-06-05T17:37:40.175804",
     "exception": false,
     "start_time": "2022-06-05T17:37:32.860014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### References\n",
    "[Reference 1](https://github.com/google-research/simclr)\n",
    "<br>\n",
    "[Reference 2](https://github.com/sayakpaul/SimCLR-in-TensorFlow-2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1718.03429,
   "end_time": "2022-06-05T17:37:50.060056",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-05T17:09:12.025766",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
