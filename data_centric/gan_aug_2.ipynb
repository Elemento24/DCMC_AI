{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aa8c582",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.022346,
     "end_time": "2022-05-10T05:54:36.506629",
     "exception": false,
     "start_time": "2022-05-10T05:54:36.484283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Generative Adversarial Networks for Data Augmentation | Part 2\n",
    "- In this second part, we will be using the augmented datasets from part 1 to perform the classification task.\n",
    "\n",
    "# 1. Importing the Packages & Boilerplate Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "644b3ddb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-10T05:54:36.560161Z",
     "iopub.status.busy": "2022-05-10T05:54:36.559210Z",
     "iopub.status.idle": "2022-05-10T05:54:42.414478Z",
     "shell.execute_reply": "2022-05-10T05:54:42.413798Z",
     "shell.execute_reply.started": "2022-05-10T05:22:59.550004Z"
    },
    "papermill": {
     "duration": 5.887135,
     "end_time": "2022-05-10T05:54:42.414627",
     "exception": false,
     "start_time": "2022-05-10T05:54:36.527492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from shutil import copyfile\n",
    "from tabulate import tabulate\n",
    "from sklearn.metrics import accuracy_score, log_loss, confusion_matrix, f1_score\n",
    "\n",
    "# https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/274717\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01c6be09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-10T05:54:42.465396Z",
     "iopub.status.busy": "2022-05-10T05:54:42.464287Z",
     "iopub.status.idle": "2022-05-10T05:54:42.466192Z",
     "shell.execute_reply": "2022-05-10T05:54:42.466624Z",
     "shell.execute_reply.started": "2022-05-10T05:23:01.914936Z"
    },
    "papermill": {
     "duration": 0.030432,
     "end_time": "2022-05-10T05:54:42.466771",
     "exception": false,
     "start_time": "2022-05-10T05:54:42.436339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting the seeds\n",
    "SEED = 0\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8899d6b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-10T05:54:44.678645Z",
     "iopub.status.busy": "2022-05-10T05:54:44.677124Z",
     "iopub.status.idle": "2022-05-10T05:54:44.682822Z",
     "shell.execute_reply": "2022-05-10T05:54:44.683242Z",
     "shell.execute_reply.started": "2022-05-10T05:23:01.920974Z"
    },
    "papermill": {
     "duration": 2.194768,
     "end_time": "2022-05-10T05:54:44.683397",
     "exception": false,
     "start_time": "2022-05-10T05:54:42.488629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Making sure that Tensorflow is able to detect the GPU\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if \"GPU\" not in device_name:\n",
    "    print(\"GPU device not found\")\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b311927",
   "metadata": {
    "papermill": {
     "duration": 0.021243,
     "end_time": "2022-05-10T05:54:44.726991",
     "exception": false,
     "start_time": "2022-05-10T05:54:44.705748",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Importing the Train/Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd097291",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-10T05:54:44.779732Z",
     "iopub.status.busy": "2022-05-10T05:54:44.779049Z",
     "iopub.status.idle": "2022-05-10T05:55:13.495643Z",
     "shell.execute_reply": "2022-05-10T05:55:13.496713Z",
     "shell.execute_reply.started": "2022-05-10T05:23:02.587224Z"
    },
    "papermill": {
     "duration": 28.748531,
     "end_time": "2022-05-10T05:55:13.496963",
     "exception": false,
     "start_time": "2022-05-10T05:54:44.748432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Test Dataset:\n",
      "(10000, 3072) (10000, 1)\n",
      "(10000, 3, 32, 32)\n",
      "(10000, 32, 32, 3) (10000, 10)\n",
      "For Train Dataset:\n",
      "(40006, 3072) (40006, 1)\n",
      "(40006, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Importing the Test Dataset\n",
    "print(\"For Test Dataset:\")\n",
    "df_test = pd.read_csv(\"../input/cifar10/test_x.csv\")\n",
    "y_test = pd.read_csv(\"../input/cifar10/test_y.csv\")\n",
    "df_test = np.array(df_test)\n",
    "y_test = np.array(y_test)\n",
    "print(df_test.shape, y_test.shape)\n",
    "\n",
    "# Reshaping the dataset\n",
    "df_test = np.reshape(df_test, (-1, 3, 32, 32))\n",
    "print(df_test.shape)\n",
    "\n",
    "# Reshaping, rescaling and one-hot encoding\n",
    "df_test = np.transpose(np.array(df_test), (0, 2, 3, 1))\n",
    "df_test = df_test / 255\n",
    "y_test_oh = tf.one_hot(np.ravel(y_test), depth = 10)\n",
    "print(df_test.shape, y_test_oh.shape)\n",
    "\n",
    "# =========================================================\n",
    "print(\"For Train Dataset:\")\n",
    "# Importing the Labelled Training Dataset\n",
    "df_train = pd.read_csv(\"../input/cifar10/train_lab_x.csv\")\n",
    "y_train = pd.read_csv(\"../input/cifar10/train_lab_y.csv\")\n",
    "df_train = np.array(df_train)\n",
    "y_train = np.array(y_train)\n",
    "print(df_train.shape, y_train.shape)\n",
    "\n",
    "# Reshaping, rescaling and one-hot encoding\n",
    "df_train = np.reshape(df_train, (-1, 3, 32, 32))\n",
    "df_train = np.transpose(np.array(df_train), (0, 2, 3, 1))\n",
    "df_train = df_train / 255\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02455926",
   "metadata": {
    "papermill": {
     "duration": 0.04555,
     "end_time": "2022-05-10T05:55:13.590971",
     "exception": false,
     "start_time": "2022-05-10T05:55:13.545421",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Image Augmentation on 25% of the Training Dataset\n",
    "## 3.1. Augmenting the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73fd6f66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-10T05:55:13.697337Z",
     "iopub.status.busy": "2022-05-10T05:55:13.696253Z",
     "iopub.status.idle": "2022-05-10T05:55:26.664467Z",
     "shell.execute_reply": "2022-05-10T05:55:26.663547Z",
     "shell.execute_reply.started": "2022-05-10T05:23:26.057660Z"
    },
    "papermill": {
     "duration": 13.025193,
     "end_time": "2022-05-10T05:55:26.664601",
     "exception": false,
     "start_time": "2022-05-10T05:55:13.639408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50022, 32, 32, 3) (50022, 1)\n",
      "(50022, 32, 32, 3) (50022, 1) (50022, 10)\n"
     ]
    }
   ],
   "source": [
    "# Importing the Augmented Dataset\n",
    "df_aug = pd.read_csv(\"../input/cifar10/df_25per_aug.csv\")\n",
    "y_aug = pd.read_csv(\"../input/cifar10/y_25per_aug.csv\")\n",
    "df_aug = np.array(df_aug)\n",
    "y_aug = np.array(y_aug)\n",
    "\n",
    "# Reshaping, rescaling and one-hot encoding\n",
    "df_aug = np.reshape(df_aug, (-1, 3, 32, 32))\n",
    "df_aug = np.transpose(np.array(df_aug), (0, 2, 3, 1))\n",
    "\n",
    "# Concatenating the Training with Augmenting Dataset\n",
    "df_aug = np.concatenate([df_train, df_aug], axis=0)\n",
    "y_aug = np.concatenate([y_train, y_aug], axis=0)\n",
    "print(df_aug.shape, y_aug.shape)\n",
    "\n",
    "# Creating a random permutation & shuffling the dataset\n",
    "perm = np.random.permutation(df_aug.shape[0])\n",
    "df_aug = np.array(df_aug[perm, : , : , : ])\n",
    "y_aug = y_aug[perm]\n",
    "y_aug_oh = tf.one_hot(np.ravel(y_aug), depth = 10)\n",
    "print(df_aug.shape, y_aug.shape, y_aug_oh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10668312",
   "metadata": {
    "papermill": {
     "duration": 0.02297,
     "end_time": "2022-05-10T05:55:26.710861",
     "exception": false,
     "start_time": "2022-05-10T05:55:26.687891",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.2. Training the Baseline Model on the Augmented Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6ec58a3",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-05-10T05:55:26.764174Z",
     "iopub.status.busy": "2022-05-10T05:55:26.763435Z",
     "iopub.status.idle": "2022-05-10T05:55:29.435288Z",
     "shell.execute_reply": "2022-05-10T05:55:29.434751Z",
     "shell.execute_reply.started": "2022-05-10T05:23:36.487842Z"
    },
    "papermill": {
     "duration": 2.699602,
     "end_time": "2022-05-10T05:55:29.435430",
     "exception": false,
     "start_time": "2022-05-10T05:55:26.735828",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing the Baseline Model Architecture\n",
    "copyfile(src = \"../input/dcai-rw/baseline_arch.py\", dst = \"../working/baseline_arch.py\")\n",
    "from baseline_arch import cnn_model\n",
    "\n",
    "# Creating Batches from the Augmented Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((df_aug, y_aug_oh)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f5258c6",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-05-10T05:55:29.495048Z",
     "iopub.status.busy": "2022-05-10T05:55:29.491706Z",
     "iopub.status.idle": "2022-05-10T05:55:29.745424Z",
     "shell.execute_reply": "2022-05-10T05:55:29.744857Z",
     "shell.execute_reply.started": "2022-05-10T05:23:39.069057Z"
    },
    "papermill": {
     "duration": 0.286075,
     "end_time": "2022-05-10T05:55:29.745564",
     "exception": false,
     "start_time": "2022-05-10T05:55:29.459489",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If the model has been pre-trained\n",
    "try:\n",
    "    conv_model = cnn_model((32, 32, 3))\n",
    "    conv_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "    conv_model.load_weights(\"../input/dcai-rw/gan_augmented_all.h5\")\n",
    "\n",
    "# If the model hasn't been pre-trained\n",
    "except:\n",
    "    num_epochs = [10, 20, 30, 40, 50]\n",
    "    train_loss, test_loss, train_acc, test_acc = [], [], [], []\n",
    "\n",
    "    for epochs in num_epochs:\n",
    "        # Training the Model\n",
    "        conv_model = cnn_model((32, 32, 3))\n",
    "        conv_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "        conv_model.fit(train_dataset, epochs = epochs)\n",
    "\n",
    "        # Predicting on the Train/Test Datasets\n",
    "        preds_train = conv_model.predict(df_aug)\n",
    "        preds_test = conv_model.predict(df_test)\n",
    "\n",
    "        # Finding the Predicted Classes\n",
    "        cls_train = np.argmax(preds_train, axis = 1)\n",
    "        cls_test = np.argmax(preds_test, axis = 1)\n",
    "\n",
    "        # Finding the Train/Test set Loss\n",
    "        train_loss.append(log_loss(y_aug_oh, preds_train))\n",
    "        test_loss.append(log_loss(y_test_oh, preds_test))\n",
    "        train_acc.append(accuracy_score(y_aug, cls_train))\n",
    "        test_acc.append(accuracy_score(y_test, cls_test))\n",
    "\n",
    "        print(\"For \", epochs, \" Epochs:\")\n",
    "        print(\"Log-loss for Train Dataset = \", train_loss[-1])\n",
    "        print(\"Log-loss for Test Dataset = \", test_loss[-1])\n",
    "        print(\"Accuracy for Train Dataset = \", train_acc[-1])\n",
    "        print(\"Accuracy for Test Dataset = \", test_acc[-1])\n",
    "        print()\n",
    "    \n",
    "    # Training the Model with the best hyper-parameter settings\n",
    "    ind = np.argmax(test_acc)\n",
    "    best_num_epochs = num_epochs[ind]\n",
    "    conv_model = cnn_model((32, 32, 3))\n",
    "    conv_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "    conv_model.fit(train_dataset, epochs = best_num_epochs)\n",
    "\n",
    "    # Saving the model along with it's weights\n",
    "    conv_model.save('gan_augmented_all.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e760757",
   "metadata": {
    "papermill": {
     "duration": 0.023282,
     "end_time": "2022-05-10T05:55:29.793328",
     "exception": false,
     "start_time": "2022-05-10T05:55:29.770046",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.3. Predicting the Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeeaf31b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-10T05:55:29.850174Z",
     "iopub.status.busy": "2022-05-10T05:55:29.849205Z",
     "iopub.status.idle": "2022-05-10T05:55:42.584716Z",
     "shell.execute_reply": "2022-05-10T05:55:42.585238Z",
     "shell.execute_reply.started": "2022-05-10T05:23:39.285174Z"
    },
    "papermill": {
     "duration": 12.767947,
     "end_time": "2022-05-10T05:55:42.585416",
     "exception": false,
     "start_time": "2022-05-10T05:55:29.817469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-loss for Augmented Dataset =  0.10731828685173772\n",
      "Log-loss for Test Dataset =  0.8453338678017722\n",
      "Weighted F1 Score for Augmented Dataset =  0.9678309627897712\n",
      "Weighted F1 Score for Test Dataset =  0.7541726334311778\n",
      "Accuracy for Augmented Dataset =  0.9678741353804327\n",
      "Accuracy for Test Dataset =  0.7547\n"
     ]
    }
   ],
   "source": [
    "# Predicting on the Train/Test Datasets\n",
    "preds_train = conv_model.predict(df_aug)\n",
    "preds_test = conv_model.predict(df_test)\n",
    "\n",
    "# Finding the Predicted Classes\n",
    "cls_train = np.argmax(preds_train, axis = 1)\n",
    "cls_test = np.argmax(preds_test, axis = 1)\n",
    "\n",
    "# Finding the Train/Test set Loss\n",
    "print(\"Log-loss for Augmented Dataset = \", log_loss(y_aug_oh, preds_train))\n",
    "print(\"Log-loss for Test Dataset = \", log_loss(y_test_oh, preds_test))\n",
    "print(\"Weighted F1 Score for Augmented Dataset = \", f1_score(y_aug, cls_train, average = 'weighted'))\n",
    "print(\"Weighted F1 Score for Test Dataset = \", f1_score(y_test, cls_test, average = 'weighted'))\n",
    "print(\"Accuracy for Augmented Dataset = \", accuracy_score(y_aug, cls_train))\n",
    "print(\"Accuracy for Test Dataset = \", accuracy_score(y_test, cls_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1014a24a",
   "metadata": {
    "papermill": {
     "duration": 0.023312,
     "end_time": "2022-05-10T05:55:42.632444",
     "exception": false,
     "start_time": "2022-05-10T05:55:42.609132",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Image Augmentation for Class Balancing\n",
    "## 4.1. Augmenting the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a9dbf7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-10T05:55:42.688492Z",
     "iopub.status.busy": "2022-05-10T05:55:42.687977Z",
     "iopub.status.idle": "2022-05-10T05:55:49.616743Z",
     "shell.execute_reply": "2022-05-10T05:55:49.617201Z",
     "shell.execute_reply.started": "2022-05-10T05:23:46.899395Z"
    },
    "papermill": {
     "duration": 6.960784,
     "end_time": "2022-05-10T05:55:49.617393",
     "exception": false,
     "start_time": "2022-05-10T05:55:42.656609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43780, 32, 32, 3) (43780, 1)\n",
      "(43780, 32, 32, 3) (43780, 1) (43780, 10)\n"
     ]
    }
   ],
   "source": [
    "# Importing the Augmented Dataset\n",
    "df_aug = pd.read_csv(\"../input/cifar10/df_clsbal_aug.csv\")\n",
    "y_aug = pd.read_csv(\"../input/cifar10/y_clsbal_aug.csv\")\n",
    "df_aug = np.array(df_aug)\n",
    "y_aug = np.array(y_aug)\n",
    "\n",
    "# Reshaping, rescaling and one-hot encoding\n",
    "df_aug = np.reshape(df_aug, (-1, 3, 32, 32))\n",
    "df_aug = np.transpose(np.array(df_aug), (0, 2, 3, 1))\n",
    "\n",
    "# Concatenating the Training with Augmenting Dataset\n",
    "df_aug = np.concatenate([df_train, df_aug], axis=0)\n",
    "y_aug = np.concatenate([y_train, y_aug], axis=0)\n",
    "print(df_aug.shape, y_aug.shape)\n",
    "\n",
    "# Creating a random permutation & shuffling the dataset\n",
    "perm = np.random.permutation(df_aug.shape[0])\n",
    "df_aug = np.array(df_aug[perm, : , : , : ])\n",
    "y_aug = y_aug[perm]\n",
    "y_aug_oh = tf.one_hot(np.ravel(y_aug), depth = 10)\n",
    "print(df_aug.shape, y_aug.shape, y_aug_oh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdd61c4",
   "metadata": {
    "papermill": {
     "duration": 0.025734,
     "end_time": "2022-05-10T05:55:49.668921",
     "exception": false,
     "start_time": "2022-05-10T05:55:49.643187",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.2. Training the Baseline Model on the Augmented Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fce7cd54",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-05-10T05:55:49.726841Z",
     "iopub.status.busy": "2022-05-10T05:55:49.725826Z",
     "iopub.status.idle": "2022-05-10T05:55:52.157791Z",
     "shell.execute_reply": "2022-05-10T05:55:52.156682Z",
     "shell.execute_reply.started": "2022-05-10T05:23:53.644811Z"
    },
    "papermill": {
     "duration": 2.463051,
     "end_time": "2022-05-10T05:55:52.157959",
     "exception": false,
     "start_time": "2022-05-10T05:55:49.694908",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing the Baseline Model Architecture\n",
    "copyfile(src = \"../input/dcai-rw/baseline_arch.py\", dst = \"../working/baseline_arch.py\")\n",
    "from baseline_arch import cnn_model\n",
    "\n",
    "# Creating Batches from the Augmented Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((df_aug, y_aug_oh)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21eb88ac",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-05-10T05:55:52.222179Z",
     "iopub.status.busy": "2022-05-10T05:55:52.221316Z",
     "iopub.status.idle": "2022-05-10T05:55:52.420352Z",
     "shell.execute_reply": "2022-05-10T05:55:52.421210Z",
     "shell.execute_reply.started": "2022-05-10T05:23:56.160402Z"
    },
    "papermill": {
     "duration": 0.239319,
     "end_time": "2022-05-10T05:55:52.421397",
     "exception": false,
     "start_time": "2022-05-10T05:55:52.182078",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If the model has been pre-trained\n",
    "try:\n",
    "    conv_model = cnn_model((32, 32, 3))\n",
    "    conv_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "    conv_model.load_weights(\"../input/dcai-rw/gan_augmented_cls_imbalance.h5\")\n",
    "\n",
    "# If the model hasn't been pre-trained\n",
    "except:\n",
    "    num_epochs = [10, 20, 30, 40, 50]\n",
    "    train_loss, test_loss, train_acc, test_acc = [], [], [], []\n",
    "\n",
    "    for epochs in num_epochs:\n",
    "        # Training the Model\n",
    "        conv_model = cnn_model((32, 32, 3))\n",
    "        conv_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "        conv_model.fit(train_dataset, epochs = epochs)\n",
    "\n",
    "        # Predicting on the Train/Test Datasets\n",
    "        preds_train = conv_model.predict(df_aug)\n",
    "        preds_test = conv_model.predict(df_test)\n",
    "\n",
    "        # Finding the Predicted Classes\n",
    "        cls_train = np.argmax(preds_train, axis = 1)\n",
    "        cls_test = np.argmax(preds_test, axis = 1)\n",
    "\n",
    "        # Finding the Train/Test set Loss\n",
    "        train_loss.append(log_loss(y_aug_oh, preds_train))\n",
    "        test_loss.append(log_loss(y_test_oh, preds_test))\n",
    "        train_acc.append(accuracy_score(y_aug, cls_train))\n",
    "        test_acc.append(accuracy_score(y_test, cls_test))\n",
    "\n",
    "        print(\"For \", epochs, \" Epochs:\")\n",
    "        print(\"Log-loss for Train Dataset = \", train_loss[-1])\n",
    "        print(\"Log-loss for Test Dataset = \", test_loss[-1])\n",
    "        print(\"Accuracy for Train Dataset = \", train_acc[-1])\n",
    "        print(\"Accuracy for Test Dataset = \", test_acc[-1])\n",
    "        print()\n",
    "    \n",
    "    # Training the Model with the best hyper-parameter settings\n",
    "    ind = np.argmax(test_acc)\n",
    "    best_num_epochs = num_epochs[ind]\n",
    "    conv_model = cnn_model((32, 32, 3))\n",
    "    conv_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "    conv_model.fit(train_dataset, epochs = best_num_epochs)\n",
    "\n",
    "    # Saving the model along with it's weights\n",
    "    conv_model.save('gan_augmented_cls_imbalance.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ecdafd",
   "metadata": {
    "papermill": {
     "duration": 0.024103,
     "end_time": "2022-05-10T05:55:52.470352",
     "exception": false,
     "start_time": "2022-05-10T05:55:52.446249",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.3. Predicting the Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca6429cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-10T05:55:52.525298Z",
     "iopub.status.busy": "2022-05-10T05:55:52.524470Z",
     "iopub.status.idle": "2022-05-10T05:55:57.410972Z",
     "shell.execute_reply": "2022-05-10T05:55:57.406322Z",
     "shell.execute_reply.started": "2022-05-10T05:23:56.336970Z"
    },
    "papermill": {
     "duration": 4.916894,
     "end_time": "2022-05-10T05:55:57.411359",
     "exception": false,
     "start_time": "2022-05-10T05:55:52.494465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-loss for Augmented Dataset =  0.21893769497461882\n",
      "Log-loss for Test Dataset =  0.7985118761176211\n",
      "Weighted F1 Score for Augmented Dataset =  0.929251487184076\n",
      "Weighted F1 Score for Test Dataset =  0.7526024160507786\n",
      "Accuracy for Augmented Dataset =  0.9294883508451348\n",
      "Accuracy for Test Dataset =  0.7533\n"
     ]
    }
   ],
   "source": [
    "# Predicting on the Train/Test Datasets\n",
    "preds_train = conv_model.predict(df_aug)\n",
    "preds_test = conv_model.predict(df_test)\n",
    "\n",
    "# Finding the Predicted Classes\n",
    "cls_train = np.argmax(preds_train, axis = 1)\n",
    "cls_test = np.argmax(preds_test, axis = 1)\n",
    "\n",
    "# Finding the Train/Test set Loss\n",
    "print(\"Log-loss for Augmented Dataset = \", log_loss(y_aug_oh, preds_train))\n",
    "print(\"Log-loss for Test Dataset = \", log_loss(y_test_oh, preds_test))\n",
    "print(\"Weighted F1 Score for Augmented Dataset = \", f1_score(y_aug, cls_train, average = 'weighted'))\n",
    "print(\"Weighted F1 Score for Test Dataset = \", f1_score(y_test, cls_test, average = 'weighted'))\n",
    "print(\"Accuracy for Augmented Dataset = \", accuracy_score(y_aug, cls_train))\n",
    "print(\"Accuracy for Test Dataset = \", accuracy_score(y_test, cls_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef360851",
   "metadata": {
    "papermill": {
     "duration": 0.041496,
     "end_time": "2022-05-10T05:55:57.503599",
     "exception": false,
     "start_time": "2022-05-10T05:55:57.462103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Image Augmentation based on Class-wise Performance\n",
    "## 5.1. Augmenting the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a78832b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-10T05:55:57.598736Z",
     "iopub.status.busy": "2022-05-10T05:55:57.597929Z",
     "iopub.status.idle": "2022-05-10T05:56:11.312626Z",
     "shell.execute_reply": "2022-05-10T05:56:11.313098Z",
     "shell.execute_reply.started": "2022-05-10T05:24:00.798385Z"
    },
    "papermill": {
     "duration": 13.767396,
     "end_time": "2022-05-10T05:56:11.313260",
     "exception": false,
     "start_time": "2022-05-10T05:55:57.545864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50054, 32, 32, 3) (50054, 1)\n",
      "(50054, 32, 32, 3) (50054, 1) (50054, 10)\n"
     ]
    }
   ],
   "source": [
    "# Importing the Augmented Dataset\n",
    "df_aug = pd.read_csv(\"../input/cifar10/df_clsper_aug.csv\")\n",
    "y_aug = pd.read_csv(\"../input/cifar10/y_clsper_aug.csv\")\n",
    "df_aug = np.array(df_aug)\n",
    "y_aug = np.array(y_aug)\n",
    "\n",
    "# Reshaping, rescaling and one-hot encoding\n",
    "df_aug = np.reshape(df_aug, (-1, 3, 32, 32))\n",
    "df_aug = np.transpose(np.array(df_aug), (0, 2, 3, 1))\n",
    "\n",
    "# Concatenating the Training with Augmenting Dataset\n",
    "df_aug = np.concatenate([df_train, df_aug], axis=0)\n",
    "y_aug = np.concatenate([y_train, y_aug], axis=0)\n",
    "print(df_aug.shape, y_aug.shape)\n",
    "\n",
    "# Creating a random permutation & shuffling the dataset\n",
    "perm = np.random.permutation(df_aug.shape[0])\n",
    "df_aug = np.array(df_aug[perm, : , : , : ])\n",
    "y_aug = y_aug[perm]\n",
    "y_aug_oh = tf.one_hot(np.ravel(y_aug), depth = 10)\n",
    "print(df_aug.shape, y_aug.shape, y_aug_oh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a9b98d",
   "metadata": {
    "papermill": {
     "duration": 0.027588,
     "end_time": "2022-05-10T05:56:11.368794",
     "exception": false,
     "start_time": "2022-05-10T05:56:11.341206",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.2. Training the Baseline Model on the Augmented Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7778b79d",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-05-10T05:56:11.431405Z",
     "iopub.status.busy": "2022-05-10T05:56:11.428329Z",
     "iopub.status.idle": "2022-05-10T05:56:14.134330Z",
     "shell.execute_reply": "2022-05-10T05:56:14.134743Z",
     "shell.execute_reply.started": "2022-05-10T05:24:15.365667Z"
    },
    "papermill": {
     "duration": 2.736674,
     "end_time": "2022-05-10T05:56:14.134920",
     "exception": false,
     "start_time": "2022-05-10T05:56:11.398246",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing the Baseline Model Architecture\n",
    "copyfile(src = \"../input/dcai-rw/baseline_arch.py\", dst = \"../working/baseline_arch.py\")\n",
    "from baseline_arch import cnn_model\n",
    "\n",
    "# Creating Batches from the Augmented Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((df_aug, y_aug_oh)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6532bc0c",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-05-10T05:56:14.200317Z",
     "iopub.status.busy": "2022-05-10T05:56:14.188342Z",
     "iopub.status.idle": "2022-05-10T05:56:14.364986Z",
     "shell.execute_reply": "2022-05-10T05:56:14.364426Z",
     "shell.execute_reply.started": "2022-05-10T05:24:18.154528Z"
    },
    "papermill": {
     "duration": 0.203942,
     "end_time": "2022-05-10T05:56:14.365177",
     "exception": false,
     "start_time": "2022-05-10T05:56:14.161235",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If the model has been pre-trained\n",
    "try:\n",
    "    conv_model = cnn_model((32, 32, 3))\n",
    "    conv_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "    conv_model.load_weights(\"../input/dcai-rw/gan_augmented_cls_performance_wise.h5\")\n",
    "\n",
    "# If the model hasn't been pre-trained\n",
    "except:\n",
    "    num_epochs = [10, 20, 30, 40, 50]\n",
    "    train_loss, test_loss, train_acc, test_acc = [], [], [], []\n",
    "\n",
    "    for epochs in num_epochs:\n",
    "        # Training the Model\n",
    "        conv_model = cnn_model((32, 32, 3))\n",
    "        conv_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "        conv_model.fit(train_dataset, epochs = epochs)\n",
    "\n",
    "        # Predicting on the Train/Test Datasets\n",
    "        preds_train = conv_model.predict(df_aug)\n",
    "        preds_test = conv_model.predict(df_test)\n",
    "\n",
    "        # Finding the Predicted Classes\n",
    "        cls_train = np.argmax(preds_train, axis = 1)\n",
    "        cls_test = np.argmax(preds_test, axis = 1)\n",
    "\n",
    "        # Finding the Train/Test set Loss\n",
    "        train_loss.append(log_loss(y_aug_oh, preds_train))\n",
    "        test_loss.append(log_loss(y_test_oh, preds_test))\n",
    "        train_acc.append(accuracy_score(y_aug, cls_train))\n",
    "        test_acc.append(accuracy_score(y_test, cls_test))\n",
    "\n",
    "        print(\"For \", epochs, \" Epochs:\")\n",
    "        print(\"Log-loss for Train Dataset = \", train_loss[-1])\n",
    "        print(\"Log-loss for Test Dataset = \", test_loss[-1])\n",
    "        print(\"Accuracy for Train Dataset = \", train_acc[-1])\n",
    "        print(\"Accuracy for Test Dataset = \", test_acc[-1])\n",
    "        print()\n",
    "    \n",
    "    # Training the Model with the best hyper-parameter settings\n",
    "    ind = np.argmax(test_acc)\n",
    "    best_num_epochs = num_epochs[ind]\n",
    "    conv_model = cnn_model((32, 32, 3))\n",
    "    conv_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "    conv_model.fit(train_dataset, epochs = best_num_epochs)\n",
    "\n",
    "    # Saving the model along with it's weights\n",
    "    conv_model.save('gan_augmented_cls_performance_wise.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d1a4a",
   "metadata": {
    "papermill": {
     "duration": 0.026773,
     "end_time": "2022-05-10T05:56:14.417970",
     "exception": false,
     "start_time": "2022-05-10T05:56:14.391197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.3. Predicting the Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "671ebd81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-10T05:56:14.475505Z",
     "iopub.status.busy": "2022-05-10T05:56:14.474679Z",
     "iopub.status.idle": "2022-05-10T05:56:19.624052Z",
     "shell.execute_reply": "2022-05-10T05:56:19.625328Z",
     "shell.execute_reply.started": "2022-05-10T05:24:18.344786Z"
    },
    "papermill": {
     "duration": 5.182404,
     "end_time": "2022-05-10T05:56:19.625509",
     "exception": false,
     "start_time": "2022-05-10T05:56:14.443105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-loss for Augmented Dataset =  0.08465463174989617\n",
      "Log-loss for Test Dataset =  0.8320012305460958\n",
      "Weighted F1 Score for Augmented Dataset =  0.9757746202916417\n",
      "Weighted F1 Score for Test Dataset =  0.764638518226788\n",
      "Accuracy for Augmented Dataset =  0.9758061293802693\n",
      "Accuracy for Test Dataset =  0.7664\n"
     ]
    }
   ],
   "source": [
    "# Predicting on the Train/Test Datasets\n",
    "preds_train = conv_model.predict(df_aug)\n",
    "preds_test = conv_model.predict(df_test)\n",
    "\n",
    "# Finding the Predicted Classes\n",
    "cls_train = np.argmax(preds_train, axis = 1)\n",
    "cls_test = np.argmax(preds_test, axis = 1)\n",
    "\n",
    "# Finding the Train/Test set Loss\n",
    "print(\"Log-loss for Augmented Dataset = \", log_loss(y_aug_oh, preds_train))\n",
    "print(\"Log-loss for Test Dataset = \", log_loss(y_test_oh, preds_test))\n",
    "print(\"Weighted F1 Score for Augmented Dataset = \", f1_score(y_aug, cls_train, average = 'weighted'))\n",
    "print(\"Weighted F1 Score for Test Dataset = \", f1_score(y_test, cls_test, average = 'weighted'))\n",
    "print(\"Accuracy for Augmented Dataset = \", accuracy_score(y_aug, cls_train))\n",
    "print(\"Accuracy for Test Dataset = \", accuracy_score(y_test, cls_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 114.39827,
   "end_time": "2022-05-10T05:56:22.554677",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-10T05:54:28.156407",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
