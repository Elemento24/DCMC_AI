{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Generative Adversarial Networks for Data Augmentation | Part 2\n- In this second part, we will be using the augmented datasets from part 1 to perform the classification task.\n\n# 1. Importing the Packages & Boilerplate Code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import os\nimport sys\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom shutil import copyfile\nfrom tabulate import tabulate\nfrom sklearn.metrics import accuracy_score, log_loss, confusion_matrix\n\n# https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/274717\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n\nimport tensorflow as tf\nimport tensorflow.keras.layers as tfl","metadata":{"execution":{"iopub.status.busy":"2022-04-04T05:48:18.133265Z","iopub.execute_input":"2022-04-04T05:48:18.133664Z","iopub.status.idle":"2022-04-04T05:48:24.030631Z","shell.execute_reply.started":"2022-04-04T05:48:18.133560Z","shell.execute_reply":"2022-04-04T05:48:24.029904Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Setting the seeds\nSEED = 0\nos.environ['PYTHONHASHSEED']=str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T05:48:24.032169Z","iopub.execute_input":"2022-04-04T05:48:24.032403Z","iopub.status.idle":"2022-04-04T05:48:24.039667Z","shell.execute_reply.started":"2022-04-04T05:48:24.032369Z","shell.execute_reply":"2022-04-04T05:48:24.038601Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Making sure that Tensorflow is able to detect the GPU\ndevice_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","metadata":{"execution":{"iopub.status.busy":"2022-04-04T05:48:24.040753Z","iopub.execute_input":"2022-04-04T05:48:24.040993Z","iopub.status.idle":"2022-04-04T05:48:26.128231Z","shell.execute_reply.started":"2022-04-04T05:48:24.040959Z","shell.execute_reply":"2022-04-04T05:48:26.127262Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Found GPU at: /device:GPU:0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 2. Importing the Train/Test Sets","metadata":{}},{"cell_type":"code","source":"# Importing the Test Dataset\nprint(\"For Test Dataset:\")\ndf_test = pd.read_csv(\"../input/cifar10/test_x.csv\")\ny_test = pd.read_csv(\"../input/cifar10/test_y.csv\")\ndf_test = np.array(df_test)\ny_test = np.array(y_test)\nprint(df_test.shape, y_test.shape)\n\n# Reshaping the dataset\ndf_test = np.reshape(df_test, (-1, 3, 32, 32))\nprint(df_test.shape)\n\n# Reshaping, rescaling and one-hot encoding\ndf_test = np.transpose(np.array(df_test), (0, 2, 3, 1))\ndf_test = df_test / 255\ny_test_oh = tf.one_hot(np.ravel(y_test), depth = 10)\nprint(df_test.shape, y_test_oh.shape)\n\n# =========================================================\nprint(\"For Train Dataset:\")\n# Importing the Labelled Training Dataset\ndf_train = pd.read_csv(\"../input/cifar10/train_lab_x.csv\")\ny_train = pd.read_csv(\"../input/cifar10/train_lab_y.csv\")\ndf_train = np.array(df_train)\ny_train = np.array(y_train)\nprint(df_train.shape, y_train.shape)\n\n# Reshaping, rescaling and one-hot encoding\ndf_train = np.reshape(df_train, (-1, 3, 32, 32))\ndf_train = np.transpose(np.array(df_train), (0, 2, 3, 1))\ndf_train = df_train / 255\nprint(df_train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T05:48:26.131594Z","iopub.execute_input":"2022-04-04T05:48:26.131796Z","iopub.status.idle":"2022-04-04T05:48:53.898641Z","shell.execute_reply.started":"2022-04-04T05:48:26.131769Z","shell.execute_reply":"2022-04-04T05:48:53.897145Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"For Test Dataset:\n(10000, 3072) (10000, 1)\n(10000, 3, 32, 32)\n(10000, 32, 32, 3) (10000, 10)\nFor Train Dataset:\n(40006, 3072) (40006, 1)\n(40006, 32, 32, 3)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 3. Image Augmentation on 25% of the Training Dataset\n## 3.1. Augmenting the Training Dataset","metadata":{}},{"cell_type":"code","source":"# Importing the Augmented Dataset\ndf_aug = pd.read_csv(\"../input/cifar10/df_25per_aug.csv\")\ny_aug = pd.read_csv(\"../input/cifar10/y_25per_aug.csv\")\ndf_aug = np.array(df_aug)\ny_aug = np.array(y_aug)\n\n# Reshaping, rescaling and one-hot encoding\ndf_aug = np.reshape(df_aug, (-1, 3, 32, 32))\ndf_aug = np.transpose(np.array(df_aug), (0, 2, 3, 1))\n\n# Concatenating the Training with Augmenting Dataset\ndf_aug = np.concatenate([df_train, df_aug], axis=0)\ny_aug = np.concatenate([y_train, y_aug], axis=0)\nprint(df_aug.shape, y_aug.shape)\n\n# Creating a random permutation & shuffling the dataset\nperm = np.random.permutation(df_aug.shape[0])\ndf_aug = np.array(df_aug[perm, : , : , : ])\ny_aug = y_aug[perm]\ny_aug_oh = tf.one_hot(np.ravel(y_aug), depth = 10)\nprint(df_aug.shape, y_aug.shape, y_aug_oh.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T05:48:53.899939Z","iopub.execute_input":"2022-04-04T05:48:53.900258Z","iopub.status.idle":"2022-04-04T05:49:05.983299Z","shell.execute_reply.started":"2022-04-04T05:48:53.900219Z","shell.execute_reply":"2022-04-04T05:49:05.982515Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"(50022, 32, 32, 3) (50022, 1)\n(50022, 32, 32, 3) (50022, 1) (50022, 10)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 3.2. Training the Baseline Model on the Augmented Dataset","metadata":{}},{"cell_type":"code","source":"# Importing the Baseline Model Architecture\ncopyfile(src = \"../input/dcai-rw/baseline_arch.py\", dst = \"../working/baseline_arch.py\")\nfrom baseline_arch import cnn_model\n\nconv_model = cnn_model((32, 32, 3))\nconv_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n\n# Creating Batches from the Augmented Dataset\naug_dataset = tf.data.Dataset.from_tensor_slices((df_aug, y_aug_oh)).batch(32)\nhistory = conv_model.fit(aug_dataset, epochs = 25)\n\n# Saving the model along with it's weights\nconv_model.save('baseline_gan_augmented_all.h5')","metadata":{"scrolled":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-04T05:49:05.984660Z","iopub.execute_input":"2022-04-04T05:49:05.984925Z","iopub.status.idle":"2022-04-04T05:53:11.870171Z","shell.execute_reply.started":"2022-04-04T05:49:05.984887Z","shell.execute_reply":"2022-04-04T05:53:11.869387Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Epoch 1/25\n1564/1564 [==============================] - 14s 5ms/step - loss: 1.6716 - accuracy: 0.3929\nEpoch 2/25\n1564/1564 [==============================] - 7s 4ms/step - loss: 1.2083 - accuracy: 0.5752\nEpoch 3/25\n1564/1564 [==============================] - 7s 4ms/step - loss: 1.0299 - accuracy: 0.6426\nEpoch 4/25\n1564/1564 [==============================] - 8s 5ms/step - loss: 0.9303 - accuracy: 0.6797\nEpoch 5/25\n1564/1564 [==============================] - 7s 5ms/step - loss: 0.8480 - accuracy: 0.7061\nEpoch 6/25\n1564/1564 [==============================] - 7s 5ms/step - loss: 0.7999 - accuracy: 0.7229\nEpoch 7/25\n1564/1564 [==============================] - 8s 5ms/step - loss: 0.7611 - accuracy: 0.7355\nEpoch 8/25\n1564/1564 [==============================] - 7s 5ms/step - loss: 0.7188 - accuracy: 0.7518\nEpoch 9/25\n1564/1564 [==============================] - 7s 4ms/step - loss: 0.6802 - accuracy: 0.7651\nEpoch 10/25\n1564/1564 [==============================] - 7s 4ms/step - loss: 0.6548 - accuracy: 0.7735\nEpoch 11/25\n1564/1564 [==============================] - 8s 5ms/step - loss: 0.6263 - accuracy: 0.7852\nEpoch 12/25\n1564/1564 [==============================] - 7s 5ms/step - loss: 0.6011 - accuracy: 0.7911\nEpoch 13/25\n1564/1564 [==============================] - 7s 5ms/step - loss: 0.5806 - accuracy: 0.7983\nEpoch 14/25\n1564/1564 [==============================] - 8s 5ms/step - loss: 0.5623 - accuracy: 0.8062\nEpoch 15/25\n1564/1564 [==============================] - 7s 4ms/step - loss: 0.5484 - accuracy: 0.8091\nEpoch 16/25\n1564/1564 [==============================] - 7s 5ms/step - loss: 0.5305 - accuracy: 0.8163\nEpoch 17/25\n1564/1564 [==============================] - 7s 4ms/step - loss: 0.5186 - accuracy: 0.8214\nEpoch 18/25\n1564/1564 [==============================] - 8s 5ms/step - loss: 0.5018 - accuracy: 0.8271\nEpoch 19/25\n1564/1564 [==============================] - 7s 4ms/step - loss: 0.4924 - accuracy: 0.8300\nEpoch 20/25\n1564/1564 [==============================] - 7s 5ms/step - loss: 0.4858 - accuracy: 0.8308\nEpoch 21/25\n1564/1564 [==============================] - 7s 5ms/step - loss: 0.4649 - accuracy: 0.8393\nEpoch 22/25\n1564/1564 [==============================] - 7s 5ms/step - loss: 0.4591 - accuracy: 0.8434\nEpoch 23/25\n1564/1564 [==============================] - 7s 5ms/step - loss: 0.4429 - accuracy: 0.8467\nEpoch 24/25\n1564/1564 [==============================] - 7s 4ms/step - loss: 0.4372 - accuracy: 0.8487\nEpoch 25/25\n1564/1564 [==============================] - 8s 5ms/step - loss: 0.4241 - accuracy: 0.8526\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 3.3. Predicting the Performance","metadata":{}},{"cell_type":"code","source":"# Predicting on the Train/Test Datasets\npreds_aug = conv_model.predict(df_aug)\npreds_test = conv_model.predict(df_test)\n\n# Finding the Predicted Classes\ncls_aug = np.argmax(preds_aug, axis = 1)\ncls_test = np.argmax(preds_test, axis = 1)\n\n# Finding the Train/Test set Loss\nprint(\"Log-loss for Augmented Dataset = \", log_loss(y_aug_oh, preds_aug))\nprint(\"Log-loss for Test Dataset = \", log_loss(y_test_oh, preds_test))\nprint(\"Accuracy for Augmented Dataset = \", accuracy_score(y_aug, cls_aug))\nprint(\"Accuracy for Test Dataset = \", accuracy_score(y_test, cls_test))","metadata":{"execution":{"iopub.status.busy":"2022-04-04T05:53:11.872537Z","iopub.execute_input":"2022-04-04T05:53:11.872810Z","iopub.status.idle":"2022-04-04T05:53:16.839975Z","shell.execute_reply.started":"2022-04-04T05:53:11.872772Z","shell.execute_reply":"2022-04-04T05:53:16.839211Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Log-loss for Augmented Dataset =  0.2295699061216759\nLog-loss for Test Dataset =  0.8003369166397284\nAccuracy for Augmented Dataset =  0.9206749030426612\nAccuracy for Test Dataset =  0.7518\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 4. Image Augmentation for Class Balancing\n## 4.1. Augmenting the Training Dataset","metadata":{}},{"cell_type":"code","source":"# Importing the Augmented Dataset\ndf_aug = pd.read_csv(\"../input/cifar10/df_clsbal_aug.csv\")\ny_aug = pd.read_csv(\"../input/cifar10/y_clsbal_aug.csv\")\ndf_aug = np.array(df_aug)\ny_aug = np.array(y_aug)\n\n# Reshaping, rescaling and one-hot encoding\ndf_aug = np.reshape(df_aug, (-1, 3, 32, 32))\ndf_aug = np.transpose(np.array(df_aug), (0, 2, 3, 1))\n\n# Concatenating the Training with Augmenting Dataset\ndf_aug = np.concatenate([df_train, df_aug], axis=0)\ny_aug = np.concatenate([y_train, y_aug], axis=0)\nprint(df_aug.shape, y_aug.shape)\n\n# Creating a random permutation & shuffling the dataset\nperm = np.random.permutation(df_aug.shape[0])\ndf_aug = np.array(df_aug[perm, : , : , : ])\ny_aug = y_aug[perm]\ny_aug_oh = tf.one_hot(np.ravel(y_aug), depth = 10)\nprint(df_aug.shape, y_aug.shape, y_aug_oh.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T05:53:16.841524Z","iopub.execute_input":"2022-04-04T05:53:16.841947Z","iopub.status.idle":"2022-04-04T05:53:23.075399Z","shell.execute_reply.started":"2022-04-04T05:53:16.841908Z","shell.execute_reply":"2022-04-04T05:53:23.074511Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"(43780, 32, 32, 3) (43780, 1)\n(43780, 32, 32, 3) (43780, 1) (43780, 10)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 4.2. Training the Baseline Model on the Augmented Dataset","metadata":{}},{"cell_type":"code","source":"# Importing the Baseline Model Architecture\ncopyfile(src = \"../input/dcai-rw/baseline_arch.py\", dst = \"../working/baseline_arch.py\")\nfrom baseline_arch import cnn_model\n\nconv_model = cnn_model((32, 32, 3))\nconv_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n\n# Creating Batches from the Augmented Dataset\naug_dataset = tf.data.Dataset.from_tensor_slices((df_aug, y_aug_oh)).batch(32)\nhistory = conv_model.fit(aug_dataset, epochs = 25)\n\n# Saving the model along with it's weights\nconv_model.save('baseline_gan_augmented_all.h5')","metadata":{"scrolled":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-04T05:53:23.076795Z","iopub.execute_input":"2022-04-04T05:53:23.077049Z","iopub.status.idle":"2022-04-04T05:56:57.822777Z","shell.execute_reply.started":"2022-04-04T05:53:23.077019Z","shell.execute_reply":"2022-04-04T05:56:57.816367Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Epoch 1/25\n1369/1369 [==============================] - 7s 5ms/step - loss: 1.7263 - accuracy: 0.3790\nEpoch 2/25\n1369/1369 [==============================] - 7s 5ms/step - loss: 1.3509 - accuracy: 0.5213\nEpoch 3/25\n1369/1369 [==============================] - 6s 4ms/step - loss: 1.1573 - accuracy: 0.5944\nEpoch 4/25\n1369/1369 [==============================] - 6s 4ms/step - loss: 1.0456 - accuracy: 0.6396\nEpoch 5/25\n1369/1369 [==============================] - 6s 5ms/step - loss: 0.9597 - accuracy: 0.6717\nEpoch 6/25\n1369/1369 [==============================] - 7s 5ms/step - loss: 0.8931 - accuracy: 0.6923\nEpoch 7/25\n1369/1369 [==============================] - 6s 4ms/step - loss: 0.8382 - accuracy: 0.7104\nEpoch 8/25\n1369/1369 [==============================] - 6s 4ms/step - loss: 0.7942 - accuracy: 0.7266\nEpoch 9/25\n1369/1369 [==============================] - 6s 5ms/step - loss: 0.7624 - accuracy: 0.7372\nEpoch 10/25\n1369/1369 [==============================] - 7s 5ms/step - loss: 0.7289 - accuracy: 0.7490\nEpoch 11/25\n1369/1369 [==============================] - 6s 5ms/step - loss: 0.6928 - accuracy: 0.7619\nEpoch 12/25\n1369/1369 [==============================] - 6s 5ms/step - loss: 0.6640 - accuracy: 0.7712\nEpoch 13/25\n1369/1369 [==============================] - 6s 5ms/step - loss: 0.6454 - accuracy: 0.7769\nEpoch 14/25\n1369/1369 [==============================] - 7s 5ms/step - loss: 0.6204 - accuracy: 0.7857\nEpoch 15/25\n1369/1369 [==============================] - 6s 5ms/step - loss: 0.5999 - accuracy: 0.7932\nEpoch 16/25\n1369/1369 [==============================] - 6s 5ms/step - loss: 0.5806 - accuracy: 0.7974\nEpoch 17/25\n1369/1369 [==============================] - 6s 5ms/step - loss: 0.5663 - accuracy: 0.8043\nEpoch 18/25\n1369/1369 [==============================] - 6s 5ms/step - loss: 0.5480 - accuracy: 0.8110\nEpoch 19/25\n1369/1369 [==============================] - 6s 4ms/step - loss: 0.5371 - accuracy: 0.8133\nEpoch 20/25\n1369/1369 [==============================] - 6s 5ms/step - loss: 0.5217 - accuracy: 0.8191\nEpoch 21/25\n1369/1369 [==============================] - 6s 5ms/step - loss: 0.5107 - accuracy: 0.8238\nEpoch 22/25\n1369/1369 [==============================] - 7s 5ms/step - loss: 0.4974 - accuracy: 0.8285\nEpoch 23/25\n1369/1369 [==============================] - 6s 5ms/step - loss: 0.4878 - accuracy: 0.8313\nEpoch 24/25\n1369/1369 [==============================] - 6s 4ms/step - loss: 0.4784 - accuracy: 0.8356\nEpoch 25/25\n1369/1369 [==============================] - 6s 4ms/step - loss: 0.4615 - accuracy: 0.8420\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 4.3. Predicting the Performance","metadata":{}},{"cell_type":"code","source":"# Predicting on the Train/Test Datasets\npreds_aug = conv_model.predict(df_aug)\npreds_test = conv_model.predict(df_test)\n\n# Finding the Predicted Classes\ncls_aug = np.argmax(preds_aug, axis = 1)\ncls_test = np.argmax(preds_test, axis = 1)\n\n# Finding the Train/Test set Loss\nprint(\"Log-loss for Augmented Dataset = \", log_loss(y_aug_oh, preds_aug))\nprint(\"Log-loss for Test Dataset = \", log_loss(y_test_oh, preds_test))\nprint(\"Accuracy for Augmented Dataset = \", accuracy_score(y_aug, cls_aug))\nprint(\"Accuracy for Test Dataset = \", accuracy_score(y_test, cls_test))","metadata":{"execution":{"iopub.status.busy":"2022-04-04T05:56:57.825349Z","iopub.execute_input":"2022-04-04T05:56:57.828009Z","iopub.status.idle":"2022-04-04T05:57:03.027622Z","shell.execute_reply.started":"2022-04-04T05:56:57.827967Z","shell.execute_reply":"2022-04-04T05:57:03.026920Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Log-loss for Augmented Dataset =  0.23891070328193428\nLog-loss for Test Dataset =  0.8148165362078066\nAccuracy for Augmented Dataset =  0.9202147099132024\nAccuracy for Test Dataset =  0.7475\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 5. Image Augmentation based on Class-wise Performance\n## 5.1. Augmenting the Training Dataset","metadata":{}},{"cell_type":"code","source":"# Importing the Augmented Dataset\ndf_aug = pd.read_csv(\"../input/cifar10/df_clsper_aug.csv\")\ny_aug = pd.read_csv(\"../input/cifar10/y_clsper_aug.csv\")\ndf_aug = np.array(df_aug)\ny_aug = np.array(y_aug)\n\n# Reshaping, rescaling and one-hot encoding\ndf_aug = np.reshape(df_aug, (-1, 3, 32, 32))\ndf_aug = np.transpose(np.array(df_aug), (0, 2, 3, 1))\n\n# Concatenating the Training with Augmenting Dataset\ndf_aug = np.concatenate([df_train, df_aug], axis=0)\ny_aug = np.concatenate([y_train, y_aug], axis=0)\nprint(df_aug.shape, y_aug.shape)\n\n# Creating a random permutation & shuffling the dataset\nperm = np.random.permutation(df_aug.shape[0])\ndf_aug = np.array(df_aug[perm, : , : , : ])\ny_aug = y_aug[perm]\ny_aug_oh = tf.one_hot(np.ravel(y_aug), depth = 10)\nprint(df_aug.shape, y_aug.shape, y_aug_oh.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T05:57:03.028745Z","iopub.execute_input":"2022-04-04T05:57:03.029519Z","iopub.status.idle":"2022-04-04T05:57:13.843105Z","shell.execute_reply.started":"2022-04-04T05:57:03.029479Z","shell.execute_reply":"2022-04-04T05:57:13.841555Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"(48262, 32, 32, 3) (48262, 1)\n(48262, 32, 32, 3) (48262, 1) (48262, 10)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 5.2. Training the Baseline Model on the Augmented Dataset","metadata":{}},{"cell_type":"code","source":"# Importing the Baseline Model Architecture\ncopyfile(src = \"../input/dcai-rw/baseline_arch.py\", dst = \"../working/baseline_arch.py\")\nfrom baseline_arch import cnn_model\n\nconv_model = cnn_model((32, 32, 3))\nconv_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n\n# Creating Batches from the Augmented Dataset\naug_dataset = tf.data.Dataset.from_tensor_slices((df_aug, y_aug_oh)).batch(32)\nhistory = conv_model.fit(aug_dataset, epochs = 25)\n\n# Saving the model along with it's weights\nconv_model.save('baseline_gan_augmented_all.h5')","metadata":{"scrolled":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-04T05:57:13.844544Z","iopub.execute_input":"2022-04-04T05:57:13.844787Z","iopub.status.idle":"2022-04-04T06:00:58.966730Z","shell.execute_reply.started":"2022-04-04T05:57:13.844753Z","shell.execute_reply":"2022-04-04T06:00:58.965985Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Epoch 1/25\n1509/1509 [==============================] - 8s 5ms/step - loss: 1.6511 - accuracy: 0.3893\nEpoch 2/25\n1509/1509 [==============================] - 7s 5ms/step - loss: 1.2357 - accuracy: 0.5585\nEpoch 3/25\n1509/1509 [==============================] - 7s 5ms/step - loss: 1.0640 - accuracy: 0.6262\nEpoch 4/25\n1509/1509 [==============================] - 7s 5ms/step - loss: 0.9560 - accuracy: 0.6674\nEpoch 5/25\n1509/1509 [==============================] - 7s 5ms/step - loss: 0.8734 - accuracy: 0.6934\nEpoch 6/25\n1509/1509 [==============================] - 7s 5ms/step - loss: 0.8173 - accuracy: 0.7161\nEpoch 7/25\n1509/1509 [==============================] - 7s 4ms/step - loss: 0.7688 - accuracy: 0.7333\nEpoch 8/25\n1509/1509 [==============================] - 7s 4ms/step - loss: 0.7320 - accuracy: 0.7478\nEpoch 9/25\n1509/1509 [==============================] - 7s 5ms/step - loss: 0.6976 - accuracy: 0.7582\nEpoch 10/25\n1509/1509 [==============================] - 7s 4ms/step - loss: 0.6684 - accuracy: 0.7702\nEpoch 11/25\n1509/1509 [==============================] - 7s 5ms/step - loss: 0.6477 - accuracy: 0.7790\nEpoch 12/25\n1509/1509 [==============================] - 7s 5ms/step - loss: 0.6115 - accuracy: 0.7868\nEpoch 13/25\n1509/1509 [==============================] - 7s 5ms/step - loss: 0.5911 - accuracy: 0.7948\nEpoch 14/25\n1509/1509 [==============================] - 7s 4ms/step - loss: 0.5821 - accuracy: 0.7993\nEpoch 15/25\n1509/1509 [==============================] - 7s 5ms/step - loss: 0.5568 - accuracy: 0.8076\nEpoch 16/25\n1509/1509 [==============================] - 7s 5ms/step - loss: 0.5458 - accuracy: 0.8127\nEpoch 17/25\n1509/1509 [==============================] - 7s 5ms/step - loss: 0.5318 - accuracy: 0.8163\nEpoch 18/25\n1509/1509 [==============================] - 7s 5ms/step - loss: 0.5111 - accuracy: 0.8243\nEpoch 19/25\n1509/1509 [==============================] - 7s 4ms/step - loss: 0.4992 - accuracy: 0.8277\nEpoch 20/25\n1509/1509 [==============================] - 7s 5ms/step - loss: 0.4841 - accuracy: 0.8334\nEpoch 21/25\n1509/1509 [==============================] - 7s 5ms/step - loss: 0.4740 - accuracy: 0.8364\nEpoch 22/25\n1509/1509 [==============================] - 7s 5ms/step - loss: 0.4659 - accuracy: 0.8377\nEpoch 23/25\n1509/1509 [==============================] - 7s 5ms/step - loss: 0.4524 - accuracy: 0.8444\nEpoch 24/25\n1509/1509 [==============================] - 7s 5ms/step - loss: 0.4452 - accuracy: 0.8448\nEpoch 25/25\n1509/1509 [==============================] - 7s 5ms/step - loss: 0.4340 - accuracy: 0.8510\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 5.3. Predicting the Performance","metadata":{}},{"cell_type":"code","source":"# Predicting on the Train/Test Datasets\npreds_aug = conv_model.predict(df_aug)\npreds_test = conv_model.predict(df_test)\n\n# Finding the Predicted Classes\ncls_aug = np.argmax(preds_aug, axis = 1)\ncls_test = np.argmax(preds_test, axis = 1)\n\n# Finding the Train/Test set Loss\nprint(\"Log-loss for Augmented Dataset = \", log_loss(y_aug_oh, preds_aug))\nprint(\"Log-loss for Test Dataset = \", log_loss(y_test_oh, preds_test))\nprint(\"Accuracy for Augmented Dataset = \", accuracy_score(y_aug, cls_aug))\nprint(\"Accuracy for Test Dataset = \", accuracy_score(y_test, cls_test))","metadata":{"execution":{"iopub.status.busy":"2022-04-04T06:00:58.968642Z","iopub.execute_input":"2022-04-04T06:00:58.969100Z","iopub.status.idle":"2022-04-04T06:01:03.917424Z","shell.execute_reply.started":"2022-04-04T06:00:58.969059Z","shell.execute_reply":"2022-04-04T06:01:03.916245Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Log-loss for Augmented Dataset =  0.24085062231891433\nLog-loss for Test Dataset =  0.7980606513905791\nAccuracy for Augmented Dataset =  0.9197505283660022\nAccuracy for Test Dataset =  0.749\n","output_type":"stream"}]}]}