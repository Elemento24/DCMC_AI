{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60ec66e6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.032366,
     "end_time": "2022-06-01T11:25:50.193340",
     "exception": false,
     "start_time": "2022-06-01T11:25:50.160974",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Iterative Sampling with Augmentation\n",
    "- In this kernel, we will be performing **Iterative Sampling with Augmentation**, an approach borrowed from a paper entitled \"Increasing Data Diversity with Iterative Sampling to Improve Performance\".\n",
    "- For this approach, we require a pool of Augmented Images (for convenience, we will refer to it as Pool). The **Pool** contains only augmented samples and not the base images.\n",
    "- The research paper has used many different augmentation techniques for creating the **Pool**. However, for starters, we will be using **random-sampling based augmentation**, using both Traditional Augmentation and GAN-based Augmentation techniques. \n",
    "\n",
    "### References\n",
    "- Cavusoglu, Devrim, Ogulcan Eryuksel, and Sinan Altinuc. \"Increasing Data Diversity with Iterative Sampling to Improve Performance.\" arXiv preprint arXiv:2111.03743 (2021).\n",
    "- [Convert .CSV file to Images](https://medium.com/lifeandtech/convert-csv-file-to-images-309b6fdb8c49)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c34517f",
   "metadata": {
    "papermill": {
     "duration": 0.029878,
     "end_time": "2022-06-01T11:25:50.253956",
     "exception": false,
     "start_time": "2022-06-01T11:25:50.224078",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Importing the Packages & Boilerplate Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b9c80ff",
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-06-01T11:25:50.317085Z",
     "iopub.status.busy": "2022-06-01T11:25:50.316275Z",
     "iopub.status.idle": "2022-06-01T11:27:13.556113Z",
     "shell.execute_reply": "2022-06-01T11:27:13.554662Z"
    },
    "papermill": {
     "duration": 83.275055,
     "end_time": "2022-06-01T11:27:13.559255",
     "exception": false,
     "start_time": "2022-06-01T11:25:50.284200",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fiftyone\r\n",
      "  Downloading fiftyone-0.16.1-py3-none-any.whl (1.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from fiftyone) (21.3)\r\n",
      "Collecting aiofiles\r\n",
      "  Downloading aiofiles-0.8.0-py3-none-any.whl (13 kB)\r\n",
      "Collecting sse-starlette<1,>=0.10.3\r\n",
      "  Downloading sse_starlette-0.10.3-py3-none-any.whl (8.0 kB)\r\n",
      "Collecting ndjson\r\n",
      "  Downloading ndjson-0.3.1-py2.py3-none-any.whl (5.3 kB)\r\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from fiftyone) (2021.3)\r\n",
      "Collecting strawberry-graphql==0.96.0\r\n",
      "  Downloading strawberry_graphql-0.96.0-py3-none-any.whl (135 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.0/135.0 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting fiftyone-brain<0.9,>=0.8\r\n",
      "  Downloading fiftyone_brain-0.8.2-py3-none-any.whl (47 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.9/47.9 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting kaleido\r\n",
      "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting mongoengine==0.20.0\r\n",
      "  Downloading mongoengine-0.20.0-py3-none-any.whl (106 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting universal-analytics-python3<2,>=1.0.1\r\n",
      "  Downloading universal_analytics_python3-1.1.1-py3-none-any.whl (10 kB)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from fiftyone) (59.8.0)\r\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from fiftyone) (0.19.2)\r\n",
      "Collecting hypercorn>=0.13.2\r\n",
      "  Downloading Hypercorn-0.13.2-py3-none-any.whl (56 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from fiftyone) (3.5.1)\r\n",
      "Collecting motor<3,>=2.3\r\n",
      "  Downloading motor-2.5.1-py3-none-any.whl (55 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 KB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting fiftyone-db<0.4,>=0.3\r\n",
      "  Downloading fiftyone_db-0.3.0-py3-none-manylinux1_x86_64.whl (29.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.2/29.2 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting starlette==0.16.0\r\n",
      "  Downloading starlette-0.16.0-py3-none-any.whl (61 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting eventlet\r\n",
      "  Downloading eventlet-0.33.1-py2.py3-none-any.whl (226 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 KB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from fiftyone) (1.0.2)\r\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (from fiftyone) (0.8.9)\r\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.7/site-packages (from fiftyone) (1.22.0)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from fiftyone) (0.18.2)\r\n",
      "Collecting sseclient-py<2,>=1.7.2\r\n",
      "  Downloading sseclient_py-1.7.2-py2.py3-none-any.whl (8.4 kB)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from fiftyone) (1.21.6)\r\n",
      "Collecting argcomplete\r\n",
      "  Downloading argcomplete-2.0.0-py2.py3-none-any.whl (37 kB)\r\n",
      "Requirement already satisfied: pymongo<4,>=3.11 in /opt/conda/lib/python3.7/site-packages (from fiftyone) (3.12.3)\r\n",
      "Collecting xmltodict\r\n",
      "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\r\n",
      "Requirement already satisfied: Jinja2>=3 in /opt/conda/lib/python3.7/site-packages (from fiftyone) (3.1.1)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from fiftyone) (1.3.5)\r\n",
      "Collecting dacite>=1.6.0\r\n",
      "  Downloading dacite-1.6.0-py3-none-any.whl (12 kB)\r\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from fiftyone) (6.0)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from fiftyone) (5.9.0)\r\n",
      "Requirement already satisfied: retrying in /opt/conda/lib/python3.7/site-packages (from fiftyone) (1.3.3)\r\n",
      "Collecting pprintpp\r\n",
      "  Downloading pprintpp-0.4.0-py2.py3-none-any.whl (16 kB)\r\n",
      "Requirement already satisfied: Pillow>=6.2 in /opt/conda/lib/python3.7/site-packages (from fiftyone) (9.0.1)\r\n",
      "Collecting Deprecated\r\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\r\n",
      "Collecting plotly<5,>=4.14\r\n",
      "  Downloading plotly-4.14.3-py2.py3-none-any.whl (13.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.7/site-packages (from fiftyone) (4.5.4.60)\r\n",
      "Collecting voxel51-eta<0.8,>=0.7.0\r\n",
      "  Downloading voxel51_eta-0.7.0-py2.py3-none-any.whl (563 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.7/563.7 KB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: anyio<4,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from starlette==0.16.0->fiftyone) (3.5.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from starlette==0.16.0->fiftyone) (4.2.0)\r\n",
      "Requirement already satisfied: pygments<3.0,>=2.3 in /opt/conda/lib/python3.7/site-packages (from strawberry-graphql==0.96.0->fiftyone) (2.11.2)\r\n",
      "Requirement already satisfied: click<9.0,>=7.0 in /opt/conda/lib/python3.7/site-packages (from strawberry-graphql==0.96.0->fiftyone) (8.0.4)\r\n",
      "Collecting backports.cached-property<2.0.0,>=1.0.1\r\n",
      "  Downloading backports.cached_property-1.0.1-py3-none-any.whl (5.7 kB)\r\n",
      "Collecting python-multipart<0.0.6,>=0.0.5\r\n",
      "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting graphql-core<3.2.0,>=3.1.0\r\n",
      "  Downloading graphql_core-3.1.7-py3-none-any.whl (189 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.5/189.5 KB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting sentinel<0.4.0,>=0.3.0\r\n",
      "  Downloading sentinel-0.3.0-py3-none-any.whl (6.0 kB)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.7.0 in /opt/conda/lib/python3.7/site-packages (from strawberry-graphql==0.96.0->fiftyone) (2.8.2)\r\n",
      "Requirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from fiftyone-brain<0.9,>=0.8->fiftyone) (1.7.3)\r\n",
      "Requirement already satisfied: h11 in /opt/conda/lib/python3.7/site-packages (from hypercorn>=0.13.2->fiftyone) (0.13.0)\r\n",
      "Collecting h2>=3.1.0\r\n",
      "  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 KB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting priority\r\n",
      "  Downloading priority-2.0.0-py3-none-any.whl (8.9 kB)\r\n",
      "Collecting wsproto>=0.14.0\r\n",
      "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\r\n",
      "Requirement already satisfied: toml in /opt/conda/lib/python3.7/site-packages (from hypercorn>=0.13.2->fiftyone) (0.10.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from Jinja2>=3->fiftyone) (2.0.1)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from plotly<5,>=4.14->fiftyone) (1.16.0)\r\n",
      "Collecting httpx>=0.10.0\r\n",
      "  Downloading httpx-0.23.0-py3-none-any.whl (84 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.8/84.8 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting patool\r\n",
      "  Downloading patool-1.12-py2.py3-none-any.whl (77 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: urllib3 in /opt/conda/lib/python3.7/site-packages (from voxel51-eta<0.8,>=0.7.0->fiftyone) (1.26.8)\r\n",
      "Requirement already satisfied: tzlocal in /opt/conda/lib/python3.7/site-packages (from voxel51-eta<0.8,>=0.7.0->fiftyone) (4.2)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from voxel51-eta<0.8,>=0.7.0->fiftyone) (4.11.3)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from voxel51-eta<0.8,>=0.7.0->fiftyone) (0.3.4)\r\n",
      "Requirement already satisfied: sortedcontainers in /opt/conda/lib/python3.7/site-packages (from voxel51-eta<0.8,>=0.7.0->fiftyone) (2.4.0)\r\n",
      "Collecting glob2\r\n",
      "  Downloading glob2-0.7.tar.gz (10 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from voxel51-eta<0.8,>=0.7.0->fiftyone) (2.27.1)\r\n",
      "Requirement already satisfied: botocore<1.26.0,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from boto3->fiftyone) (1.25.0)\r\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3->fiftyone) (1.0.0)\r\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from boto3->fiftyone) (0.5.2)\r\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated->fiftyone) (1.14.0)\r\n",
      "Collecting dnspython>=1.15.0\r\n",
      "  Downloading dnspython-2.2.1-py3-none-any.whl (269 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.1/269.1 KB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: greenlet>=0.3 in /opt/conda/lib/python3.7/site-packages (from eventlet->fiftyone) (1.1.2)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fiftyone) (4.30.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fiftyone) (0.11.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fiftyone) (1.4.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fiftyone) (3.0.7)\r\n",
      "Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.7/site-packages (from scikit-image->fiftyone) (2.5)\r\n",
      "Requirement already satisfied: imageio>=2.4.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->fiftyone) (2.16.1)\r\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image->fiftyone) (2021.11.2)\r\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->fiftyone) (1.3.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->fiftyone) (3.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->fiftyone) (1.0.1)\r\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.7/site-packages (from anyio<4,>=3.0.0->starlette==0.16.0->fiftyone) (3.3)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.7/site-packages (from anyio<4,>=3.0.0->starlette==0.16.0->fiftyone) (1.2.0)\r\n",
      "Collecting hyperframe<7,>=6.0\r\n",
      "  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\r\n",
      "Collecting hpack<5,>=4.0\r\n",
      "  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\r\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (2021.10.8)\r\n",
      "Collecting rfc3986[idna2008]<2,>=1.3\r\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\r\n",
      "Collecting httpcore<0.16.0,>=0.15.0\r\n",
      "  Downloading httpcore-0.15.0-py3-none-any.whl (68 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.4/68.4 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->voxel51-eta<0.8,>=0.7.0->fiftyone) (3.7.0)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.2->scikit-image->fiftyone) (5.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->voxel51-eta<0.8,>=0.7.0->fiftyone) (2.0.12)\r\n",
      "Requirement already satisfied: pytz-deprecation-shim in /opt/conda/lib/python3.7/site-packages (from tzlocal->voxel51-eta<0.8,>=0.7.0->fiftyone) (0.1.0.post0)\r\n",
      "Requirement already satisfied: backports.zoneinfo in /opt/conda/lib/python3.7/site-packages (from tzlocal->voxel51-eta<0.8,>=0.7.0->fiftyone) (0.2.1)\r\n",
      "Collecting h11\r\n",
      "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: tzdata in /opt/conda/lib/python3.7/site-packages (from pytz-deprecation-shim->tzlocal->voxel51-eta<0.8,>=0.7.0->fiftyone) (2022.1)\r\n",
      "Building wheels for collected packages: python-multipart, glob2\r\n",
      "  Building wheel for python-multipart (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=8eeb0b5db111c5e39b040b662d63737390ab4da746e8db49b873869ac2e70c85\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/2c/41/7c/bfd1c180534ffdcc0972f78c5758f89881602175d48a8bcd2c\r\n",
      "  Building wheel for glob2 (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for glob2: filename=glob2-0.7-py2.py3-none-any.whl size=9320 sha256=ca8fc3569e8311d8ffc1f3f754eb992ef8f76504dcd68428fdcee3a5188d1884\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/d7/3c/72/5300602ba1269ffce8cff5dcf7b525fee756b57455903c37ba\r\n",
      "Successfully built python-multipart glob2\r\n",
      "Installing collected packages: sseclient-py, rfc3986, pprintpp, patool, ndjson, kaleido, glob2, xmltodict, sentinel, python-multipart, priority, motor, mongoengine, hyperframe, hpack, h11, graphql-core, fiftyone-db, dnspython, Deprecated, dacite, backports.cached-property, aiofiles, wsproto, starlette, plotly, httpcore, h2, eventlet, argcomplete, voxel51-eta, strawberry-graphql, sse-starlette, hypercorn, httpx, fiftyone-brain, universal-analytics-python3, fiftyone\r\n",
      "  Attempting uninstall: h11\r\n",
      "    Found existing installation: h11 0.13.0\r\n",
      "    Uninstalling h11-0.13.0:\r\n",
      "      Successfully uninstalled h11-0.13.0\r\n",
      "  Attempting uninstall: starlette\r\n",
      "    Found existing installation: starlette 0.17.1\r\n",
      "    Uninstalling starlette-0.17.1:\r\n",
      "      Successfully uninstalled starlette-0.17.1\r\n",
      "  Attempting uninstall: plotly\r\n",
      "    Found existing installation: plotly 5.7.0\r\n",
      "    Uninstalling plotly-5.7.0:\r\n",
      "      Successfully uninstalled plotly-5.7.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "fastapi 0.75.2 requires starlette==0.17.1, but you have starlette 0.16.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed Deprecated-1.2.13 aiofiles-0.8.0 argcomplete-2.0.0 backports.cached-property-1.0.1 dacite-1.6.0 dnspython-2.2.1 eventlet-0.33.1 fiftyone-0.16.1 fiftyone-brain-0.8.2 fiftyone-db-0.3.0 glob2-0.7 graphql-core-3.1.7 h11-0.12.0 h2-4.1.0 hpack-4.0.0 httpcore-0.15.0 httpx-0.23.0 hypercorn-0.13.2 hyperframe-6.0.1 kaleido-0.2.1 mongoengine-0.20.0 motor-2.5.1 ndjson-0.3.1 patool-1.12 plotly-4.14.3 pprintpp-0.4.0 priority-2.0.0 python-multipart-0.0.5 rfc3986-1.5.0 sentinel-0.3.0 sse-starlette-0.10.3 sseclient-py-1.7.2 starlette-0.16.0 strawberry-graphql-0.96.0 universal-analytics-python3-1.1.1 voxel51-eta-0.7.0 wsproto-1.1.0 xmltodict-0.13.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install fiftyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4b6a4d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T11:27:13.784579Z",
     "iopub.status.busy": "2022-06-01T11:27:13.784183Z",
     "iopub.status.idle": "2022-06-01T11:27:32.963595Z",
     "shell.execute_reply": "2022-06-01T11:27:32.962421Z"
    },
    "papermill": {
     "duration": 19.295966,
     "end_time": "2022-06-01T11:27:32.966364",
     "exception": false,
     "start_time": "2022-06-01T11:27:13.670398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumExpr defaulting to 4 threads.\n",
      "Migrating database to v0.16.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "\n",
    "from tqdm import tqdm\n",
    "from shutil import copyfile\n",
    "from tabulate import tabulate\n",
    "from sklearn.metrics import accuracy_score, log_loss, confusion_matrix, f1_score\n",
    "\n",
    "# https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/274717\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b94602de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T11:27:33.195044Z",
     "iopub.status.busy": "2022-06-01T11:27:33.193885Z",
     "iopub.status.idle": "2022-06-01T11:27:33.200040Z",
     "shell.execute_reply": "2022-06-01T11:27:33.199080Z"
    },
    "papermill": {
     "duration": 0.121957,
     "end_time": "2022-06-01T11:27:33.202199",
     "exception": false,
     "start_time": "2022-06-01T11:27:33.080242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting the seeds\n",
    "SEED = 0\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fb999f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T11:27:33.428350Z",
     "iopub.status.busy": "2022-06-01T11:27:33.428018Z",
     "iopub.status.idle": "2022-06-01T11:27:33.449652Z",
     "shell.execute_reply": "2022-06-01T11:27:33.448530Z"
    },
    "papermill": {
     "duration": 0.138502,
     "end_time": "2022-06-01T11:27:33.452723",
     "exception": false,
     "start_time": "2022-06-01T11:27:33.314221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device not found\n",
      "Found GPU at: \n"
     ]
    }
   ],
   "source": [
    "# Making sure that Tensorflow is able to detect the GPU\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if \"GPU\" not in device_name:\n",
    "    print(\"GPU device not found\")\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f846d8",
   "metadata": {
    "papermill": {
     "duration": 0.111391,
     "end_time": "2022-06-01T11:27:33.676726",
     "exception": false,
     "start_time": "2022-06-01T11:27:33.565335",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Importing the Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cce1baaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T11:27:33.904204Z",
     "iopub.status.busy": "2022-06-01T11:27:33.903831Z",
     "iopub.status.idle": "2022-06-01T11:28:13.493520Z",
     "shell.execute_reply": "2022-06-01T11:28:13.491974Z"
    },
    "papermill": {
     "duration": 39.707922,
     "end_time": "2022-06-01T11:28:13.496926",
     "exception": false,
     "start_time": "2022-06-01T11:27:33.789004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Train Dataset:\n",
      "(40006, 3072) (40006, 1) (40006, 10)\n",
      "(40006, 32, 32, 3)\n",
      "For Test Dataset:\n",
      "(10000, 3072) (10000, 1)\n",
      "(10000, 3, 32, 32)\n",
      "(10000, 32, 32, 3) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Importing the Labelled Training Dataset\n",
    "print(\"For Train Dataset:\")\n",
    "df_train = pd.read_csv(\"../input/cifar10/train_lab_x.csv\")\n",
    "y_train = pd.read_csv(\"../input/cifar10/train_lab_y.csv\")\n",
    "df_train = np.array(df_train)\n",
    "y_train = np.array(y_train)\n",
    "y_train_oh = tf.one_hot(np.ravel(y_train), depth = 10)\n",
    "print(df_train.shape, y_train.shape, y_train_oh.shape)\n",
    "\n",
    "# Reshaping, rescaling and one-hot encoding\n",
    "df_train = np.reshape(df_train, (-1, 3, 32, 32))\n",
    "df_train = np.transpose(np.array(df_train), (0, 2, 3, 1))\n",
    "df_train = df_train / 255\n",
    "print(df_train.shape)\n",
    "\n",
    "# Importing the Test Dataset\n",
    "print(\"For Test Dataset:\")\n",
    "df_test = pd.read_csv(\"../input/cifar10/test_x.csv\")\n",
    "y_test = pd.read_csv(\"../input/cifar10/test_y.csv\")\n",
    "df_test = np.array(df_test)\n",
    "y_test = np.array(y_test)\n",
    "print(df_test.shape, y_test.shape)\n",
    "\n",
    "# Reshaping the dataset\n",
    "df_test = np.reshape(df_test, (-1, 3, 32, 32))\n",
    "print(df_test.shape)\n",
    "\n",
    "# Reshaping, rescaling and one-hot encoding\n",
    "df_test = np.transpose(np.array(df_test), (0, 2, 3, 1))\n",
    "df_test = df_test / 255\n",
    "y_test_oh = tf.one_hot(np.ravel(y_test), depth = 10)\n",
    "print(df_test.shape, y_test_oh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c09350d",
   "metadata": {
    "papermill": {
     "duration": 0.121077,
     "end_time": "2022-06-01T11:28:13.739206",
     "exception": false,
     "start_time": "2022-06-01T11:28:13.618129",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Performing the Augmentations on the Training Set (Creating the Pool)\n",
    "## 3.1. GAN-Based Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "660f0019",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T11:28:13.974342Z",
     "iopub.status.busy": "2022-06-01T11:28:13.973967Z",
     "iopub.status.idle": "2022-06-01T11:28:26.846559Z",
     "shell.execute_reply": "2022-06-01T11:28:26.845404Z"
    },
    "papermill": {
     "duration": 12.994277,
     "end_time": "2022-06-01T11:28:26.849935",
     "exception": false,
     "start_time": "2022-06-01T11:28:13.855658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10016, 32, 32, 3) (10016, 1)\n"
     ]
    }
   ],
   "source": [
    "df_gan_aug = pd.read_csv(\"../input/cifar10/df_25per_gan_aug.csv\")\n",
    "y_gan_aug = pd.read_csv(\"../input/cifar10/y_25per_gan_aug.csv\")\n",
    "df_gan_aug = np.array(df_gan_aug)\n",
    "y_gan_aug = np.array(y_gan_aug)\n",
    "\n",
    "# Reshaping, rescaling and one-hot encoding\n",
    "df_gan_aug = np.reshape(df_gan_aug, (-1, 3, 32, 32))\n",
    "df_gan_aug = np.transpose(np.array(df_gan_aug), (0, 2, 3, 1))\n",
    "print(df_gan_aug.shape, y_gan_aug.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60df32f5",
   "metadata": {
    "papermill": {
     "duration": 0.120034,
     "end_time": "2022-06-01T11:28:27.089726",
     "exception": false,
     "start_time": "2022-06-01T11:28:26.969692",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.2. Traditional Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84d8c84d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T11:28:27.327892Z",
     "iopub.status.busy": "2022-06-01T11:28:27.326947Z",
     "iopub.status.idle": "2022-06-01T11:28:39.244809Z",
     "shell.execute_reply": "2022-06-01T11:28:39.244110Z"
    },
    "papermill": {
     "duration": 12.040924,
     "end_time": "2022-06-01T11:28:39.247270",
     "exception": false,
     "start_time": "2022-06-01T11:28:27.206346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10045, 32, 32, 3) (10045, 1)\n"
     ]
    }
   ],
   "source": [
    "df_trad_aug = pd.read_csv(\"../input/cifar10/df_25per_trad_aug.csv\")\n",
    "y_trad_aug = pd.read_csv(\"../input/cifar10/y_25per_trad_aug.csv\")\n",
    "df_trad_aug = np.array(df_trad_aug)\n",
    "y_trad_aug = np.array(y_trad_aug)\n",
    "\n",
    "# Reshaping, rescaling and one-hot encoding\n",
    "df_trad_aug = np.reshape(df_trad_aug, (-1, 32, 32, 3))\n",
    "print(df_trad_aug.shape, y_trad_aug.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c12732",
   "metadata": {
    "papermill": {
     "duration": 0.1155,
     "end_time": "2022-06-01T11:28:39.481779",
     "exception": false,
     "start_time": "2022-06-01T11:28:39.366279",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.3. Creating the Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac787093",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T11:28:39.714312Z",
     "iopub.status.busy": "2022-06-01T11:28:39.713324Z",
     "iopub.status.idle": "2022-06-01T11:28:41.725061Z",
     "shell.execute_reply": "2022-06-01T11:28:41.723956Z"
    },
    "papermill": {
     "duration": 2.130722,
     "end_time": "2022-06-01T11:28:41.727855",
     "exception": false,
     "start_time": "2022-06-01T11:28:39.597133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20061, 32, 32, 3) (20061, 1) (20061, 10)\n"
     ]
    }
   ],
   "source": [
    "# Concatenating the Augmented Datasets\n",
    "df_pool = np.concatenate([df_gan_aug, df_trad_aug], axis=0)\n",
    "y_pool = np.concatenate([y_gan_aug, y_trad_aug], axis=0)\n",
    "\n",
    "# Creating a random permutation & shuffling the dataset\n",
    "perm = np.random.permutation(df_pool.shape[0])\n",
    "df_pool = np.array(df_pool[perm, : , : , : ])\n",
    "y_pool = y_pool[perm]\n",
    "\n",
    "# One-Hot Encoding\n",
    "y_pool_oh = tf.one_hot(np.ravel(y_pool), depth = 10)\n",
    "print(df_pool.shape, y_pool.shape, y_pool_oh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50a0beb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T11:28:41.963866Z",
     "iopub.status.busy": "2022-06-01T11:28:41.962796Z",
     "iopub.status.idle": "2022-06-01T11:28:41.968189Z",
     "shell.execute_reply": "2022-06-01T11:28:41.967448Z"
    },
    "papermill": {
     "duration": 0.125645,
     "end_time": "2022-06-01T11:28:41.970303",
     "exception": false,
     "start_time": "2022-06-01T11:28:41.844658",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Code to create a 5*5 grid of images, along with their labels\n",
    "# fig, ax = plt.subplots(5, 5, figsize = (7, 7))\n",
    "# fig.tight_layout()\n",
    "\n",
    "# for ind in range(25):\n",
    "#     example = df_pool[ind, : , : , : ]\n",
    "#     axis = ax[ind // 5][ind % 5]\n",
    "#     axis.get_xaxis().set_visible(False)\n",
    "#     axis.get_yaxis().set_visible(False)\n",
    "#     axis.imshow(example)\n",
    "#     axis.set(title = str(y_pool[ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991fc375",
   "metadata": {
    "papermill": {
     "duration": 0.114957,
     "end_time": "2022-06-01T11:28:42.203666",
     "exception": false,
     "start_time": "2022-06-01T11:28:42.088709",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Creating Images and Labels (JSON) in a Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0b64580",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T11:28:42.437958Z",
     "iopub.status.busy": "2022-06-01T11:28:42.437581Z",
     "iopub.status.idle": "2022-06-01T11:28:42.442700Z",
     "shell.execute_reply": "2022-06-01T11:28:42.441701Z"
    },
    "papermill": {
     "duration": 0.126214,
     "end_time": "2022-06-01T11:28:42.444921",
     "exception": false,
     "start_time": "2022-06-01T11:28:42.318707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TRAIN_IMGS_PATH = r'./train/'\n",
    "# POOL_IMGS_PATH = r'./pool/'\n",
    "\n",
    "# Making Directories if they don't exist\n",
    "# try:\n",
    "#     os.mkdir(TRAIN_IMGS_PATH)\n",
    "#     os.mkdir(POOL_IMGS_PATH)\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# def create_dir_images(data_csv, path, is_train = True):\n",
    "#     \"\"\"A function to create images from CSV file\"\"\"\n",
    "#     for i in tqdm(range(len(data_csv))):\n",
    "#         img = data_csv[i, : , : , : ] * 255\n",
    "#         img = img.astype(np.uint8)\n",
    "#         if is_train:\n",
    "#             cv2.imwrite(path + str(i) + '_train.png', img)\n",
    "#         else:\n",
    "#             cv2.imwrite(path + str(i) + '_pool.png', img)  \n",
    "\n",
    "# Create Training & Pool Images\n",
    "# create_dir_images(df_train, TRAIN_IMGS_PATH, True)\n",
    "# create_dir_images(df_pool, POOL_IMGS_PATH, False)\n",
    "\n",
    "# Create JSON files for the corresponding labels\n",
    "# y_train = pd.Series(np.squeeze(y_train))\n",
    "# y_train.to_json(\"./y_train.json\")\n",
    "# y_pool = pd.Series(np.squeeze(y_pool))\n",
    "# y_pool.to_json(\"./y_pool.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e2bf3a7",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-06-01T11:28:42.676058Z",
     "iopub.status.busy": "2022-06-01T11:28:42.675720Z",
     "iopub.status.idle": "2022-06-01T11:28:42.679082Z",
     "shell.execute_reply": "2022-06-01T11:28:42.678338Z"
    },
    "papermill": {
     "duration": 0.121353,
     "end_time": "2022-06-01T11:28:42.680970",
     "exception": false,
     "start_time": "2022-06-01T11:28:42.559617",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Zipping the files for download\n",
    "# !tar chvfz data.tar.gz \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be48e6b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T11:28:42.915162Z",
     "iopub.status.busy": "2022-06-01T11:28:42.914162Z",
     "iopub.status.idle": "2022-06-01T11:30:47.686882Z",
     "shell.execute_reply": "2022-06-01T11:30:47.685617Z"
    },
    "papermill": {
     "duration": 124.893504,
     "end_time": "2022-06-01T11:30:47.689748",
     "exception": false,
     "start_time": "2022-06-01T11:28:42.796244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 40006/40006 [30.6s elapsed, 0s remaining, 1.1K samples/s]       \n",
      " 100% |█████████████| 20061/20061 [15.0s elapsed, 0s remaining, 1.4K samples/s]      \n"
     ]
    }
   ],
   "source": [
    "# Importing the Train Dataset\n",
    "train_name = \"train-dataset\"\n",
    "train_dataset_dir = \"../input/cifar10/iter_samp/train_data\"\n",
    "\n",
    "# Create the Voxel training dataset\n",
    "train_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=train_dataset_dir,\n",
    "    dataset_type=fo.types.FiftyOneImageClassificationDataset,\n",
    "    name=train_name,\n",
    ")\n",
    "\n",
    "# Importing the Pool Dataset\n",
    "pool_name = \"pool-dataset\"\n",
    "pool_dataset_dir = \"../input/cifar10/iter_samp/pool_data\"\n",
    "\n",
    "# Create the Voxel pool dataset\n",
    "pool_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=pool_dataset_dir,\n",
    "    dataset_type=fo.types.FiftyOneImageClassificationDataset,\n",
    "    name=pool_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8bfd57",
   "metadata": {
    "papermill": {
     "duration": 0.277788,
     "end_time": "2022-06-01T11:30:48.250596",
     "exception": false,
     "start_time": "2022-06-01T11:30:47.972808",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Performing Iterative Sampling\n",
    "## 5.1. Defining a function to get class-wise samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "416d7196",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T11:30:48.812486Z",
     "iopub.status.busy": "2022-06-01T11:30:48.812160Z",
     "iopub.status.idle": "2022-06-01T11:30:48.852870Z",
     "shell.execute_reply": "2022-06-01T11:30:48.851788Z"
    },
    "papermill": {
     "duration": 0.324839,
     "end_time": "2022-06-01T11:30:48.854924",
     "exception": false,
     "start_time": "2022-06-01T11:30:48.530085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████████| 50/50 [24.5ms elapsed, 0s remaining, 2.0K samples/s] \n",
      "[ 5  0 45  0  0  0  0  0  0  0] 10 5\n"
     ]
    }
   ],
   "source": [
    "def get_class_samples(data, progress = True):\n",
    "    \"\"\"A function to get the class-wise indices for a dataset\"\"\"\n",
    "    \n",
    "    ## Initialization\n",
    "    num_examples = np.zeros((10,))\n",
    "    # Creating a list of lists for storing the indices of data-points in the dataset, class-wise\n",
    "    classes_ind = []\n",
    "    for i in range(10):\n",
    "        classes_ind.append([])\n",
    "    \n",
    "    ## Iterating over the dataset\n",
    "    for sample in data.iter_samples(progress=progress):\n",
    "        label = int(sample['ground_truth']['label'])\n",
    "        voxel_id = sample['id']\n",
    "        num_examples[label] += 1\n",
    "        classes_ind[label].append(voxel_id)\n",
    "    \n",
    "    ## Number of examples from each class\n",
    "    num_exa = num_examples.astype('int32')\n",
    "    return num_exa, classes_ind\n",
    "\n",
    "# Example\n",
    "num_exa, classes_ind = get_class_samples(train_dataset[:50])\n",
    "print(num_exa, len(classes_ind), len(classes_ind[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0f4210",
   "metadata": {
    "papermill": {
     "duration": 0.283334,
     "end_time": "2022-06-01T11:30:49.424026",
     "exception": false,
     "start_time": "2022-06-01T11:30:49.140692",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.2. Defining a function to get the embeddings\n",
    "- We will be using the default embeddings for the images, generated by the FiftyOne's library functions. They are generated using the **MobileNetv2** model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590ad24f",
   "metadata": {
    "papermill": {
     "duration": 0.306435,
     "end_time": "2022-06-01T11:30:50.029649",
     "exception": false,
     "start_time": "2022-06-01T11:30:49.723214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.3. Defining the Similarity Function\n",
    "- For this, we will be using the [FiftyOne](https://voxel51.com/docs/fiftyone/) library.\n",
    "\n",
    "#### References\n",
    "- [Compute Similarity from data-points and embeddings](https://voxel51.com/docs/fiftyone/api/fiftyone.brain.html#fiftyone.brain.compute_similarity)\n",
    "- [Find duplicates based on threshold/fraction](https://voxel51.com/docs/fiftyone/api/fiftyone.brain.similarity.html#fiftyone.brain.similarity.SimilarityResults.find_duplicates)\n",
    "- [Select samples from a dataset](https://voxel51.com/docs/fiftyone/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.select)\n",
    "- [Add samples to a dataset](https://voxel51.com/docs/fiftyone/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.add_samples)\n",
    "- [Remove samples from a dataset](https://voxel51.com/docs/fiftyone/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.delete_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2325bbb2",
   "metadata": {
    "papermill": {
     "duration": 0.285598,
     "end_time": "2022-06-01T11:30:50.600567",
     "exception": false,
     "start_time": "2022-06-01T11:30:50.314969",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.4. Running the Iterative Sampling Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f24e03ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T11:30:51.165857Z",
     "iopub.status.busy": "2022-06-01T11:30:51.165135Z",
     "iopub.status.idle": "2022-06-01T11:31:17.789559Z",
     "shell.execute_reply": "2022-06-01T11:31:17.788567Z"
    },
    "papermill": {
     "duration": 26.911786,
     "end_time": "2022-06-01T11:31:17.792403",
     "exception": false,
     "start_time": "2022-06-01T11:30:50.880617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 40006/40006 [18.0s elapsed, 0s remaining, 2.4K samples/s]      \n",
      " 100% |█████████████| 20061/20061 [8.6s elapsed, 0s remaining, 2.4K samples/s]        \n",
      "[4109 3839 4022 4116 4312 3952 4290 3552 3436 4378] [2073 1917 2026 2021 2177 1975 2184 1783 1691 2214]\n"
     ]
    }
   ],
   "source": [
    "num_train_samples, train_classes = get_class_samples(train_dataset)\n",
    "num_pool_samples, pool_classes = get_class_samples(pool_dataset)\n",
    "print(num_train_samples, num_pool_samples)\n",
    "\n",
    "# We will consider the `max_sizes` equal to the number of samples for each \n",
    "# class with which we started\n",
    "max_sizes = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2a2ad9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T11:31:18.745463Z",
     "iopub.status.busy": "2022-06-01T11:31:18.745056Z",
     "iopub.status.idle": "2022-06-01T11:31:18.750447Z",
     "shell.execute_reply": "2022-06-01T11:31:18.749236Z"
    },
    "papermill": {
     "duration": 0.478676,
     "end_time": "2022-06-01T11:31:18.752771",
     "exception": false,
     "start_time": "2022-06-01T11:31:18.274095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes = np.arange(10)\n",
    "max_iterations = 5\n",
    "threshold = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eddd72",
   "metadata": {
    "papermill": {
     "duration": 0.468951,
     "end_time": "2022-06-01T11:31:19.720495",
     "exception": false,
     "start_time": "2022-06-01T11:31:19.251544",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- When we delete samples from train_dataset, train_view gets automatically updated. But when we add samples to the dataset, we need to re-initialize the view. For reference, refer to the example mentioned towards the end of this kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfadf7e1",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-06-01T11:31:20.670590Z",
     "iopub.status.busy": "2022-06-01T11:31:20.669710Z",
     "iopub.status.idle": "2022-06-01T15:03:26.863547Z",
     "shell.execute_reply": "2022-06-01T15:03:26.861707Z"
    },
    "papermill": {
     "duration": 12726.668368,
     "end_time": "2022-06-01T15:03:26.866931",
     "exception": false,
     "start_time": "2022-06-01T11:31:20.198563",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Class 0:\n",
      "Downloading model from 'https://download.pytorch.org/models/mobilenet_v2-b0353104.pth'...\n",
      " 100% |████|  108.4Mb/108.4Mb [330.1ms elapsed, 0s remaining, 328.5Mb/s]     \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4109/4109 [2.5m elapsed, 0s remaining, 26.6 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 1, # Duplicate Images = 4\n",
      " 100% |█████████████████| 722/722 [920.7ms elapsed, 0s remaining, 786.8 samples/s]      \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4827/4827 [3.0m elapsed, 0s remaining, 25.1 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 2, # Duplicate Images = 8\n",
      " 100% |█████████████████| 165/165 [229.2ms elapsed, 0s remaining, 719.9 samples/s]     \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4984/4984 [3.1m elapsed, 0s remaining, 28.6 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 3, # Duplicate Images = 5\n",
      " 100% |███████████████████| 21/21 [43.3ms elapsed, 0s remaining, 484.7 samples/s]     \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 5000/5000 [3.2m elapsed, 0s remaining, 25.9 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 4, # Duplicate Images = 0\n",
      " 100% |█████████████████████| 0/0 [2.5ms elapsed, ? remaining, ? samples/s] \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 5000/5000 [3.1m elapsed, 0s remaining, 26.8 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 5, # Duplicate Images = 0\n",
      " 100% |█████████████████████| 0/0 [2.8ms elapsed, ? remaining, ? samples/s] \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 5000/5000 [3.1m elapsed, 0s remaining, 27.9 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "\n",
      "For Class 1:\n",
      "Computing embeddings...\n",
      " 100% |███████████████| 3839/3839 [2.4m elapsed, 0s remaining, 23.0 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 1, # Duplicate Images = 1\n",
      " 100% |█████████████████| 897/897 [1.7s elapsed, 0s remaining, 523.6 samples/s]      \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4735/4735 [3.0m elapsed, 0s remaining, 28.3 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 2, # Duplicate Images = 14\n",
      " 100% |█████████████████| 246/246 [474.1ms elapsed, 0s remaining, 518.9 samples/s]      \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4967/4967 [3.1m elapsed, 0s remaining, 20.7 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 3, # Duplicate Images = 4\n",
      " 100% |███████████████████| 37/37 [65.7ms elapsed, 0s remaining, 563.4 samples/s]     \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 5000/5000 [3.1m elapsed, 0s remaining, 28.0 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 4, # Duplicate Images = 0\n",
      " 100% |█████████████████████| 0/0 [2.2ms elapsed, ? remaining, ? samples/s] \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 5000/5000 [3.1m elapsed, 0s remaining, 26.6 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 5, # Duplicate Images = 0\n",
      " 100% |█████████████████████| 0/0 [2.7ms elapsed, ? remaining, ? samples/s] \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 5000/5000 [3.2m elapsed, 0s remaining, 28.5 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "\n",
      "For Class 2:\n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4022/4022 [2.6m elapsed, 0s remaining, 24.7 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 1, # Duplicate Images = 0\n",
      " 100% |█████████████████| 765/765 [1.1s elapsed, 0s remaining, 724.2 samples/s]         \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4787/4787 [3.1m elapsed, 0s remaining, 19.8 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 2, # Duplicate Images = 14\n",
      " 100% |█████████████████| 215/215 [298.5ms elapsed, 0s remaining, 728.1 samples/s]      \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4988/4988 [3.3m elapsed, 0s remaining, 19.8 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 3, # Duplicate Images = 2\n",
      " 100% |███████████████████| 14/14 [30.6ms elapsed, 0s remaining, 457.9 samples/s]    \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 5000/5000 [3.2m elapsed, 0s remaining, 26.4 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 4, # Duplicate Images = 0\n",
      " 100% |█████████████████████| 0/0 [2.3ms elapsed, ? remaining, ? samples/s] \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 5000/5000 [3.2m elapsed, 0s remaining, 26.2 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 5, # Duplicate Images = 0\n",
      " 100% |█████████████████████| 0/0 [2.9ms elapsed, ? remaining, ? samples/s] \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 5000/5000 [3.1m elapsed, 0s remaining, 25.4 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "\n",
      "For Class 3:\n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4116/4116 [2.7m elapsed, 0s remaining, 26.4 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 1, # Duplicate Images = 0\n",
      " 100% |█████████████████| 710/710 [993.2ms elapsed, 0s remaining, 714.9 samples/s]      \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4826/4826 [3.1m elapsed, 0s remaining, 25.8 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 2, # Duplicate Images = 7\n",
      " 100% |█████████████████| 168/168 [675.5ms elapsed, 0s remaining, 248.7 samples/s]      \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4987/4987 [3.2m elapsed, 0s remaining, 26.6 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 3, # Duplicate Images = 1\n",
      " 100% |███████████████████| 14/14 [34.3ms elapsed, 0s remaining, 408.7 samples/s]    \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 5000/5000 [3.1m elapsed, 0s remaining, 27.6 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 4, # Duplicate Images = 1\n",
      " 100% |█████████████████████| 1/1 [10.1ms elapsed, 0s remaining, 163.4 samples/s] \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 5000/5000 [3.2m elapsed, 0s remaining, 26.9 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 5, # Duplicate Images = 0\n",
      " 100% |█████████████████████| 0/0 [2.3ms elapsed, ? remaining, ? samples/s] \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 5000/5000 [3.2m elapsed, 0s remaining, 27.9 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "\n",
      "For Class 4:\n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4312/4312 [2.8m elapsed, 0s remaining, 28.8 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 1, # Duplicate Images = 4\n",
      " 100% |█████████████████| 574/574 [739.5ms elapsed, 0s remaining, 779.5 samples/s]      \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4882/4882 [3.0m elapsed, 0s remaining, 25.5 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 2, # Duplicate Images = 17\n",
      " 100% |█████████████████| 129/129 [185.7ms elapsed, 0s remaining, 694.8 samples/s]    \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4994/4994 [3.2m elapsed, 0s remaining, 26.0 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 3, # Duplicate Images = 3\n",
      " 100% |█████████████████████| 9/9 [22.7ms elapsed, 0s remaining, 395.7 samples/s]    \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 5000/5000 [3.2m elapsed, 0s remaining, 26.7 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 4, # Duplicate Images = 0\n",
      " 100% |█████████████████████| 0/0 [2.8ms elapsed, ? remaining, ? samples/s] \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 5000/5000 [3.3m elapsed, 0s remaining, 29.4 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 5, # Duplicate Images = 0\n",
      " 100% |█████████████████████| 0/0 [3.1ms elapsed, ? remaining, ? samples/s] \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 5000/5000 [3.1m elapsed, 0s remaining, 24.7 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "\n",
      "For Class 5:\n",
      "Computing embeddings...\n",
      " 100% |███████████████| 3952/3952 [2.7m elapsed, 0s remaining, 26.8 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 1, # Duplicate Images = 0\n",
      " 100% |█████████████████| 821/821 [1.1s elapsed, 0s remaining, 781.7 samples/s]         \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4773/4773 [3.2m elapsed, 0s remaining, 25.9 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 2, # Duplicate Images = 6\n",
      " 100% |█████████████████| 207/207 [285.4ms elapsed, 0s remaining, 738.1 samples/s]      \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4974/4974 [3.0m elapsed, 0s remaining, 25.6 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 3, # Duplicate Images = 0\n",
      " 100% |███████████████████| 26/26 [50.3ms elapsed, 0s remaining, 517.2 samples/s]     \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 5000/5000 [3.1m elapsed, 0s remaining, 25.4 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 4, # Duplicate Images = 0\n",
      " 100% |█████████████████████| 0/0 [2.4ms elapsed, ? remaining, ? samples/s] \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 5000/5000 [3.3m elapsed, 0s remaining, 26.4 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 5, # Duplicate Images = 0\n",
      " 100% |█████████████████████| 0/0 [3.1ms elapsed, ? remaining, ? samples/s] \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 5000/5000 [3.2m elapsed, 0s remaining, 28.0 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "\n",
      "For Class 6:\n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4290/4290 [2.7m elapsed, 0s remaining, 22.1 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 1, # Duplicate Images = 6\n",
      " 100% |█████████████████| 613/613 [827.3ms elapsed, 0s remaining, 740.9 samples/s]      \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4897/4897 [3.0m elapsed, 0s remaining, 26.0 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 2, # Duplicate Images = 11\n",
      " 100% |█████████████████| 111/111 [159.3ms elapsed, 0s remaining, 711.0 samples/s]    \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4997/4997 [3.2m elapsed, 0s remaining, 27.3 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 3, # Duplicate Images = 0\n",
      " 100% |█████████████████████| 3/3 [15.7ms elapsed, 0s remaining, 190.6 samples/s]    \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 5000/5000 [3.4m elapsed, 0s remaining, 28.3 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 4, # Duplicate Images = 0\n",
      " 100% |█████████████████████| 0/0 [2.4ms elapsed, ? remaining, ? samples/s] \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 5000/5000 [3.1m elapsed, 0s remaining, 26.3 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 5, # Duplicate Images = 0\n",
      " 100% |█████████████████████| 0/0 [3.1ms elapsed, ? remaining, ? samples/s] \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 5000/5000 [3.2m elapsed, 0s remaining, 21.5 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "\n",
      "For Class 7:\n",
      "Computing embeddings...\n",
      " 100% |███████████████| 3552/3552 [2.4m elapsed, 0s remaining, 26.3 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 1, # Duplicate Images = 0\n",
      " 100% |███████████████| 1007/1007 [1.3s elapsed, 0s remaining, 759.2 samples/s]         \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4559/4559 [2.9m elapsed, 0s remaining, 24.7 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 2, # Duplicate Images = 9\n",
      " 100% |█████████████████| 345/345 [467.6ms elapsed, 0s remaining, 745.4 samples/s]      \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4895/4895 [3.4m elapsed, 0s remaining, 22.0 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 3, # Duplicate Images = 3\n",
      " 100% |███████████████████| 97/97 [142.9ms elapsed, 0s remaining, 693.9 samples/s]    \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4989/4989 [3.1m elapsed, 0s remaining, 27.7 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 4, # Duplicate Images = 1\n",
      " 100% |███████████████████| 12/12 [31.4ms elapsed, 0s remaining, 382.6 samples/s]    \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 5000/5000 [3.1m elapsed, 0s remaining, 26.5 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 5, # Duplicate Images = 0\n",
      " 100% |█████████████████████| 0/0 [3.1ms elapsed, ? remaining, ? samples/s] \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 5000/5000 [3.5m elapsed, 0s remaining, 26.7 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "\n",
      "For Class 8:\n",
      "Computing embeddings...\n",
      " 100% |███████████████| 3436/3436 [2.1m elapsed, 0s remaining, 28.4 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 1, # Duplicate Images = 0\n",
      " 100% |███████████████| 1011/1011 [1.3s elapsed, 0s remaining, 777.7 samples/s]         \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4447/4447 [2.7m elapsed, 0s remaining, 28.5 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 2, # Duplicate Images = 18\n",
      " 100% |█████████████████| 392/392 [507.5ms elapsed, 0s remaining, 778.9 samples/s]      \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4821/4821 [3.2m elapsed, 0s remaining, 26.6 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 3, # Duplicate Images = 12\n",
      " 100% |█████████████████| 139/139 [202.7ms elapsed, 0s remaining, 685.8 samples/s]     \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4948/4948 [3.1m elapsed, 0s remaining, 26.0 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 4, # Duplicate Images = 1\n",
      " 100% |███████████████████| 45/45 [88.3ms elapsed, 0s remaining, 509.8 samples/s]     \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4992/4992 [3.2m elapsed, 0s remaining, 19.2 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 5, # Duplicate Images = 0\n",
      " 100% |█████████████████████| 8/8 [35.3ms elapsed, 0s remaining, 226.9 samples/s]    \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 5000/5000 [3.4m elapsed, 0s remaining, 23.8 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "\n",
      "For Class 9:\n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4378/4378 [2.7m elapsed, 0s remaining, 28.2 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 1, # Duplicate Images = 0\n",
      " 100% |█████████████████| 545/545 [1.1s elapsed, 0s remaining, 477.6 samples/s]         \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4923/4923 [3.3m elapsed, 0s remaining, 25.2 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 2, # Duplicate Images = 7\n",
      " 100% |███████████████████| 82/82 [123.7ms elapsed, 0s remaining, 679.1 samples/s]    \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 4998/4998 [3.1m elapsed, 0s remaining, 25.9 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 3, # Duplicate Images = 1\n",
      " 100% |█████████████████████| 3/3 [13.6ms elapsed, 0s remaining, 220.9 samples/s]    \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 5000/5000 [3.4m elapsed, 0s remaining, 22.1 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 4, # Duplicate Images = 0\n",
      " 100% |█████████████████████| 0/0 [3.1ms elapsed, ? remaining, ? samples/s] \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 5000/5000 [3.9m elapsed, 0s remaining, 23.8 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "For iteration 5, # Duplicate Images = 0\n",
      " 100% |█████████████████████| 0/0 [3.1ms elapsed, ? remaining, ? samples/s] \n",
      "Computing embeddings...\n",
      " 100% |███████████████| 5000/5000 [3.2m elapsed, 0s remaining, 26.5 samples/s]      \n",
      "Generating index...\n",
      "Index complete\n",
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for clas in classes:\n",
    "    print(f\"For Class {clas}:\")\n",
    "    num_train_samples, train_classes = get_class_samples(train_dataset, progress = False)\n",
    "    num_pool_samples, pool_classes = get_class_samples(pool_dataset, progress = False)\n",
    "    \n",
    "    train_view = train_dataset.select(train_classes[clas])\n",
    "    \n",
    "    # Ensuring that the train dataset has at least 1 sample for this class\n",
    "    if len(train_view) > 0:\n",
    "        iteration = 1\n",
    "        results = fob.compute_similarity(train_view)\n",
    "        results.find_duplicates(thresh = threshold)\n",
    "        duplicate_ids = results.duplicate_ids\n",
    "\n",
    "        while len(duplicate_ids) > 0 or iteration <= max_iterations:\n",
    "            print(f\"For iteration {iteration}, # Duplicate Images = {len(duplicate_ids)}\")\n",
    "            train_dataset.delete_samples(duplicate_ids)\n",
    "            m = max_sizes - len(train_view)\n",
    "            \n",
    "            # Making sure that the pool dataset has enough samples\n",
    "            if num_pool_samples[clas] >= m:\n",
    "                pass\n",
    "            elif num_pool_samples[clas] > 0 and num_pool_samples[clas] < m:\n",
    "                # Decrease the value of m\n",
    "                m = num_pool_samples[clas]\n",
    "            else:\n",
    "                duplicate_ids = []\n",
    "                iteration += 1\n",
    "                continue\n",
    "                \n",
    "            m_ids = random.choices(pool_classes[clas], k = m)\n",
    "            pool_samples = pool_dataset.select(m_ids)\n",
    "            train_dataset.add_samples(pool_samples)\n",
    "            pool_dataset.delete_samples(m_ids)\n",
    "\n",
    "            # Updating the variables\n",
    "            num_train_samples, train_classes = get_class_samples(train_dataset, progress = False)\n",
    "            num_pool_samples, pool_classes = get_class_samples(pool_dataset, progress = False)\n",
    "            \n",
    "            # Finding duplicates again\n",
    "            duplicate_ids = []\n",
    "            train_view = train_dataset.select(train_classes[clas])\n",
    "            if len(train_view) > 0:\n",
    "                results = fob.compute_similarity(train_view)\n",
    "                results.find_duplicates(thresh = threshold)\n",
    "                duplicate_ids = results.duplicate_ids\n",
    "            \n",
    "            iteration += 1\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c73987",
   "metadata": {
    "papermill": {
     "duration": 75.122638,
     "end_time": "2022-06-01T15:05:57.752208",
     "exception": false,
     "start_time": "2022-06-01T15:04:42.629570",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.5. Preparing the Dataset for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4218f343",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T15:08:27.361697Z",
     "iopub.status.busy": "2022-06-01T15:08:27.361234Z",
     "iopub.status.idle": "2022-06-01T15:08:47.761321Z",
     "shell.execute_reply": "2022-06-01T15:08:47.760590Z"
    },
    "papermill": {
     "duration": 95.570146,
     "end_time": "2022-06-01T15:08:47.763809",
     "exception": false,
     "start_time": "2022-06-01T15:07:12.193663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    /kaggle/input/cifar10/iter_samp/train_data/dat...\n",
       "1    /kaggle/input/cifar10/iter_samp/train_data/dat...\n",
       "2    /kaggle/input/cifar10/iter_samp/train_data/dat...\n",
       "3    /kaggle/input/cifar10/iter_samp/train_data/dat...\n",
       "4    /kaggle/input/cifar10/iter_samp/train_data/dat...\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepaths = []\n",
    "for sample in train_dataset.iter_samples():\n",
    "    filepaths.append(sample['filepath'])\n",
    "    \n",
    "filepaths = pd.Series(filepaths)\n",
    "filepaths.to_csv(\"./filepaths.csv\", index = False)\n",
    "filepaths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ba1a3b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T15:11:26.282902Z",
     "iopub.status.busy": "2022-06-01T15:11:26.282129Z",
     "iopub.status.idle": "2022-06-01T15:11:26.292830Z",
     "shell.execute_reply": "2022-06-01T15:11:26.291477Z"
    },
    "papermill": {
     "duration": 83.195403,
     "end_time": "2022-06-01T15:11:26.296621",
     "exception": false,
     "start_time": "2022-06-01T15:10:03.101218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40006, 32, 32, 3) (40006, 1)\n",
      "(20061, 32, 32, 3) (20061, 1)\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables to store the images and labels\n",
    "iter_samp_df = []\n",
    "iter_samp_y = []\n",
    "\n",
    "# # Loading the JSON files and extracting the labels\n",
    "# train_json = json.load(open(\"../input/cifar10/iter_samp/train_data/labels.json\"))\n",
    "# pool_json = json.load(open(\"../input/cifar10/iter_samp/pool_data/labels.json\"))\n",
    "# train_labels = train_json[\"labels\"]\n",
    "# pool_labels = pool_json[\"labels\"]\n",
    "\n",
    "print(df_train.shape, y_train.shape)\n",
    "print(df_pool.shape, y_pool.shape)\n",
    "print(type(df_train), type(df_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "520cb55e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T15:14:30.443988Z",
     "iopub.status.busy": "2022-06-01T15:14:30.443542Z",
     "iopub.status.idle": "2022-06-01T15:24:34.351324Z",
     "shell.execute_reply": "2022-06-01T15:24:34.349816Z"
    },
    "papermill": {
     "duration": 696.720407,
     "end_time": "2022-06-01T15:24:34.355552",
     "exception": false,
     "start_time": "2022-06-01T15:12:57.635145",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:12<00:00, 3991.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072) (50000,)\n"
     ]
    }
   ],
   "source": [
    "for file in tqdm(filepaths):\n",
    "    iden = file.split(\"/\")[-1].split(\".\")[0]\n",
    "    is_train = iden.split(\"_\")[-1]\n",
    "    index = int(iden.split(\"_\")[0])\n",
    "    # print(file, iden, index, type(index), is_train)\n",
    "    \n",
    "    if is_train == \"train\": \n",
    "        img = df_train[index]\n",
    "        label = y_train[index][0]\n",
    "    else: \n",
    "        img = df_pool[index]\n",
    "        label = y_pool[index][0]\n",
    "    img = np.reshape(img, (-1))\n",
    "    # print(label.shape, img.shape)\n",
    "\n",
    "    iter_samp_df.append(img)\n",
    "    iter_samp_y.append(label)\n",
    "    \n",
    "iter_samp_df = pd.DataFrame(iter_samp_df)\n",
    "iter_samp_y = pd.Series(iter_samp_y)\n",
    "print(iter_samp_df.shape, iter_samp_y.shape)\n",
    "\n",
    "# Saving the Images and the Labels\n",
    "iter_samp_df.to_csv(\"iter_samp_df.csv\", index = False)\n",
    "iter_samp_y.to_csv(\"iter_samp_y.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e5d36d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T15:27:04.867817Z",
     "iopub.status.busy": "2022-06-01T15:27:04.867312Z",
     "iopub.status.idle": "2022-06-01T15:27:13.606319Z",
     "shell.execute_reply": "2022-06-01T15:27:13.604230Z"
    },
    "papermill": {
     "duration": 83.966015,
     "end_time": "2022-06-01T15:27:13.610903",
     "exception": false,
     "start_time": "2022-06-01T15:25:49.644888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000,) (50000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the Dataset\n",
    "iter_samp_df = np.reshape(np.array(iter_samp_df), (-1, 32, 32, 3))\n",
    "\n",
    "# Creating a random permutation\n",
    "perm = np.random.permutation(iter_samp_df.shape[0])\n",
    "\n",
    "# Shuffling the training dataset\n",
    "iter_samp_df = iter_samp_df[perm, : , : , : ]\n",
    "iter_samp_y = iter_samp_y[perm]\n",
    "iter_samp_oh = tf.one_hot(np.ravel(iter_samp_y), depth = 10)\n",
    "print(iter_samp_df.shape, iter_samp_y.shape, iter_samp_oh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e73dbc2",
   "metadata": {
    "papermill": {
     "duration": 78.952012,
     "end_time": "2022-06-01T15:29:48.612427",
     "exception": false,
     "start_time": "2022-06-01T15:28:29.660415",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.6. Training the Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76086265",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T15:32:19.049945Z",
     "iopub.status.busy": "2022-06-01T15:32:19.049519Z",
     "iopub.status.idle": "2022-06-01T15:32:22.045081Z",
     "shell.execute_reply": "2022-06-01T15:32:22.044021Z"
    },
    "papermill": {
     "duration": 78.203484,
     "end_time": "2022-06-01T15:32:22.048030",
     "exception": false,
     "start_time": "2022-06-01T15:31:03.844546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing the Baseline Model Architecture\n",
    "copyfile(src = \"../input/dcai-rw/baseline_arch.py\", dst = \"../working/baseline_arch.py\")\n",
    "from baseline_arch import cnn_model\n",
    "\n",
    "# Creating Batches from the Augmented Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((iter_samp_df, iter_samp_oh)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47d30c40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T15:34:51.549461Z",
     "iopub.status.busy": "2022-06-01T15:34:51.549096Z",
     "iopub.status.idle": "2022-06-01T18:20:28.339615Z",
     "shell.execute_reply": "2022-06-01T18:20:28.338171Z"
    },
    "papermill": {
     "duration": 10011.244378,
     "end_time": "2022-06-01T18:20:28.342935",
     "exception": false,
     "start_time": "2022-06-01T15:33:37.098557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 48s 30ms/step - loss: 1.6859 - accuracy: 0.3903\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 1.2809 - accuracy: 0.5520\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 1.1111 - accuracy: 0.6171\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 1.0087 - accuracy: 0.6509\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.9377 - accuracy: 0.6766\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.8810 - accuracy: 0.6978\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.8366 - accuracy: 0.7125\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 0.7901 - accuracy: 0.7269\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 53s 34ms/step - loss: 0.7577 - accuracy: 0.7369\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 55s 35ms/step - loss: 0.7294 - accuracy: 0.7485\n",
      "For  10  Epochs:\n",
      "Log-loss for Train Dataset =  0.5414839492597625\n",
      "Log-loss for Test Dataset =  0.8328513454330656\n",
      "Accuracy for Train Dataset =  0.80594\n",
      "Accuracy for Test Dataset =  0.7303\n",
      "\n",
      "Epoch 1/20\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 1.6768 - accuracy: 0.3948\n",
      "Epoch 2/20\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 1.2672 - accuracy: 0.5556\n",
      "Epoch 3/20\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 1.1162 - accuracy: 0.6143\n",
      "Epoch 4/20\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 1.0229 - accuracy: 0.6487\n",
      "Epoch 5/20\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.9425 - accuracy: 0.6756\n",
      "Epoch 6/20\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.8819 - accuracy: 0.6955\n",
      "Epoch 7/20\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.8381 - accuracy: 0.7102\n",
      "Epoch 8/20\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.7890 - accuracy: 0.7243\n",
      "Epoch 9/20\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.7542 - accuracy: 0.7379\n",
      "Epoch 10/20\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.7212 - accuracy: 0.7511\n",
      "Epoch 11/20\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.6918 - accuracy: 0.7613\n",
      "Epoch 12/20\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.6646 - accuracy: 0.7694\n",
      "Epoch 13/20\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.6467 - accuracy: 0.7750\n",
      "Epoch 14/20\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.6258 - accuracy: 0.7826\n",
      "Epoch 15/20\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.6084 - accuracy: 0.7892\n",
      "Epoch 16/20\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.5925 - accuracy: 0.7952\n",
      "Epoch 17/20\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.5779 - accuracy: 0.7984\n",
      "Epoch 18/20\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.5595 - accuracy: 0.8054\n",
      "Epoch 19/20\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.5396 - accuracy: 0.8137\n",
      "Epoch 20/20\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 0.5316 - accuracy: 0.8148\n",
      "For  20  Epochs:\n",
      "Log-loss for Train Dataset =  0.32867434461801837\n",
      "Log-loss for Test Dataset =  0.8123821097538599\n",
      "Accuracy for Train Dataset =  0.8836\n",
      "Accuracy for Test Dataset =  0.749\n",
      "\n",
      "Epoch 1/30\n",
      "1563/1563 [==============================] - 52s 32ms/step - loss: 1.7179 - accuracy: 0.3799\n",
      "Epoch 2/30\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 1.3223 - accuracy: 0.5354\n",
      "Epoch 3/30\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 1.1425 - accuracy: 0.5995\n",
      "Epoch 4/30\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 1.0251 - accuracy: 0.6419\n",
      "Epoch 5/30\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.9474 - accuracy: 0.6725\n",
      "Epoch 6/30\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.8897 - accuracy: 0.6919\n",
      "Epoch 7/30\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.8363 - accuracy: 0.7111\n",
      "Epoch 8/30\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.7946 - accuracy: 0.7223\n",
      "Epoch 9/30\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.7673 - accuracy: 0.7338\n",
      "Epoch 10/30\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.7340 - accuracy: 0.7449\n",
      "Epoch 11/30\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 0.7009 - accuracy: 0.7564\n",
      "Epoch 12/30\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.6750 - accuracy: 0.7672\n",
      "Epoch 13/30\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.6579 - accuracy: 0.7695\n",
      "Epoch 14/30\n",
      "1563/1563 [==============================] - 48s 31ms/step - loss: 0.6313 - accuracy: 0.7799\n",
      "Epoch 15/30\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.6098 - accuracy: 0.7879\n",
      "Epoch 16/30\n",
      "1563/1563 [==============================] - 48s 31ms/step - loss: 0.5978 - accuracy: 0.7915\n",
      "Epoch 17/30\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.5804 - accuracy: 0.7999\n",
      "Epoch 18/30\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.5669 - accuracy: 0.8034\n",
      "Epoch 19/30\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.5591 - accuracy: 0.8053\n",
      "Epoch 20/30\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.5388 - accuracy: 0.8120\n",
      "Epoch 21/30\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.5306 - accuracy: 0.8162\n",
      "Epoch 22/30\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.5186 - accuracy: 0.8189\n",
      "Epoch 23/30\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.5103 - accuracy: 0.8222\n",
      "Epoch 24/30\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.5002 - accuracy: 0.8252\n",
      "Epoch 25/30\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 0.4901 - accuracy: 0.8300\n",
      "Epoch 26/30\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.4805 - accuracy: 0.8319\n",
      "Epoch 27/30\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 0.4692 - accuracy: 0.8378\n",
      "Epoch 28/30\n",
      "1563/1563 [==============================] - 48s 31ms/step - loss: 0.4697 - accuracy: 0.8369\n",
      "Epoch 29/30\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 0.4573 - accuracy: 0.8413\n",
      "Epoch 30/30\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.4562 - accuracy: 0.8436\n",
      "For  30  Epochs:\n",
      "Log-loss for Train Dataset =  0.1933080823406008\n",
      "Log-loss for Test Dataset =  0.7759166087952383\n",
      "Accuracy for Train Dataset =  0.93698\n",
      "Accuracy for Test Dataset =  0.7619\n",
      "\n",
      "Epoch 1/40\n",
      "1563/1563 [==============================] - 48s 30ms/step - loss: 1.6811 - accuracy: 0.3977\n",
      "Epoch 2/40\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 1.2786 - accuracy: 0.5503\n",
      "Epoch 3/40\n",
      "1563/1563 [==============================] - 48s 31ms/step - loss: 1.1220 - accuracy: 0.6083\n",
      "Epoch 4/40\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 1.0236 - accuracy: 0.6482\n",
      "Epoch 5/40\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.9467 - accuracy: 0.6736\n",
      "Epoch 6/40\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.8901 - accuracy: 0.6931\n",
      "Epoch 7/40\n",
      "1563/1563 [==============================] - 54s 35ms/step - loss: 0.8385 - accuracy: 0.7104\n",
      "Epoch 8/40\n",
      "1563/1563 [==============================] - 57s 36ms/step - loss: 0.7945 - accuracy: 0.7237\n",
      "Epoch 9/40\n",
      "1563/1563 [==============================] - 57s 36ms/step - loss: 0.7597 - accuracy: 0.7372\n",
      "Epoch 10/40\n",
      "1563/1563 [==============================] - 59s 38ms/step - loss: 0.7264 - accuracy: 0.7479\n",
      "Epoch 11/40\n",
      "1563/1563 [==============================] - 54s 35ms/step - loss: 0.6995 - accuracy: 0.7576\n",
      "Epoch 12/40\n",
      "1563/1563 [==============================] - 55s 35ms/step - loss: 0.6777 - accuracy: 0.7650\n",
      "Epoch 13/40\n",
      "1563/1563 [==============================] - 55s 35ms/step - loss: 0.6543 - accuracy: 0.7738\n",
      "Epoch 14/40\n",
      "1563/1563 [==============================] - 53s 34ms/step - loss: 0.6299 - accuracy: 0.7804\n",
      "Epoch 15/40\n",
      "1563/1563 [==============================] - 59s 38ms/step - loss: 0.6129 - accuracy: 0.7892\n",
      "Epoch 16/40\n",
      "1563/1563 [==============================] - 59s 38ms/step - loss: 0.5944 - accuracy: 0.7936\n",
      "Epoch 17/40\n",
      "1563/1563 [==============================] - 58s 37ms/step - loss: 0.5805 - accuracy: 0.7983\n",
      "Epoch 18/40\n",
      "1563/1563 [==============================] - 56s 36ms/step - loss: 0.5656 - accuracy: 0.8049\n",
      "Epoch 19/40\n",
      "1563/1563 [==============================] - 53s 34ms/step - loss: 0.5465 - accuracy: 0.8112\n",
      "Epoch 20/40\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.5384 - accuracy: 0.8125\n",
      "Epoch 21/40\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.5207 - accuracy: 0.8181\n",
      "Epoch 22/40\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.5097 - accuracy: 0.8237\n",
      "Epoch 23/40\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.5040 - accuracy: 0.8262\n",
      "Epoch 24/40\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.4929 - accuracy: 0.8274\n",
      "Epoch 25/40\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.4851 - accuracy: 0.8318\n",
      "Epoch 26/40\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.4678 - accuracy: 0.8387\n",
      "Epoch 27/40\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.4654 - accuracy: 0.8378\n",
      "Epoch 28/40\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.4552 - accuracy: 0.8424\n",
      "Epoch 29/40\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.4496 - accuracy: 0.8412\n",
      "Epoch 30/40\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.4453 - accuracy: 0.8453\n",
      "Epoch 31/40\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.4397 - accuracy: 0.8481\n",
      "Epoch 32/40\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.4298 - accuracy: 0.8522\n",
      "Epoch 33/40\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.4235 - accuracy: 0.8529\n",
      "Epoch 34/40\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.4176 - accuracy: 0.8565\n",
      "Epoch 35/40\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.4160 - accuracy: 0.8568\n",
      "Epoch 36/40\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.4069 - accuracy: 0.8591\n",
      "Epoch 37/40\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.4007 - accuracy: 0.8608\n",
      "Epoch 38/40\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.3985 - accuracy: 0.8632\n",
      "Epoch 39/40\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.3979 - accuracy: 0.8622\n",
      "Epoch 40/40\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.3827 - accuracy: 0.8677\n",
      "For  40  Epochs:\n",
      "Log-loss for Train Dataset =  0.13784130474160022\n",
      "Log-loss for Test Dataset =  0.7676983762374725\n",
      "Accuracy for Train Dataset =  0.96036\n",
      "Accuracy for Test Dataset =  0.7649\n",
      "\n",
      "Epoch 1/50\n",
      "1563/1563 [==============================] - 47s 29ms/step - loss: 1.6732 - accuracy: 0.4025\n",
      "Epoch 2/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 1.2561 - accuracy: 0.5623\n",
      "Epoch 3/50\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 1.0979 - accuracy: 0.6185\n",
      "Epoch 4/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 1.0031 - accuracy: 0.6541\n",
      "Epoch 5/50\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.9280 - accuracy: 0.6801\n",
      "Epoch 6/50\n",
      "1563/1563 [==============================] - 48s 30ms/step - loss: 0.8799 - accuracy: 0.7011\n",
      "Epoch 7/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.8312 - accuracy: 0.7144\n",
      "Epoch 8/50\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.7891 - accuracy: 0.7319\n",
      "Epoch 9/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.7574 - accuracy: 0.7387\n",
      "Epoch 10/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.7278 - accuracy: 0.7478\n",
      "Epoch 11/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.6994 - accuracy: 0.7560\n",
      "Epoch 12/50\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.6799 - accuracy: 0.7652\n",
      "Epoch 13/50\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.6559 - accuracy: 0.7735\n",
      "Epoch 14/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.6350 - accuracy: 0.7819\n",
      "Epoch 15/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.6142 - accuracy: 0.7883\n",
      "Epoch 16/50\n",
      "1563/1563 [==============================] - 48s 30ms/step - loss: 0.5997 - accuracy: 0.7911\n",
      "Epoch 17/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.5864 - accuracy: 0.7974\n",
      "Epoch 18/50\n",
      "1563/1563 [==============================] - 48s 31ms/step - loss: 0.5698 - accuracy: 0.8035\n",
      "Epoch 19/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.5636 - accuracy: 0.8054\n",
      "Epoch 20/50\n",
      "1563/1563 [==============================] - 48s 31ms/step - loss: 0.5447 - accuracy: 0.8113\n",
      "Epoch 21/50\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 0.5292 - accuracy: 0.8160\n",
      "Epoch 22/50\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 0.5156 - accuracy: 0.8223\n",
      "Epoch 23/50\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 0.5110 - accuracy: 0.8244\n",
      "Epoch 24/50\n",
      "1563/1563 [==============================] - 51s 32ms/step - loss: 0.4991 - accuracy: 0.8281\n",
      "Epoch 25/50\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.4894 - accuracy: 0.8317\n",
      "Epoch 26/50\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 0.4808 - accuracy: 0.8330\n",
      "Epoch 27/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.4707 - accuracy: 0.8383\n",
      "Epoch 28/50\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.4639 - accuracy: 0.8395\n",
      "Epoch 29/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.4547 - accuracy: 0.8417\n",
      "Epoch 30/50\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.4510 - accuracy: 0.8449\n",
      "Epoch 31/50\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.4454 - accuracy: 0.8446\n",
      "Epoch 32/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.4384 - accuracy: 0.8483\n",
      "Epoch 33/50\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.4297 - accuracy: 0.8522\n",
      "Epoch 34/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.4268 - accuracy: 0.8521\n",
      "Epoch 35/50\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.4228 - accuracy: 0.8531\n",
      "Epoch 36/50\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.4148 - accuracy: 0.8571\n",
      "Epoch 37/50\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.4154 - accuracy: 0.8577\n",
      "Epoch 38/50\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.4057 - accuracy: 0.8613\n",
      "Epoch 39/50\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.3999 - accuracy: 0.8627\n",
      "Epoch 40/50\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.3900 - accuracy: 0.8653\n",
      "Epoch 41/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.3913 - accuracy: 0.8656\n",
      "Epoch 42/50\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.3854 - accuracy: 0.8681\n",
      "Epoch 43/50\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.3786 - accuracy: 0.8699\n",
      "Epoch 44/50\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.3842 - accuracy: 0.8695\n",
      "Epoch 45/50\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.3802 - accuracy: 0.8705\n",
      "Epoch 46/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.3701 - accuracy: 0.8745\n",
      "Epoch 47/50\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.3694 - accuracy: 0.8741\n",
      "Epoch 48/50\n",
      "1563/1563 [==============================] - 48s 31ms/step - loss: 0.3657 - accuracy: 0.8757\n",
      "Epoch 49/50\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.3596 - accuracy: 0.8781\n",
      "Epoch 50/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.3577 - accuracy: 0.8766\n",
      "For  50  Epochs:\n",
      "Log-loss for Train Dataset =  0.09194198950579427\n",
      "Log-loss for Test Dataset =  0.7480772982667389\n",
      "Accuracy for Train Dataset =  0.97742\n",
      "Accuracy for Test Dataset =  0.7797\n",
      "\n",
      "Epoch 1/50\n",
      "1563/1563 [==============================] - 48s 30ms/step - loss: 1.7581 - accuracy: 0.3626\n",
      "Epoch 2/50\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 1.3271 - accuracy: 0.5306\n",
      "Epoch 3/50\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 1.1382 - accuracy: 0.6029\n",
      "Epoch 4/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 1.0255 - accuracy: 0.6430\n",
      "Epoch 5/50\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.9505 - accuracy: 0.6689\n",
      "Epoch 6/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.8918 - accuracy: 0.6917\n",
      "Epoch 7/50\n",
      "1563/1563 [==============================] - 48s 31ms/step - loss: 0.8447 - accuracy: 0.7087\n",
      "Epoch 8/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.7971 - accuracy: 0.7245\n",
      "Epoch 9/50\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.7698 - accuracy: 0.7334\n",
      "Epoch 10/50\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.7353 - accuracy: 0.7464\n",
      "Epoch 11/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.7081 - accuracy: 0.7536\n",
      "Epoch 12/50\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.6907 - accuracy: 0.7614\n",
      "Epoch 13/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.6637 - accuracy: 0.7716\n",
      "Epoch 14/50\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.6412 - accuracy: 0.7772\n",
      "Epoch 15/50\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.6221 - accuracy: 0.7863\n",
      "Epoch 16/50\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.6070 - accuracy: 0.7906\n",
      "Epoch 17/50\n",
      "1563/1563 [==============================] - 51s 32ms/step - loss: 0.5837 - accuracy: 0.7966\n",
      "Epoch 18/50\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 0.5699 - accuracy: 0.8015\n",
      "Epoch 19/50\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 0.5601 - accuracy: 0.8053\n",
      "Epoch 20/50\n",
      "1563/1563 [==============================] - 51s 32ms/step - loss: 0.5411 - accuracy: 0.8132\n",
      "Epoch 21/50\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.5316 - accuracy: 0.8165\n",
      "Epoch 22/50\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.5175 - accuracy: 0.8206\n",
      "Epoch 23/50\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.5110 - accuracy: 0.8229\n",
      "Epoch 24/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.5066 - accuracy: 0.8236\n",
      "Epoch 25/50\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.4872 - accuracy: 0.8306\n",
      "Epoch 26/50\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.4820 - accuracy: 0.8328\n",
      "Epoch 27/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.4743 - accuracy: 0.8346\n",
      "Epoch 28/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.4653 - accuracy: 0.8398\n",
      "Epoch 29/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.4566 - accuracy: 0.8423\n",
      "Epoch 30/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.4507 - accuracy: 0.8427\n",
      "Epoch 31/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.4448 - accuracy: 0.8477\n",
      "Epoch 32/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.4348 - accuracy: 0.8499\n",
      "Epoch 33/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.4287 - accuracy: 0.8531\n",
      "Epoch 34/50\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.4295 - accuracy: 0.8529\n",
      "Epoch 35/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.4230 - accuracy: 0.8557\n",
      "Epoch 36/50\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.4083 - accuracy: 0.8592\n",
      "Epoch 37/50\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.4095 - accuracy: 0.8595\n",
      "Epoch 38/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.4054 - accuracy: 0.8626\n",
      "Epoch 39/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.4023 - accuracy: 0.8620\n",
      "Epoch 40/50\n",
      "1563/1563 [==============================] - 48s 30ms/step - loss: 0.3921 - accuracy: 0.8642\n",
      "Epoch 41/50\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.3960 - accuracy: 0.8654\n",
      "Epoch 42/50\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.3904 - accuracy: 0.8654\n",
      "Epoch 43/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.3853 - accuracy: 0.8686\n",
      "Epoch 44/50\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.3767 - accuracy: 0.8724\n",
      "Epoch 45/50\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.3757 - accuracy: 0.8710\n",
      "Epoch 46/50\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.3770 - accuracy: 0.8715\n",
      "Epoch 47/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.3766 - accuracy: 0.8725\n",
      "Epoch 48/50\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.3692 - accuracy: 0.8717\n",
      "Epoch 49/50\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.3540 - accuracy: 0.8784\n",
      "Epoch 50/50\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.3582 - accuracy: 0.8777\n"
     ]
    }
   ],
   "source": [
    "# If the model has been pre-trained\n",
    "try:\n",
    "    conv_model = cnn_model((32, 32, 3))\n",
    "    conv_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "    conv_model.load_weights(\"../input/dcai-rw/itersamp_25per_trad_gan.h\")\n",
    "\n",
    "# If the model hasn't been pre-trained\n",
    "except:\n",
    "    num_epochs = [10, 20, 30, 40, 50]\n",
    "    train_loss, test_loss, train_acc, test_acc = [], [], [], []\n",
    "\n",
    "    for epochs in num_epochs:\n",
    "        # Training the Model\n",
    "        conv_model = cnn_model((32, 32, 3))\n",
    "        conv_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "        conv_model.fit(train_dataset, epochs = epochs)\n",
    "\n",
    "        # Predicting on the Train/Test Datasets\n",
    "        preds_train = conv_model.predict(iter_samp_df)\n",
    "        preds_test = conv_model.predict(df_test)\n",
    "\n",
    "        # Finding the Predicted Classes\n",
    "        cls_train = np.argmax(preds_train, axis = 1)\n",
    "        cls_test = np.argmax(preds_test, axis = 1)\n",
    "\n",
    "        # Finding the Train/Test set Loss\n",
    "        train_loss.append(log_loss(iter_samp_oh, preds_train))\n",
    "        test_loss.append(log_loss(y_test_oh, preds_test))\n",
    "        train_acc.append(accuracy_score(iter_samp_y, cls_train))\n",
    "        test_acc.append(accuracy_score(y_test, cls_test))\n",
    "\n",
    "        print(\"For \", epochs, \" Epochs:\")\n",
    "        print(\"Log-loss for Train Dataset = \", train_loss[-1])\n",
    "        print(\"Log-loss for Test Dataset = \", test_loss[-1])\n",
    "        print(\"Accuracy for Train Dataset = \", train_acc[-1])\n",
    "        print(\"Accuracy for Test Dataset = \", test_acc[-1])\n",
    "        print()\n",
    "\n",
    "    # Training the Model with the best hyper-parameter settings\n",
    "    ind = np.argmax(test_acc)\n",
    "    best_num_epochs = num_epochs[ind]\n",
    "    conv_model = cnn_model((32, 32, 3))\n",
    "    conv_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "    conv_model.fit(train_dataset, epochs = best_num_epochs)\n",
    "\n",
    "    # Saving the model along with it's weights\n",
    "    conv_model.save('itersamp_25per_trad_gan.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d178a12",
   "metadata": {
    "papermill": {
     "duration": 136.10466,
     "end_time": "2022-06-01T18:25:00.894418",
     "exception": false,
     "start_time": "2022-06-01T18:22:44.789758",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.7. Predicting the Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f695cef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T18:29:37.038102Z",
     "iopub.status.busy": "2022-06-01T18:29:37.036834Z",
     "iopub.status.idle": "2022-06-01T18:29:52.535691Z",
     "shell.execute_reply": "2022-06-01T18:29:52.534741Z"
    },
    "papermill": {
     "duration": 151.368982,
     "end_time": "2022-06-01T18:29:52.537991",
     "exception": false,
     "start_time": "2022-06-01T18:27:21.169009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10)\n",
      "Log-loss for Train Dataset =  0.09436428350136501\n",
      "Log-loss for Test Dataset =  0.7759590346173721\n",
      "Weighted F1 Score for Train Dataset =  0.9758555895807672\n",
      "Weighted F1 Score for Test Dataset =  0.7699744204844067\n",
      "Accuracy for Train Dataset =  0.97592\n",
      "Accuracy for Test Dataset =  0.7727\n"
     ]
    }
   ],
   "source": [
    "# Predicting on the Train/Test Datasets\n",
    "preds_train = conv_model.predict(iter_samp_df)\n",
    "preds_test = conv_model.predict(df_test)\n",
    "print(preds_train.shape)\n",
    "\n",
    "# Finding the Predicted Classes\n",
    "cls_train = np.argmax(preds_train, axis = 1)\n",
    "cls_test = np.argmax(preds_test, axis = 1)\n",
    "\n",
    "# Finding the Train/Test set Loss\n",
    "print(\"Log-loss for Train Dataset = \", log_loss(iter_samp_oh, preds_train))\n",
    "print(\"Log-loss for Test Dataset = \", log_loss(y_test_oh, preds_test))\n",
    "print(\"Weighted F1 Score for Train Dataset = \", f1_score(iter_samp_y, cls_train, average = 'weighted'))\n",
    "print(\"Weighted F1 Score for Test Dataset = \", f1_score(y_test, cls_test, average = 'weighted'))\n",
    "print(\"Accuracy for Train Dataset = \", accuracy_score(iter_samp_y, cls_train))\n",
    "print(\"Accuracy for Test Dataset = \", accuracy_score(y_test, cls_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbb232d",
   "metadata": {
    "papermill": {
     "duration": 135.742352,
     "end_time": "2022-06-01T18:34:23.828743",
     "exception": false,
     "start_time": "2022-06-01T18:32:08.086391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Reference: The effect of modifying Voxel51 dataset on the Voxel51 dataset's view\n",
    "- We can't add the samples directly using the list of IDs for the corresponding samples.\n",
    "- First, we have to select the samples using the list of IDs, and then only the samples will be added.\n",
    "- Another thing to keep in mind is that once the samples have been added, the indices need to be sorted and listed again, and the view needs to be re-initialized.\n",
    "\n",
    "#### Output from the below code cell:\n",
    "` # Samples in train_view before deleting any samples: 4022` <br>\n",
    "`# Samples in pool_view for the corresponding class: 4083` <br>\n",
    "`# Samples in train_view after deleting 1 sample: 4021` <br>\n",
    "`100% |███████████████| 4083/4083 [5.3s elapsed, 0s remaining, 840.8 samples/s]` <br>\n",
    "`# Samples in train_view after adding n samples: 4021` <br>\n",
    "`# Samples in train_view after recalculating the IDs and reinitializing the view: 8104` <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da1b6d33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T18:38:56.356268Z",
     "iopub.status.busy": "2022-06-01T18:38:56.355884Z",
     "iopub.status.idle": "2022-06-01T18:38:56.361465Z",
     "shell.execute_reply": "2022-06-01T18:38:56.360466Z"
    },
    "papermill": {
     "duration": 136.451897,
     "end_time": "2022-06-01T18:38:56.363806",
     "exception": false,
     "start_time": "2022-06-01T18:36:39.911909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# _ , train_classes = get_class_samples(train_dataset, progress = False)\n",
    "# _ , pool_classes = get_class_samples(pool_dataset, progress = False)\n",
    "# train_view = train_dataset.select(train_classes[2])\n",
    "# pool_view = pool_dataset.select(pool_classes[2])\n",
    "# print(f\"# Samples in train_view before deleting any samples: {len(train_view)}\")\n",
    "# print(f\"# Samples in pool_view for the corresponding class: {len(pool_view)}\")\n",
    "\n",
    "# # Extracting the IDs of samples in train_view\n",
    "# ids = []\n",
    "# for sample in train_view.iter_samples():\n",
    "#     voxel_id = sample['id']\n",
    "#     ids.append(voxel_id)\n",
    "\n",
    "# # Deleting a single example from train_dataset\n",
    "# train_dataset.delete_samples([ids[0]])\n",
    "# print(f\"# Samples in train_view after deleting 1 sample: {len(train_view)}\")\n",
    "\n",
    "# # Adding examples to train_dataset\n",
    "# train_dataset.add_samples(pool_view)\n",
    "# print(f\"# Samples in train_view after adding `n` samples: {len(train_view)}\")\n",
    "\n",
    "# _ , train_classes = get_class_samples(train_dataset, progress = False)\n",
    "# train_view = train_dataset.select(train_classes[2])\n",
    "# print(f\"# Samples in train_view after recalculating the IDs and reinitializing the view: {len(train_view)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26136.242137,
   "end_time": "2022-06-01T18:41:15.430822",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-01T11:25:39.188685",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
