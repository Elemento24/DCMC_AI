{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3b3df3d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.018966,
     "end_time": "2022-04-09T15:22:36.766254",
     "exception": false,
     "start_time": "2022-04-09T15:22:36.747288",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Xception\n",
    "- CIFAR10 dataset is used to train the pre-trained model **Xception**. \n",
    "- Only the labelled dataset is used in this model.\n",
    "- Fine-tuning of the model is also done, through freezing the layer, adding new layers to the model etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc52772c",
   "metadata": {
    "papermill": {
     "duration": 0.017721,
     "end_time": "2022-04-09T15:22:36.802257",
     "exception": false,
     "start_time": "2022-04-09T15:22:36.784536",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importing the Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abddf216",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T15:22:36.850790Z",
     "iopub.status.busy": "2022-04-09T15:22:36.850001Z",
     "iopub.status.idle": "2022-04-09T15:22:43.158614Z",
     "shell.execute_reply": "2022-04-09T15:22:43.159239Z",
     "shell.execute_reply.started": "2022-04-09T10:20:37.742129Z"
    },
    "papermill": {
     "duration": 6.339083,
     "end_time": "2022-04-09T15:22:43.159522",
     "exception": false,
     "start_time": "2022-04-09T15:22:36.820439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "# https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/274717\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cef1cc24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T15:22:45.288007Z",
     "iopub.status.busy": "2022-04-09T15:22:43.202258Z",
     "iopub.status.idle": "2022-04-09T15:22:45.291808Z",
     "shell.execute_reply": "2022-04-09T15:22:45.292578Z",
     "shell.execute_reply.started": "2022-04-09T10:20:43.65026Z"
    },
    "papermill": {
     "duration": 2.114846,
     "end_time": "2022-04-09T15:22:45.292802",
     "exception": false,
     "start_time": "2022-04-09T15:22:43.177956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Making sure that Tensorflow is able to detect the GPU\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if \"GPU\" not in device_name:\n",
    "    print(\"GPU device not found\")\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13f4935c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T15:22:45.341818Z",
     "iopub.status.busy": "2022-04-09T15:22:45.340855Z",
     "iopub.status.idle": "2022-04-09T15:22:45.343459Z",
     "shell.execute_reply": "2022-04-09T15:22:45.343030Z",
     "shell.execute_reply.started": "2022-04-09T10:20:45.832925Z"
    },
    "papermill": {
     "duration": 0.029592,
     "end_time": "2022-04-09T15:22:45.343579",
     "exception": false,
     "start_time": "2022-04-09T15:22:45.313987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# These are the usual ipython objects\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Defining a function to list the memory consumed\n",
    "# Only outputs variables taking at least 1MB space\n",
    "def list_storage(inp_dir):\n",
    "    # Get a sorted list of the objects and their sizes\n",
    "    vars_defined = [x for x in inp_dir if not x.startswith('_') and x not in sys.modules and x not in ipython_vars]\n",
    "    sto = sorted([(x, sys.getsizeof(globals().get(x))) for x in vars_defined], key=lambda x: x[1], reverse=True)\n",
    "    sto = [(x[0], str(round((x[1] / 2**20), 2)) + ' MB') for x in sto if x[1] >= 2**20]\n",
    "    print(tabulate(sto, headers = ['Variable', 'Storage (in MB)']))\n",
    "\n",
    "# In order to use this function, use the below line of code\n",
    "# list_storage(dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "629bd7d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T15:22:45.388102Z",
     "iopub.status.busy": "2022-04-09T15:22:45.387514Z",
     "iopub.status.idle": "2022-04-09T15:23:13.313574Z",
     "shell.execute_reply": "2022-04-09T15:23:13.313106Z",
     "shell.execute_reply.started": "2022-04-09T10:21:12.852724Z"
    },
    "papermill": {
     "duration": 27.95053,
     "end_time": "2022-04-09T15:23:13.313741",
     "exception": false,
     "start_time": "2022-04-09T15:22:45.363211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing the Labelled Dataset\n",
    "df_train = pd.read_csv(\"../input/cifar10/train_lab_x.csv\")\n",
    "y_train = pd.read_csv(\"../input/cifar10/train_lab_y.csv\")\n",
    "\n",
    "# Importing the Test Dataset\n",
    "df_test = pd.read_csv(\"../input/cifar10/test_x.csv\")\n",
    "y_test = pd.read_csv(\"../input/cifar10/test_y.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1813d2",
   "metadata": {
    "papermill": {
     "duration": 0.018479,
     "end_time": "2022-04-09T15:23:13.352764",
     "exception": false,
     "start_time": "2022-04-09T15:23:13.334285",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Basic Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5a0bb3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T15:23:13.397588Z",
     "iopub.status.busy": "2022-04-09T15:23:13.396332Z",
     "iopub.status.idle": "2022-04-09T15:23:13.981218Z",
     "shell.execute_reply": "2022-04-09T15:23:13.982264Z",
     "shell.execute_reply.started": "2022-04-09T10:21:33.894782Z"
    },
    "papermill": {
     "duration": 0.611477,
     "end_time": "2022-04-09T15:23:13.982566",
     "exception": false,
     "start_time": "2022-04-09T15:23:13.371089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40006, 3072) (40006, 1)\n",
      "(40006, 3, 32, 32)\n",
      "[0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 108x108 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = np.array(df_train)\n",
    "y_train = np.array(y_train)\n",
    "print(df_train.shape, y_train.shape)\n",
    "\n",
    "# Reshaping the dataset\n",
    "df_train = np.reshape(df_train, (-1, 3, 32, 32))\n",
    "print(df_train.shape)\n",
    "\n",
    "# Visualizing a single image\n",
    "ind = 15\n",
    "example = df_train[ind, : , : , : ]\n",
    "example = example.transpose((1, 2, 0))\n",
    "plt.figure(figsize=(1.5, 1.5))\n",
    "plt.imshow(example)\n",
    "print(y_train[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "749ae3de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T15:23:14.075729Z",
     "iopub.status.busy": "2022-04-09T15:23:14.074746Z",
     "iopub.status.idle": "2022-04-09T15:23:16.639782Z",
     "shell.execute_reply": "2022-04-09T15:23:16.640508Z",
     "shell.execute_reply.started": "2022-04-09T10:21:34.521024Z"
    },
    "papermill": {
     "duration": 2.61301,
     "end_time": "2022-04-09T15:23:16.640716",
     "exception": false,
     "start_time": "2022-04-09T15:23:14.027706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40006, 32, 32, 3) (40006, 10)\n"
     ]
    }
   ],
   "source": [
    "# Creating a random permutation\n",
    "perm = np.random.permutation(df_train.shape[0])\n",
    "\n",
    "# Shuffling the training dataset\n",
    "df_train = df_train[perm, : , : , : ]\n",
    "y_train = y_train[perm]\n",
    "\n",
    "# Reshaping, rescaling and one-hot encoding\n",
    "df_train = np.transpose(np.array(df_train), (0, 2, 3, 1))\n",
    "df_train = df_train / 255\n",
    "y_train_oh = tf.one_hot(np.ravel(y_train), depth = 10)\n",
    "\n",
    "print(df_train.shape, y_train_oh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20724234",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T15:23:16.689567Z",
     "iopub.status.busy": "2022-04-09T15:23:16.688355Z",
     "iopub.status.idle": "2022-04-09T15:23:16.969502Z",
     "shell.execute_reply": "2022-04-09T15:23:16.970096Z",
     "shell.execute_reply.started": "2022-04-09T10:21:36.994275Z"
    },
    "papermill": {
     "duration": 0.308348,
     "end_time": "2022-04-09T15:23:16.970290",
     "exception": false,
     "start_time": "2022-04-09T15:23:16.661942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3072) (10000, 1)\n",
      "(10000, 3, 32, 32)\n",
      "(10000, 32, 32, 3) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "df_test = np.array(df_test)\n",
    "y_test = np.array(y_test)\n",
    "print(df_test.shape, y_test.shape)\n",
    "\n",
    "# Reshaping the dataset\n",
    "df_test = np.reshape(df_test, (-1, 3, 32, 32))\n",
    "print(df_test.shape)\n",
    "\n",
    "# Reshaping, rescaling and one-hot encoding\n",
    "df_test = np.transpose(np.array(df_test), (0, 2, 3, 1))\n",
    "df_test = df_test / 255\n",
    "y_test_oh = tf.one_hot(np.ravel(y_test), depth = 10)\n",
    "print(df_test.shape, y_test_oh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbf19cc",
   "metadata": {
    "papermill": {
     "duration": 0.020093,
     "end_time": "2022-04-09T15:23:17.011789",
     "exception": false,
     "start_time": "2022-04-09T15:23:16.991696",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccbc6c87",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-04-09T15:23:17.058966Z",
     "iopub.status.busy": "2022-04-09T15:23:17.058137Z",
     "iopub.status.idle": "2022-04-09T15:23:18.737186Z",
     "shell.execute_reply": "2022-04-09T15:23:18.736550Z",
     "shell.execute_reply.started": "2022-04-09T10:24:02.720878Z"
    },
    "papermill": {
     "duration": 1.704632,
     "end_time": "2022-04-09T15:23:18.737322",
     "exception": false,
     "start_time": "2022-04-09T15:23:17.032690",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "83689472/83683744 [==============================] - 0s 0us/step\n",
      "83697664/83683744 [==============================] - 0s 0us/step\n",
      "Model: \"xception\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 31, 31, 32)   864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_bn (BatchNormaliza (None, 31, 31, 32)   128         block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_act (Activation)   (None, 31, 31, 32)   0           block1_conv1_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 29, 29, 64)   18432       block1_conv1_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_bn (BatchNormaliza (None, 29, 29, 64)   256         block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_act (Activation)   (None, 29, 29, 64)   0           block1_conv2_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1 (SeparableConv2 (None, 29, 29, 128)  8768        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1_bn (BatchNormal (None, 29, 29, 128)  512         block2_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_act (Activation (None, 29, 29, 128)  0           block2_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2 (SeparableConv2 (None, 29, 29, 128)  17536       block2_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_bn (BatchNormal (None, 29, 29, 128)  512         block2_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 15, 15, 128)  8192        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 15, 15, 128)  0           block2_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 15, 15, 128)  512         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 15, 15, 128)  0           block2_pool[0][0]                \n",
      "                                                                 batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_act (Activation (None, 15, 15, 128)  0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1 (SeparableConv2 (None, 15, 15, 256)  33920       block3_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_bn (BatchNormal (None, 15, 15, 256)  1024        block3_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_act (Activation (None, 15, 15, 256)  0           block3_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2 (SeparableConv2 (None, 15, 15, 256)  67840       block3_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_bn (BatchNormal (None, 15, 15, 256)  1024        block3_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 8, 8, 256)    32768       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 8, 8, 256)    0           block3_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 8, 8, 256)    1024        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 8, 8, 256)    0           block3_pool[0][0]                \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_act (Activation (None, 8, 8, 256)    0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1 (SeparableConv2 (None, 8, 8, 728)    188672      block4_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_bn (BatchNormal (None, 8, 8, 728)    2912        block4_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_act (Activation (None, 8, 8, 728)    0           block4_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2 (SeparableConv2 (None, 8, 8, 728)    536536      block4_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_bn (BatchNormal (None, 8, 8, 728)    2912        block4_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 4, 4, 728)    186368      add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 4, 4, 728)    0           block4_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 4, 4, 728)    2912        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 4, 4, 728)    0           block4_pool[0][0]                \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_act (Activation (None, 4, 4, 728)    0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1 (SeparableConv2 (None, 4, 4, 728)    536536      block5_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_bn (BatchNormal (None, 4, 4, 728)    2912        block5_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_act (Activation (None, 4, 4, 728)    0           block5_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2 (SeparableConv2 (None, 4, 4, 728)    536536      block5_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_bn (BatchNormal (None, 4, 4, 728)    2912        block5_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_act (Activation (None, 4, 4, 728)    0           block5_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3 (SeparableConv2 (None, 4, 4, 728)    536536      block5_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_bn (BatchNormal (None, 4, 4, 728)    2912        block5_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 4, 4, 728)    0           block5_sepconv3_bn[0][0]         \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_act (Activation (None, 4, 4, 728)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1 (SeparableConv2 (None, 4, 4, 728)    536536      block6_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_bn (BatchNormal (None, 4, 4, 728)    2912        block6_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_act (Activation (None, 4, 4, 728)    0           block6_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2 (SeparableConv2 (None, 4, 4, 728)    536536      block6_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_bn (BatchNormal (None, 4, 4, 728)    2912        block6_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_act (Activation (None, 4, 4, 728)    0           block6_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3 (SeparableConv2 (None, 4, 4, 728)    536536      block6_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_bn (BatchNormal (None, 4, 4, 728)    2912        block6_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 4, 4, 728)    0           block6_sepconv3_bn[0][0]         \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_act (Activation (None, 4, 4, 728)    0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1 (SeparableConv2 (None, 4, 4, 728)    536536      block7_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_bn (BatchNormal (None, 4, 4, 728)    2912        block7_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_act (Activation (None, 4, 4, 728)    0           block7_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2 (SeparableConv2 (None, 4, 4, 728)    536536      block7_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_bn (BatchNormal (None, 4, 4, 728)    2912        block7_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_act (Activation (None, 4, 4, 728)    0           block7_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3 (SeparableConv2 (None, 4, 4, 728)    536536      block7_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_bn (BatchNormal (None, 4, 4, 728)    2912        block7_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 4, 4, 728)    0           block7_sepconv3_bn[0][0]         \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_act (Activation (None, 4, 4, 728)    0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1 (SeparableConv2 (None, 4, 4, 728)    536536      block8_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_bn (BatchNormal (None, 4, 4, 728)    2912        block8_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_act (Activation (None, 4, 4, 728)    0           block8_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2 (SeparableConv2 (None, 4, 4, 728)    536536      block8_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_bn (BatchNormal (None, 4, 4, 728)    2912        block8_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_act (Activation (None, 4, 4, 728)    0           block8_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3 (SeparableConv2 (None, 4, 4, 728)    536536      block8_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_bn (BatchNormal (None, 4, 4, 728)    2912        block8_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 4, 4, 728)    0           block8_sepconv3_bn[0][0]         \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_act (Activation (None, 4, 4, 728)    0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1 (SeparableConv2 (None, 4, 4, 728)    536536      block9_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_bn (BatchNormal (None, 4, 4, 728)    2912        block9_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_act (Activation (None, 4, 4, 728)    0           block9_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2 (SeparableConv2 (None, 4, 4, 728)    536536      block9_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_bn (BatchNormal (None, 4, 4, 728)    2912        block9_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_act (Activation (None, 4, 4, 728)    0           block9_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3 (SeparableConv2 (None, 4, 4, 728)    536536      block9_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_bn (BatchNormal (None, 4, 4, 728)    2912        block9_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 4, 4, 728)    0           block9_sepconv3_bn[0][0]         \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_act (Activatio (None, 4, 4, 728)    0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1 (SeparableConv (None, 4, 4, 728)    536536      block10_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_bn (BatchNorma (None, 4, 4, 728)    2912        block10_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_act (Activatio (None, 4, 4, 728)    0           block10_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2 (SeparableConv (None, 4, 4, 728)    536536      block10_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_bn (BatchNorma (None, 4, 4, 728)    2912        block10_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_act (Activatio (None, 4, 4, 728)    0           block10_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3 (SeparableConv (None, 4, 4, 728)    536536      block10_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_bn (BatchNorma (None, 4, 4, 728)    2912        block10_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 4, 4, 728)    0           block10_sepconv3_bn[0][0]        \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_act (Activatio (None, 4, 4, 728)    0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1 (SeparableConv (None, 4, 4, 728)    536536      block11_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_bn (BatchNorma (None, 4, 4, 728)    2912        block11_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_act (Activatio (None, 4, 4, 728)    0           block11_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2 (SeparableConv (None, 4, 4, 728)    536536      block11_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_bn (BatchNorma (None, 4, 4, 728)    2912        block11_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_act (Activatio (None, 4, 4, 728)    0           block11_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3 (SeparableConv (None, 4, 4, 728)    536536      block11_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_bn (BatchNorma (None, 4, 4, 728)    2912        block11_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 4, 4, 728)    0           block11_sepconv3_bn[0][0]        \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_act (Activatio (None, 4, 4, 728)    0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1 (SeparableConv (None, 4, 4, 728)    536536      block12_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_bn (BatchNorma (None, 4, 4, 728)    2912        block12_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_act (Activatio (None, 4, 4, 728)    0           block12_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2 (SeparableConv (None, 4, 4, 728)    536536      block12_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_bn (BatchNorma (None, 4, 4, 728)    2912        block12_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_act (Activatio (None, 4, 4, 728)    0           block12_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3 (SeparableConv (None, 4, 4, 728)    536536      block12_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_bn (BatchNorma (None, 4, 4, 728)    2912        block12_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 4, 4, 728)    0           block12_sepconv3_bn[0][0]        \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_act (Activatio (None, 4, 4, 728)    0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1 (SeparableConv (None, 4, 4, 728)    536536      block13_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_bn (BatchNorma (None, 4, 4, 728)    2912        block13_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_act (Activatio (None, 4, 4, 728)    0           block13_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2 (SeparableConv (None, 4, 4, 1024)   752024      block13_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_bn (BatchNorma (None, 4, 4, 1024)   4096        block13_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 2, 2, 1024)   745472      add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_pool (MaxPooling2D)     (None, 2, 2, 1024)   0           block13_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 2, 2, 1024)   4096        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 2, 2, 1024)   0           block13_pool[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1 (SeparableConv (None, 2, 2, 1536)   1582080     add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_bn (BatchNorma (None, 2, 2, 1536)   6144        block14_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_act (Activatio (None, 2, 2, 1536)   0           block14_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2 (SeparableConv (None, 2, 2, 2048)   3159552     block14_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_bn (BatchNorma (None, 2, 2, 2048)   8192        block14_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_act (Activatio (None, 2, 2, 2048)   0           block14_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d (GlobalMax (None, 2048)         0           block14_sepconv2_act[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 20,861,480\n",
      "Trainable params: 20,806,952\n",
      "Non-trainable params: 54,528\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_t = tf.keras.Input(shape = (64, 64, 3))\n",
    "xcptn_model = Xception(weights='imagenet', include_top=False, input_tensor=input_t,\n",
    "                             pooling='max')\n",
    "xcptn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afb1e3ab",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-04-09T15:23:18.801086Z",
     "iopub.status.busy": "2022-04-09T15:23:18.799587Z",
     "iopub.status.idle": "2022-04-09T15:23:18.871082Z",
     "shell.execute_reply": "2022-04-09T15:23:18.871531Z",
     "shell.execute_reply.started": "2022-04-09T10:24:08.07939Z"
    },
    "papermill": {
     "duration": 0.109029,
     "end_time": "2022-04-09T15:23:18.871712",
     "exception": false,
     "start_time": "2022-04-09T15:23:18.762683",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_1 - True\n",
      "1 block1_conv1 - True\n",
      "2 block1_conv1_bn - True\n",
      "3 block1_conv1_act - True\n",
      "4 block1_conv2 - True\n",
      "5 block1_conv2_bn - True\n",
      "6 block1_conv2_act - True\n",
      "7 block2_sepconv1 - True\n",
      "8 block2_sepconv1_bn - True\n",
      "9 block2_sepconv2_act - True\n",
      "10 block2_sepconv2 - True\n",
      "11 block2_sepconv2_bn - True\n",
      "12 conv2d - True\n",
      "13 block2_pool - True\n",
      "14 batch_normalization - True\n",
      "15 add - True\n",
      "16 block3_sepconv1_act - True\n",
      "17 block3_sepconv1 - True\n",
      "18 block3_sepconv1_bn - True\n",
      "19 block3_sepconv2_act - True\n",
      "20 block3_sepconv2 - True\n",
      "21 block3_sepconv2_bn - True\n",
      "22 conv2d_1 - True\n",
      "23 block3_pool - True\n",
      "24 batch_normalization_1 - True\n",
      "25 add_1 - True\n",
      "26 block4_sepconv1_act - True\n",
      "27 block4_sepconv1 - True\n",
      "28 block4_sepconv1_bn - True\n",
      "29 block4_sepconv2_act - True\n",
      "30 block4_sepconv2 - True\n",
      "31 block4_sepconv2_bn - True\n",
      "32 conv2d_2 - True\n",
      "33 block4_pool - True\n",
      "34 batch_normalization_2 - True\n",
      "35 add_2 - True\n",
      "36 block5_sepconv1_act - True\n",
      "37 block5_sepconv1 - True\n",
      "38 block5_sepconv1_bn - True\n",
      "39 block5_sepconv2_act - True\n",
      "40 block5_sepconv2 - True\n",
      "41 block5_sepconv2_bn - True\n",
      "42 block5_sepconv3_act - True\n",
      "43 block5_sepconv3 - True\n",
      "44 block5_sepconv3_bn - True\n",
      "45 add_3 - True\n",
      "46 block6_sepconv1_act - True\n",
      "47 block6_sepconv1 - True\n",
      "48 block6_sepconv1_bn - True\n",
      "49 block6_sepconv2_act - True\n",
      "50 block6_sepconv2 - True\n",
      "51 block6_sepconv2_bn - True\n",
      "52 block6_sepconv3_act - True\n",
      "53 block6_sepconv3 - True\n",
      "54 block6_sepconv3_bn - True\n",
      "55 add_4 - True\n",
      "56 block7_sepconv1_act - True\n",
      "57 block7_sepconv1 - True\n",
      "58 block7_sepconv1_bn - True\n",
      "59 block7_sepconv2_act - True\n",
      "60 block7_sepconv2 - True\n",
      "61 block7_sepconv2_bn - True\n",
      "62 block7_sepconv3_act - True\n",
      "63 block7_sepconv3 - True\n",
      "64 block7_sepconv3_bn - True\n",
      "65 add_5 - True\n",
      "66 block8_sepconv1_act - True\n",
      "67 block8_sepconv1 - True\n",
      "68 block8_sepconv1_bn - True\n",
      "69 block8_sepconv2_act - True\n",
      "70 block8_sepconv2 - True\n",
      "71 block8_sepconv2_bn - True\n",
      "72 block8_sepconv3_act - True\n",
      "73 block8_sepconv3 - True\n",
      "74 block8_sepconv3_bn - True\n",
      "75 add_6 - True\n",
      "76 block9_sepconv1_act - True\n",
      "77 block9_sepconv1 - True\n",
      "78 block9_sepconv1_bn - True\n",
      "79 block9_sepconv2_act - True\n",
      "80 block9_sepconv2 - True\n",
      "81 block9_sepconv2_bn - True\n",
      "82 block9_sepconv3_act - True\n",
      "83 block9_sepconv3 - True\n",
      "84 block9_sepconv3_bn - True\n",
      "85 add_7 - True\n",
      "86 block10_sepconv1_act - True\n",
      "87 block10_sepconv1 - True\n",
      "88 block10_sepconv1_bn - True\n",
      "89 block10_sepconv2_act - True\n",
      "90 block10_sepconv2 - True\n",
      "91 block10_sepconv2_bn - True\n",
      "92 block10_sepconv3_act - True\n",
      "93 block10_sepconv3 - True\n",
      "94 block10_sepconv3_bn - True\n",
      "95 add_8 - True\n",
      "96 block11_sepconv1_act - True\n",
      "97 block11_sepconv1 - True\n",
      "98 block11_sepconv1_bn - True\n",
      "99 block11_sepconv2_act - True\n",
      "100 block11_sepconv2 - True\n",
      "101 block11_sepconv2_bn - True\n",
      "102 block11_sepconv3_act - True\n",
      "103 block11_sepconv3 - True\n",
      "104 block11_sepconv3_bn - True\n",
      "105 add_9 - True\n",
      "106 block12_sepconv1_act - True\n",
      "107 block12_sepconv1 - True\n",
      "108 block12_sepconv1_bn - True\n",
      "109 block12_sepconv2_act - True\n",
      "110 block12_sepconv2 - True\n",
      "111 block12_sepconv2_bn - True\n",
      "112 block12_sepconv3_act - True\n",
      "113 block12_sepconv3 - True\n",
      "114 block12_sepconv3_bn - True\n",
      "115 add_10 - True\n",
      "116 block13_sepconv1_act - True\n",
      "117 block13_sepconv1 - True\n",
      "118 block13_sepconv1_bn - True\n",
      "119 block13_sepconv2_act - True\n",
      "120 block13_sepconv2 - True\n",
      "121 block13_sepconv2_bn - True\n",
      "122 conv2d_3 - True\n",
      "123 block13_pool - True\n",
      "124 batch_normalization_3 - True\n",
      "125 add_11 - True\n",
      "126 block14_sepconv1 - True\n",
      "127 block14_sepconv1_bn - True\n",
      "128 block14_sepconv1_act - True\n",
      "129 block14_sepconv2 - True\n",
      "130 block14_sepconv2_bn - True\n",
      "131 block14_sepconv2_act - True\n",
      "132 global_max_pooling2d - True\n"
     ]
    }
   ],
   "source": [
    "for layer in xcptn_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "for i, layer in enumerate(xcptn_model.layers):\n",
    "    print(i, layer.name, \"-\", layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c098827b",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-04-09T15:23:18.933390Z",
     "iopub.status.busy": "2022-04-09T15:23:18.932497Z",
     "iopub.status.idle": "2022-04-09T15:23:18.969091Z",
     "shell.execute_reply": "2022-04-09T15:23:18.968577Z",
     "shell.execute_reply.started": "2022-04-09T10:24:42.795232Z"
    },
    "papermill": {
     "duration": 0.071684,
     "end_time": "2022-04-09T15:23:18.969222",
     "exception": false,
     "start_time": "2022-04-09T15:23:18.897538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_res = (64, 64)\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(layers.Lambda(lambda image: tf.image.resize(image, to_res))) \n",
    "model.add(xcptn_model)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3dfb65e",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-04-09T15:23:19.024357Z",
     "iopub.status.busy": "2022-04-09T15:23:19.023549Z",
     "iopub.status.idle": "2022-04-09T15:23:19.406308Z",
     "shell.execute_reply": "2022-04-09T15:23:19.405528Z",
     "shell.execute_reply.started": "2022-04-09T10:29:24.338008Z"
    },
    "papermill": {
     "duration": 0.411436,
     "end_time": "2022-04-09T15:23:19.406433",
     "exception": false,
     "start_time": "2022-04-09T15:23:18.994997",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda (Lambda)              (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "xception (Functional)        (None, 2048)              20861480  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 21,437,810\n",
      "Trainable params: 21,378,290\n",
      "Non-trainable params: 59,520\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loss = 'categorical_crossentropy'\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "metrics = ['accuracy']\n",
    "\n",
    "model.compile(loss = loss, optimizer = opt, metrics = metrics)\n",
    "model.build((None, 64, 64, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61f7c95",
   "metadata": {
    "papermill": {
     "duration": 0.025579,
     "end_time": "2022-04-09T15:23:19.457920",
     "exception": false,
     "start_time": "2022-04-09T15:23:19.432341",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training & Hyperparamater-Tuning for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ba054de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T15:23:19.517161Z",
     "iopub.status.busy": "2022-04-09T15:23:19.516167Z",
     "iopub.status.idle": "2022-04-09T15:23:21.579431Z",
     "shell.execute_reply": "2022-04-09T15:23:21.578940Z",
     "shell.execute_reply.started": "2022-04-09T10:30:40.651572Z"
    },
    "papermill": {
     "duration": 2.095949,
     "end_time": "2022-04-09T15:23:21.579563",
     "exception": false,
     "start_time": "2022-04-09T15:23:19.483614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Composing the Train Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((df_train, y_train_oh)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d75a709",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T15:23:21.647494Z",
     "iopub.status.busy": "2022-04-09T15:23:21.643761Z",
     "iopub.status.idle": "2022-04-09T18:32:26.706596Z",
     "shell.execute_reply": "2022-04-09T18:32:26.706129Z",
     "shell.execute_reply.started": "2022-04-09T10:31:52.511142Z"
    },
    "papermill": {
     "duration": 11345.10088,
     "end_time": "2022-04-09T18:32:26.706764",
     "exception": false,
     "start_time": "2022-04-09T15:23:21.605884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1251/1251 [==============================] - 77s 52ms/step - loss: 1.8583 - accuracy: 0.3262\n",
      "Epoch 2/10\n",
      "1251/1251 [==============================] - 66s 52ms/step - loss: 1.1700 - accuracy: 0.6139\n",
      "Epoch 3/10\n",
      "1251/1251 [==============================] - 66s 52ms/step - loss: 0.9232 - accuracy: 0.7188\n",
      "Epoch 4/10\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.7582 - accuracy: 0.7828\n",
      "Epoch 5/10\n",
      "1251/1251 [==============================] - 66s 53ms/step - loss: 0.6378 - accuracy: 0.8237\n",
      "Epoch 6/10\n",
      "1251/1251 [==============================] - 66s 53ms/step - loss: 0.5423 - accuracy: 0.8526\n",
      "Epoch 7/10\n",
      "1251/1251 [==============================] - 66s 53ms/step - loss: 0.4597 - accuracy: 0.8786\n",
      "Epoch 8/10\n",
      "1251/1251 [==============================] - 66s 53ms/step - loss: 0.4009 - accuracy: 0.8950\n",
      "Epoch 9/10\n",
      "1251/1251 [==============================] - 66s 53ms/step - loss: 0.3706 - accuracy: 0.9061\n",
      "Epoch 10/10\n",
      "1251/1251 [==============================] - 66s 53ms/step - loss: 0.3354 - accuracy: 0.9142\n",
      "For  10  Epochs:\n",
      "Log-loss for Train Dataset =  0.23618224373230287\n",
      "Log-loss for Test Dataset =  0.5554009681626929\n",
      "Accuracy for Train Dataset =  0.9341348797680348\n",
      "Accuracy for Test Dataset =  0.8567\n",
      "\n",
      "Epoch 1/20\n",
      "1251/1251 [==============================] - 73s 53ms/step - loss: 0.2837 - accuracy: 0.9302\n",
      "Epoch 2/20\n",
      "1251/1251 [==============================] - 66s 53ms/step - loss: 0.2529 - accuracy: 0.9387\n",
      "Epoch 3/20\n",
      "1251/1251 [==============================] - 66s 52ms/step - loss: 0.2278 - accuracy: 0.9440\n",
      "Epoch 4/20\n",
      "1251/1251 [==============================] - 66s 52ms/step - loss: 0.2119 - accuracy: 0.9495\n",
      "Epoch 5/20\n",
      "1251/1251 [==============================] - 66s 53ms/step - loss: 0.1994 - accuracy: 0.9527\n",
      "Epoch 6/20\n",
      "1251/1251 [==============================] - 66s 53ms/step - loss: 0.1829 - accuracy: 0.9570\n",
      "Epoch 7/20\n",
      "1251/1251 [==============================] - 66s 53ms/step - loss: 0.1561 - accuracy: 0.9642\n",
      "Epoch 8/20\n",
      "1251/1251 [==============================] - 66s 53ms/step - loss: 0.1657 - accuracy: 0.9619\n",
      "Epoch 9/20\n",
      "1251/1251 [==============================] - 67s 53ms/step - loss: 0.1578 - accuracy: 0.9620\n",
      "Epoch 10/20\n",
      "1251/1251 [==============================] - 66s 53ms/step - loss: 0.1328 - accuracy: 0.9700\n",
      "Epoch 11/20\n",
      "1251/1251 [==============================] - 67s 53ms/step - loss: 0.1318 - accuracy: 0.9705\n",
      "Epoch 12/20\n",
      "1251/1251 [==============================] - 67s 54ms/step - loss: 0.1230 - accuracy: 0.9711\n",
      "Epoch 13/20\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.1340 - accuracy: 0.9689\n",
      "Epoch 14/20\n",
      "1251/1251 [==============================] - 66s 53ms/step - loss: 0.1061 - accuracy: 0.9771\n",
      "Epoch 15/20\n",
      "1251/1251 [==============================] - 66s 53ms/step - loss: 0.1172 - accuracy: 0.9740\n",
      "Epoch 16/20\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.1053 - accuracy: 0.9769\n",
      "Epoch 17/20\n",
      "1251/1251 [==============================] - 67s 54ms/step - loss: 0.1060 - accuracy: 0.9761\n",
      "Epoch 18/20\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0899 - accuracy: 0.9805\n",
      "Epoch 19/20\n",
      "1251/1251 [==============================] - 67s 54ms/step - loss: 0.0985 - accuracy: 0.9777\n",
      "Epoch 20/20\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.1067 - accuracy: 0.9757\n",
      "For  20  Epochs:\n",
      "Log-loss for Train Dataset =  0.10216296096053956\n",
      "Log-loss for Test Dataset =  0.6509600694794184\n",
      "Accuracy for Train Dataset =  0.9745538169274609\n",
      "Accuracy for Test Dataset =  0.8733\n",
      "\n",
      "Epoch 1/30\n",
      "1251/1251 [==============================] - 75s 54ms/step - loss: 0.0961 - accuracy: 0.9780\n",
      "Epoch 2/30\n",
      "1251/1251 [==============================] - 66s 52ms/step - loss: 0.0857 - accuracy: 0.9813\n",
      "Epoch 3/30\n",
      "1251/1251 [==============================] - 67s 54ms/step - loss: 0.0831 - accuracy: 0.9819\n",
      "Epoch 4/30\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0753 - accuracy: 0.9840\n",
      "Epoch 5/30\n",
      "1251/1251 [==============================] - 68s 54ms/step - loss: 0.0854 - accuracy: 0.9821\n",
      "Epoch 6/30\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0607 - accuracy: 0.9865\n",
      "Epoch 7/30\n",
      "1251/1251 [==============================] - 68s 54ms/step - loss: 0.0762 - accuracy: 0.9833\n",
      "Epoch 8/30\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0691 - accuracy: 0.9840\n",
      "Epoch 9/30\n",
      "1251/1251 [==============================] - 68s 54ms/step - loss: 0.0718 - accuracy: 0.9843\n",
      "Epoch 10/30\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0675 - accuracy: 0.9854\n",
      "Epoch 11/30\n",
      "1251/1251 [==============================] - 69s 55ms/step - loss: 0.0646 - accuracy: 0.9860\n",
      "Epoch 12/30\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0643 - accuracy: 0.9858\n",
      "Epoch 13/30\n",
      "1251/1251 [==============================] - 68s 55ms/step - loss: 0.0619 - accuracy: 0.9873\n",
      "Epoch 14/30\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0643 - accuracy: 0.9859\n",
      "Epoch 15/30\n",
      "1251/1251 [==============================] - 68s 55ms/step - loss: 0.1040 - accuracy: 0.9756\n",
      "Epoch 16/30\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0397 - accuracy: 0.9916\n",
      "Epoch 17/30\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0460 - accuracy: 0.9908\n",
      "Epoch 18/30\n",
      "1251/1251 [==============================] - 66s 53ms/step - loss: 0.0547 - accuracy: 0.9889\n",
      "Epoch 19/30\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0543 - accuracy: 0.9884\n",
      "Epoch 20/30\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0515 - accuracy: 0.9890\n",
      "Epoch 21/30\n",
      "1251/1251 [==============================] - 66s 53ms/step - loss: 0.0522 - accuracy: 0.9890\n",
      "Epoch 22/30\n",
      "1251/1251 [==============================] - 70s 56ms/step - loss: 0.0558 - accuracy: 0.9882\n",
      "Epoch 23/30\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0482 - accuracy: 0.9897\n",
      "Epoch 24/30\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0597 - accuracy: 0.9871\n",
      "Epoch 25/30\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0443 - accuracy: 0.9908\n",
      "Epoch 26/30\n",
      "1251/1251 [==============================] - 69s 55ms/step - loss: 0.0474 - accuracy: 0.9894\n",
      "Epoch 27/30\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0543 - accuracy: 0.9881\n",
      "Epoch 28/30\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0439 - accuracy: 0.9898\n",
      "Epoch 29/30\n",
      "1251/1251 [==============================] - 66s 52ms/step - loss: 0.0548 - accuracy: 0.9882\n",
      "Epoch 30/30\n",
      "1251/1251 [==============================] - 69s 55ms/step - loss: 0.0483 - accuracy: 0.9899\n",
      "For  30  Epochs:\n",
      "Log-loss for Train Dataset =  0.04132024941976583\n",
      "Log-loss for Test Dataset =  0.6976624152409385\n",
      "Accuracy for Train Dataset =  0.9899015147727841\n",
      "Accuracy for Test Dataset =  0.8852\n",
      "\n",
      "Epoch 1/40\n",
      "1251/1251 [==============================] - 70s 52ms/step - loss: 0.0761 - accuracy: 0.9821\n",
      "Epoch 2/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0232 - accuracy: 0.9948\n",
      "Epoch 3/40\n",
      "1251/1251 [==============================] - 70s 56ms/step - loss: 0.0137 - accuracy: 0.9975\n",
      "Epoch 4/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0212 - accuracy: 0.9956\n",
      "Epoch 5/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0316 - accuracy: 0.9940\n",
      "Epoch 6/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0442 - accuracy: 0.9913\n",
      "Epoch 7/40\n",
      "1251/1251 [==============================] - 71s 56ms/step - loss: 0.0364 - accuracy: 0.9923\n",
      "Epoch 8/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0487 - accuracy: 0.9895\n",
      "Epoch 9/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0420 - accuracy: 0.9912\n",
      "Epoch 10/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0461 - accuracy: 0.9912\n",
      "Epoch 11/40\n",
      "1251/1251 [==============================] - 67s 54ms/step - loss: 0.0386 - accuracy: 0.9921\n",
      "Epoch 12/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0458 - accuracy: 0.9904\n",
      "Epoch 13/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0524 - accuracy: 0.9885\n",
      "Epoch 14/40\n",
      "1251/1251 [==============================] - 68s 54ms/step - loss: 0.0347 - accuracy: 0.9928\n",
      "Epoch 15/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0435 - accuracy: 0.9909\n",
      "Epoch 16/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0420 - accuracy: 0.9915\n",
      "Epoch 17/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0428 - accuracy: 0.9908\n",
      "Epoch 18/40\n",
      "1251/1251 [==============================] - 72s 57ms/step - loss: 0.0360 - accuracy: 0.9919\n",
      "Epoch 19/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0343 - accuracy: 0.9932\n",
      "Epoch 20/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0346 - accuracy: 0.9931\n",
      "Epoch 21/40\n",
      "1251/1251 [==============================] - 66s 53ms/step - loss: 0.0394 - accuracy: 0.9918\n",
      "Epoch 22/40\n",
      "1251/1251 [==============================] - 67s 54ms/step - loss: 0.0403 - accuracy: 0.9918\n",
      "Epoch 23/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0491 - accuracy: 0.9891\n",
      "Epoch 24/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0384 - accuracy: 0.9922\n",
      "Epoch 25/40\n",
      "1251/1251 [==============================] - 66s 52ms/step - loss: 0.0387 - accuracy: 0.9922\n",
      "Epoch 26/40\n",
      "1251/1251 [==============================] - 68s 54ms/step - loss: 0.0389 - accuracy: 0.9917\n",
      "Epoch 27/40\n",
      "1251/1251 [==============================] - 66s 52ms/step - loss: 0.0300 - accuracy: 0.9940\n",
      "Epoch 28/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0458 - accuracy: 0.9900\n",
      "Epoch 29/40\n",
      "1251/1251 [==============================] - 66s 53ms/step - loss: 0.0339 - accuracy: 0.9923\n",
      "Epoch 30/40\n",
      "1251/1251 [==============================] - 67s 54ms/step - loss: 0.0349 - accuracy: 0.9930\n",
      "Epoch 31/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0394 - accuracy: 0.9922\n",
      "Epoch 32/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0343 - accuracy: 0.9935\n",
      "Epoch 33/40\n",
      "1251/1251 [==============================] - 68s 55ms/step - loss: 0.0283 - accuracy: 0.9941\n",
      "Epoch 34/40\n",
      "1251/1251 [==============================] - 66s 52ms/step - loss: 0.0340 - accuracy: 0.9928\n",
      "Epoch 35/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0328 - accuracy: 0.9937\n",
      "Epoch 36/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0303 - accuracy: 0.9929\n",
      "Epoch 37/40\n",
      "1251/1251 [==============================] - 69s 55ms/step - loss: 0.0400 - accuracy: 0.9917\n",
      "Epoch 38/40\n",
      "1251/1251 [==============================] - 66s 53ms/step - loss: 0.0237 - accuracy: 0.9953\n",
      "Epoch 39/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0483 - accuracy: 0.9895\n",
      "Epoch 40/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0113 - accuracy: 0.9974\n",
      "For  40  Epochs:\n",
      "Log-loss for Train Dataset =  0.006939108973857245\n",
      "Log-loss for Test Dataset =  0.803500938438388\n",
      "Accuracy for Train Dataset =  0.9982752587111933\n",
      "Accuracy for Test Dataset =  0.9009\n",
      "\n",
      "Epoch 1/50\n",
      "1251/1251 [==============================] - 75s 56ms/step - loss: 0.0336 - accuracy: 0.9928\n",
      "Epoch 2/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0267 - accuracy: 0.9941\n",
      "Epoch 3/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0350 - accuracy: 0.9925\n",
      "Epoch 4/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0280 - accuracy: 0.9944\n",
      "Epoch 5/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0313 - accuracy: 0.9935\n",
      "Epoch 6/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0249 - accuracy: 0.9950\n",
      "Epoch 7/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0295 - accuracy: 0.9937\n",
      "Epoch 8/50\n",
      "1251/1251 [==============================] - 68s 54ms/step - loss: 0.0304 - accuracy: 0.9935\n",
      "Epoch 9/50\n",
      "1251/1251 [==============================] - 69s 56ms/step - loss: 0.0294 - accuracy: 0.9939\n",
      "Epoch 10/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0343 - accuracy: 0.9929\n",
      "Epoch 11/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0258 - accuracy: 0.9949\n",
      "Epoch 12/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0275 - accuracy: 0.9943\n",
      "Epoch 13/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0285 - accuracy: 0.9942\n",
      "Epoch 14/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0302 - accuracy: 0.9939\n",
      "Epoch 15/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0211 - accuracy: 0.9956\n",
      "Epoch 16/50\n",
      "1251/1251 [==============================] - 71s 57ms/step - loss: 0.0246 - accuracy: 0.9951\n",
      "Epoch 17/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0360 - accuracy: 0.9925\n",
      "Epoch 18/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0267 - accuracy: 0.9941\n",
      "Epoch 19/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0318 - accuracy: 0.9940\n",
      "Epoch 20/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0245 - accuracy: 0.9956\n",
      "Epoch 21/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0192 - accuracy: 0.9962\n",
      "Epoch 22/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0265 - accuracy: 0.9949\n",
      "Epoch 23/50\n",
      "1251/1251 [==============================] - 68s 55ms/step - loss: 0.0260 - accuracy: 0.9944\n",
      "Epoch 24/50\n",
      "1251/1251 [==============================] - 68s 54ms/step - loss: 0.0308 - accuracy: 0.9935\n",
      "Epoch 25/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0193 - accuracy: 0.9965\n",
      "Epoch 26/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0278 - accuracy: 0.9939\n",
      "Epoch 27/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0253 - accuracy: 0.9948\n",
      "Epoch 28/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0204 - accuracy: 0.9959\n",
      "Epoch 29/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0244 - accuracy: 0.9948\n",
      "Epoch 30/50\n",
      "1251/1251 [==============================] - 66s 53ms/step - loss: 0.0250 - accuracy: 0.9946\n",
      "Epoch 31/50\n",
      "1251/1251 [==============================] - 73s 58ms/step - loss: 0.0236 - accuracy: 0.9952\n",
      "Epoch 32/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0230 - accuracy: 0.9949\n",
      "Epoch 33/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0227 - accuracy: 0.9953\n",
      "Epoch 34/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0279 - accuracy: 0.9946\n",
      "Epoch 35/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0218 - accuracy: 0.9953\n",
      "Epoch 36/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0182 - accuracy: 0.9965\n",
      "Epoch 37/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0202 - accuracy: 0.9957\n",
      "Epoch 38/50\n",
      "1251/1251 [==============================] - 66s 53ms/step - loss: 0.0281 - accuracy: 0.9946\n",
      "Epoch 39/50\n",
      "1251/1251 [==============================] - 73s 58ms/step - loss: 0.0268 - accuracy: 0.9945\n",
      "Epoch 40/50\n",
      "1251/1251 [==============================] - 66s 53ms/step - loss: 0.0230 - accuracy: 0.9953\n",
      "Epoch 41/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0150 - accuracy: 0.9964\n",
      "Epoch 42/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0334 - accuracy: 0.9930\n",
      "Epoch 43/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0159 - accuracy: 0.9968\n",
      "Epoch 44/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0250 - accuracy: 0.9952\n",
      "Epoch 45/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0248 - accuracy: 0.9954\n",
      "Epoch 46/50\n",
      "1251/1251 [==============================] - 69s 55ms/step - loss: 0.0222 - accuracy: 0.9958\n",
      "Epoch 47/50\n",
      "1251/1251 [==============================] - 70s 56ms/step - loss: 0.0269 - accuracy: 0.9946\n",
      "Epoch 48/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0208 - accuracy: 0.9959\n",
      "Epoch 49/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0159 - accuracy: 0.9970\n",
      "Epoch 50/50\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0278 - accuracy: 0.9946\n",
      "For  50  Epochs:\n",
      "Log-loss for Train Dataset =  0.11568610624239081\n",
      "Log-loss for Test Dataset =  1.0174927796425084\n",
      "Accuracy for Train Dataset =  0.9793780932860071\n",
      "Accuracy for Test Dataset =  0.8685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = [10, 20, 30, 40, 50]\n",
    "train_loss, test_loss, train_acc, test_acc = [], [], [], []\n",
    "\n",
    "for epochs in num_epochs:\n",
    "    # Training the Model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "    model.fit(train_dataset, epochs = epochs)\n",
    "    \n",
    "    # Predicting on the Train/Test Datasets\n",
    "    preds_train = model.predict(df_train)\n",
    "    preds_test = model.predict(df_test)\n",
    "\n",
    "    # Finding the Predicted Classes\n",
    "    cls_train = np.argmax(preds_train, axis = 1)\n",
    "    cls_test = np.argmax(preds_test, axis = 1)\n",
    "    \n",
    "    # Finding the Train/Test set Loss\n",
    "    train_loss.append(log_loss(y_train_oh, preds_train))\n",
    "    test_loss.append(log_loss(y_test_oh, preds_test))\n",
    "    train_acc.append(accuracy_score(y_train, cls_train))\n",
    "    test_acc.append(accuracy_score(y_test, cls_test))\n",
    "    \n",
    "    print(\"For \", epochs, \" Epochs:\")\n",
    "    print(\"Log-loss for Train Dataset = \", train_loss[-1])\n",
    "    print(\"Log-loss for Test Dataset = \", test_loss[-1])\n",
    "    print(\"Accuracy for Train Dataset = \", train_acc[-1])\n",
    "    print(\"Accuracy for Test Dataset = \", test_acc[-1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "005088a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T18:34:29.718248Z",
     "iopub.status.busy": "2022-04-09T18:34:29.717397Z",
     "iopub.status.idle": "2022-04-09T19:22:29.192196Z",
     "shell.execute_reply": "2022-04-09T19:22:27.621169Z",
     "shell.execute_reply.started": "2022-04-09T13:35:43.588553Z"
    },
    "papermill": {
     "duration": 2940.643271,
     "end_time": "2022-04-09T19:22:29.192365",
     "exception": false,
     "start_time": "2022-04-09T18:33:28.549094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1251/1251 [==============================] - 70s 52ms/step - loss: 0.0235 - accuracy: 0.9957\n",
      "Epoch 2/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0168 - accuracy: 0.9964\n",
      "Epoch 3/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0224 - accuracy: 0.9954\n",
      "Epoch 4/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0255 - accuracy: 0.9942\n",
      "Epoch 5/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0177 - accuracy: 0.9964\n",
      "Epoch 6/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0227 - accuracy: 0.9957\n",
      "Epoch 7/40\n",
      "1251/1251 [==============================] - 66s 52ms/step - loss: 0.0155 - accuracy: 0.9966\n",
      "Epoch 8/40\n",
      "1251/1251 [==============================] - 74s 59ms/step - loss: 0.0255 - accuracy: 0.9953\n",
      "Epoch 9/40\n",
      "1251/1251 [==============================] - 67s 54ms/step - loss: 0.0211 - accuracy: 0.9955\n",
      "Epoch 10/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0184 - accuracy: 0.9964\n",
      "Epoch 11/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0212 - accuracy: 0.9955\n",
      "Epoch 12/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0442 - accuracy: 0.9913\n",
      "Epoch 13/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0120 - accuracy: 0.9979\n",
      "Epoch 14/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0054 - accuracy: 0.9990\n",
      "Epoch 15/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0035 - accuracy: 0.9992\n",
      "Epoch 16/40\n",
      "1251/1251 [==============================] - 73s 58ms/step - loss: 0.0301 - accuracy: 0.9938\n",
      "Epoch 17/40\n",
      "1251/1251 [==============================] - 69s 55ms/step - loss: 0.0188 - accuracy: 0.9966\n",
      "Epoch 18/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0323 - accuracy: 0.9945\n",
      "Epoch 19/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0165 - accuracy: 0.9966\n",
      "Epoch 20/40\n",
      "1251/1251 [==============================] - 64s 52ms/step - loss: 0.0159 - accuracy: 0.9966\n",
      "Epoch 21/40\n",
      "1251/1251 [==============================] - 64s 51ms/step - loss: 0.0180 - accuracy: 0.9961\n",
      "Epoch 22/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0182 - accuracy: 0.9963\n",
      "Epoch 23/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0249 - accuracy: 0.9951\n",
      "Epoch 24/40\n",
      "1251/1251 [==============================] - 71s 57ms/step - loss: 0.0168 - accuracy: 0.9962\n",
      "Epoch 25/40\n",
      "1251/1251 [==============================] - 72s 57ms/step - loss: 0.0172 - accuracy: 0.9964\n",
      "Epoch 26/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0199 - accuracy: 0.9959\n",
      "Epoch 27/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0153 - accuracy: 0.9968\n",
      "Epoch 28/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0234 - accuracy: 0.9953\n",
      "Epoch 29/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0164 - accuracy: 0.9969\n",
      "Epoch 30/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0208 - accuracy: 0.9956\n",
      "Epoch 31/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0164 - accuracy: 0.9970\n",
      "Epoch 32/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0236 - accuracy: 0.9951\n",
      "Epoch 33/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0146 - accuracy: 0.9969\n",
      "Epoch 34/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0187 - accuracy: 0.9959\n",
      "Epoch 35/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0216 - accuracy: 0.9958\n",
      "Epoch 36/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0147 - accuracy: 0.9967\n",
      "Epoch 37/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0189 - accuracy: 0.9959\n",
      "Epoch 38/40\n",
      "1251/1251 [==============================] - 65s 52ms/step - loss: 0.0193 - accuracy: 0.9957\n",
      "Epoch 39/40\n",
      "1251/1251 [==============================] - 69s 55ms/step - loss: 0.0164 - accuracy: 0.9968\n",
      "Epoch 40/40\n",
      "1251/1251 [==============================] - 72s 58ms/step - loss: 0.0263 - accuracy: 0.9955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9776953b90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the Model with the best hyper-parameter settings\n",
    "ind = np.argmax(test_acc)\n",
    "best_num_epochs = num_epochs[ind]\n",
    "# model = cnn_model((32, 32, 3))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(train_dataset, epochs = best_num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce117d56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T19:25:04.677100Z",
     "iopub.status.busy": "2022-04-09T19:25:04.676337Z",
     "iopub.status.idle": "2022-04-09T19:25:07.377647Z",
     "shell.execute_reply": "2022-04-09T19:25:07.378147Z",
     "shell.execute_reply.started": "2022-04-09T14:35:29.425872Z"
    },
    "papermill": {
     "duration": 80.467157,
     "end_time": "2022-04-09T19:25:07.378317",
     "exception": false,
     "start_time": "2022-04-09T19:23:46.911160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "model.save('xcptn_model.h5')\n",
    "model = tf.keras.models.load_model('./xcptn_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6900ad95",
   "metadata": {
    "papermill": {
     "duration": 77.595222,
     "end_time": "2022-04-09T19:27:42.042916",
     "exception": false,
     "start_time": "2022-04-09T19:26:24.447694",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predicting & Evaluating on Train/Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ea7d7b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T19:30:17.189879Z",
     "iopub.status.busy": "2022-04-09T19:30:17.188947Z",
     "iopub.status.idle": "2022-04-09T19:30:45.648083Z",
     "shell.execute_reply": "2022-04-09T19:30:45.648502Z",
     "shell.execute_reply.started": "2022-04-09T14:35:29.428065Z"
    },
    "papermill": {
     "duration": 106.007075,
     "end_time": "2022-04-09T19:30:45.648695",
     "exception": false,
     "start_time": "2022-04-09T19:28:59.641620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predicting on the Train/Test Datasets\n",
    "preds_train = model.predict(df_train)\n",
    "preds_test = model.predict(df_test)\n",
    "\n",
    "# Finding the Predicted Classes\n",
    "train_cls = np.argmax(preds_train, axis = 1)\n",
    "test_cls = np.argmax(preds_test, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6923236e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T19:33:20.341487Z",
     "iopub.status.busy": "2022-04-09T19:33:20.340683Z",
     "iopub.status.idle": "2022-04-09T19:33:20.406173Z",
     "shell.execute_reply": "2022-04-09T19:33:20.406766Z",
     "shell.execute_reply.started": "2022-04-09T14:35:29.430836Z"
    },
    "papermill": {
     "duration": 77.433451,
     "end_time": "2022-04-09T19:33:20.406967",
     "exception": false,
     "start_time": "2022-04-09T19:32:02.973516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Train Dataset =  0.9972504124381343\n",
      "Accuracy for Test Dataset =  0.8957\n",
      "Log-loss for Train Dataset =  0.009731522791650571\n",
      "Log-loss for Test Dataset =  0.8017136794626224\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for Train Dataset = \", accuracy_score(y_train, train_cls))\n",
    "print(\"Accuracy for Test Dataset = \", accuracy_score(y_test, test_cls))\n",
    "\n",
    "print(\"Log-loss for Train Dataset = \", log_loss(y_train_oh, preds_train))\n",
    "print(\"Log-loss for Test Dataset = \", log_loss(y_test_oh, preds_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15133.558188,
   "end_time": "2022-04-09T19:34:41.990620",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-09T15:22:28.432432",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
